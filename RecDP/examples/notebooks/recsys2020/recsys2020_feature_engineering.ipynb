{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/nvme2/chendi/BlueWhale/recdp\n"
     ]
    }
   ],
   "source": [
    "#!/env/bin/python\n",
    "\n",
    "import init\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql import *\n",
    "from pyspark import *\n",
    "import pyspark.sql.functions as f\n",
    "from timeit import default_timer as timer\n",
    "import logging\n",
    "from RecsysSchema import RecsysSchema\n",
    "from pyrecdp.data_processor import *\n",
    "from pyrecdp.encoder import *\n",
    "from pyrecdp.utils import *\n",
    "import hashlib\n",
    "\n",
    "\n",
    "def decodeBertTokenizerAndExtractFeatures(df, proc, output_name):\n",
    "    # modification on original feature\n",
    "    op_fillna_str = FillNA(\n",
    "        ['present_domains', 'present_links', 'hashtags', 'present_media', 'tweet_id'], \"\")\n",
    "    op_fillna_num = FillNA(['reply_timestamp', 'retweet_timestamp',\n",
    "                        'retweet_with_comment_timestamp', 'like_timestamp'], 0)\n",
    "    \n",
    "    op_feature_modification_type_convert = FeatureModification(cols=['tweet_timestamp',\n",
    "                                                                     'engaged_with_user_follower_count',\n",
    "                                                                     'engaged_with_user_following_count',\n",
    "                                                                     'engaged_with_user_account_creation',\n",
    "                                                                     'engaging_user_follower_count',\n",
    "                                                                     'engaging_user_following_count',\n",
    "                                                                     'engaging_user_account_creation',\n",
    "                                                                     'reply_timestamp',\n",
    "                                                                     'retweet_timestamp',\n",
    "                                                                     'retweet_with_comment_timestamp',\n",
    "                                                                     'like_timestamp'], op='toInt')\n",
    "    \n",
    "    op_feature_target_classify = FeatureModification(cols={\n",
    "        \"reply_timestamp\": \"f.when(f.col('reply_timestamp') > 0, 1).otherwise(0)\",\n",
    "        \"retweet_timestamp\": \"f.when(f.col('retweet_timestamp') > 0, 1).otherwise(0)\",\n",
    "        \"retweet_with_comment_timestamp\": \"f.when(f.col('retweet_with_comment_timestamp') > 0, 1).otherwise(0)\",\n",
    "        \"like_timestamp\": \"f.when(f.col('like_timestamp') > 0, 1).otherwise(0)\"}, op='inline')\n",
    "    \n",
    "    # adding new features\n",
    "    op_feature_to_be_categorified = FeatureAdd(\n",
    "        cols={\"present_domains_indicator\": \"f.col('present_domains')\",\\\n",
    "              \"present_links_indicator\": \"f.col('present_links')\",\\\n",
    "              \"hashtags_indicator\": \"f.col('hashtags')\",\\\n",
    "              \"language_indicator\": \"f.col('language')\",\\\n",
    "              \"tweet_id_indicator\": \"f.col('tweet_id')\",\\\n",
    "              \"present_media_indicator\": \"f.col('present_media')\",\\\n",
    "              \"tweet_type_indicator\": \"f.col('tweet_type')\",\\\n",
    "              \"engaged_with_user_id_indicator\": \"f.col('engaged_with_user_id')\",\\\n",
    "              \"engaging_user_id_indicator\": \"f.col('engaging_user_id')\"\n",
    "             }, \n",
    "        op='inline')\n",
    "    \n",
    "    op_feature_from_original = FeatureAdd(\n",
    "        cols={\"has_photo\": \"f.col('present_media').contains('Photo').cast(t.IntegerType())\",\n",
    "              \"has_video\": \"f.col('present_media').contains('Vedio').cast(t.IntegerType())\",\n",
    "              \"has_gif\": \"f.col('present_media').contains('GIF').cast(t.IntegerType())\",             \n",
    "              \"a_ff_rate\": \"f.col('engaged_with_user_following_count')/f.col('engaged_with_user_follower_count')\",\n",
    "              \"b_ff_rate\": \"f.col('engaging_user_following_count') /f.col('engaging_user_follower_count')\",\n",
    "              \"dt_dow\": \"f.dayofweek(f.from_unixtime(f.col('tweet_timestamp'))).cast(t.IntegerType())\",\n",
    "              \"dt_hour\": \"f.hour(f.from_unixtime(f.col('tweet_timestamp'))).cast(t.IntegerType())\",  \n",
    "              \"dt_minute\": \"f.minute(f.from_unixtime(f.col('tweet_timestamp'))).cast(t.IntegerType())\",\n",
    "              \"dt_second\": \"f.second(f.from_unixtime(f.col('tweet_timestamp'))).cast(t.IntegerType())\",\n",
    "              'present_domains_indicator': \"f.concat_ws('_', f.slice(f.split(f.col('present_domains_indicator'),'\\t'), 1, 2))\",\n",
    "              'len_hashtags': \"f.when(f.col('hashtags') == '', f.lit(0)).otherwise(f.size(f.split(f.col('hashtags'), '\\t')))\",\n",
    "              'len_domains': \"f.when(f.col('present_domains') == '', f.lit(0)).otherwise(f.size(f.split(f.col('present_domains'), '\\t')))\",\n",
    "              'len_links': \"f.when(f.col('present_links') == '', f.lit(0)).otherwise(f.size(f.split(f.col('present_links'), '\\t')))\",\n",
    "              'engage_time': \"f.least(f.col('reply_timestamp'), f.col('retweet_timestamp'), f.col('retweet_with_comment_timestamp'), f.col('like_timestamp'))\"   \n",
    "        }, op='inline')\n",
    "    op_fillna_tweet_timestamp = FillNA(['tweet_timestamp'], -1)\n",
    "    ops = [op_fillna_str, op_fillna_num,\n",
    "           op_feature_modification_type_convert, op_feature_target_classify,\n",
    "           op_feature_to_be_categorified, op_feature_from_original, op_fillna_tweet_timestamp]\n",
    "\n",
    "    ########## Tweet token feature engineering ##########\n",
    "    from transformers import BertTokenizer\n",
    "    tokenizer = BertTokenizer.from_pretrained(\n",
    "        'bert-base-multilingual-cased', do_lower_case=False)\n",
    "\n",
    "    # define UDF\n",
    "    tokenizer_decode = f.udf(lambda x: tokenizer.decode(\n",
    "        [int(n) for n in x.split('\\t')]))\n",
    "    format_url = f.udf(lambda x: x.replace(\n",
    "        'https : / / t. co / ', 'https://t.co/').replace('@ ', '@'))\n",
    "\n",
    "    def extract_hash(text, split_text='@', no=0):\n",
    "        text = text.lower()\n",
    "        uhash = ''\n",
    "        text_split = text.split('@')\n",
    "        if len(text_split) > (no+1):\n",
    "            text_split = text_split[no+1].split(' ')\n",
    "            cl_loop = True\n",
    "            uhash += clean_text(text_split[0])\n",
    "            while cl_loop:\n",
    "                if len(text_split) > 1:\n",
    "                    if text_split[1] in ['_']:\n",
    "                        uhash += clean_text(text_split[1]) + \\\n",
    "                            clean_text(text_split[2])\n",
    "                        text_split = text_split[2:]\n",
    "                    else:\n",
    "                        cl_loop = False\n",
    "                else:\n",
    "                    cl_loop = False\n",
    "        hash_object = hashlib.md5(uhash.encode('utf-8'))\n",
    "        return hash_object.hexdigest()\n",
    "\n",
    "    def clean_text(text):\n",
    "        if len(text) > 1:\n",
    "            if text[-1] in ['!', '?', ':', ';', '.', ',']:\n",
    "                return(text[:-1])\n",
    "        return(text)\n",
    "    \n",
    "    # udf defines upon tweet\n",
    "    to_notsign = f.udf(lambda x: x.replace('\\[CLS\\] RT @', ''))\n",
    "    count_space = f.udf(lambda x: x.count(' '))\n",
    "    count_text_length = f.udf(lambda x: len(x))\n",
    "    user_defined_hash = f.udf(lambda x: extract_hash(x, split_text='RT @', no=0))\n",
    "    count_at = f.udf(lambda x: x.count('@'))\n",
    "    user_define_hash_1 = f.udf(lambda x: extract_hash(x))\n",
    "    user_define_hash_2 = f.udf(lambda x: extract_hash(x, no=1))\n",
    "\n",
    "    # decode\n",
    "    op_feature_modification_tokenizer_decode = FeatureAdd(cols={'tweet': 'text_tokens'}, udfImpl=tokenizer_decode)\n",
    "    op_feature_modification_format_url = FeatureModification(cols=['tweet'], udfImpl=format_url)\n",
    "\n",
    "    # adding new features\n",
    "    op_feature_add_tweet_indicator = FeatureAdd(cols={\"tweet_indicator\": \"f.col('tweet')\"}, op='inline')\n",
    "    op_feature_add_tweet_nortsign = FeatureAdd(cols={'tweet_nortsign': 'tweet'}, udfImpl=to_notsign)\n",
    "    op_feature_add_count_words = FeatureAdd(cols={'count_words': 'tweet'}, udfImpl=count_space)\n",
    "    op_feature_add_count_char = FeatureAdd(cols={'count_char': 'tweet'}, udfImpl=count_text_length)\n",
    "    op_feature_add_tw_uhash = FeatureAdd(cols={'tw_uhash': 'tweet'}, udfImpl=user_defined_hash)\n",
    "    op_feature_add_tw_hash = FeatureAdd(cols={'tw_hash': \"f.hash(f.col('tweet'))%1000000000\"}, op='inline')\n",
    "    # features upon tweet_nortsign\n",
    "    op_feature_add_count_at = FeatureAdd(cols={'count_ats': 'tweet_nortsign'}, udfImpl=count_at)\n",
    "    op_feature_add_tw_uhash0 = FeatureAdd(cols={'tw_hash0': 'tweet_nortsign'}, udfImpl=user_define_hash_1)\n",
    "    op_feature_add_tw_uhash1 = FeatureAdd(cols={'tw_hash1': 'tweet_nortsign'}, udfImpl=user_define_hash_2)\n",
    "\n",
    "    ops += [op_feature_modification_tokenizer_decode, op_feature_modification_format_url,\n",
    "            op_feature_add_tweet_indicator,\n",
    "            op_feature_add_tweet_nortsign, op_feature_add_count_words, op_feature_add_count_char,\n",
    "            op_feature_add_tw_uhash, op_feature_add_tw_hash, op_feature_add_count_at,\n",
    "            op_feature_add_tw_uhash0, op_feature_add_tw_uhash1]\n",
    "    proc.reset_ops(ops)\n",
    "\n",
    "    # execute\n",
    "    t1 = timer()\n",
    "    df = proc.transform(df, name=output_name)\n",
    "    t2 = timer()\n",
    "    print(\"BertTokenizer decode and feature extacting took %.3f\" % (t2 - t1))\n",
    "\n",
    "    return df\n",
    "\n",
    "def splitByDate(df, proc, train_output, test_output, numFolds=5):\n",
    "    # 1.1 get timestamp range\n",
    "    import datetime\n",
    "    min_timestamp = df.select('tweet_timestamp').agg({'tweet_timestamp': 'min'}).collect()[0]['min(tweet_timestamp)']\n",
    "    max_timestamp = df.select('tweet_timestamp').agg({'tweet_timestamp': 'max'}).collect()[0]['max(tweet_timestamp)']\n",
    "    seconds_in_day = 3600 * 24\n",
    "\n",
    "    print(\n",
    "        \"min_timestamp is %s, max_timestamp is %s, 20 days max is %s\" % (\n",
    "            datetime.datetime.fromtimestamp(min_timestamp).strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            datetime.datetime.fromtimestamp(max_timestamp).strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            datetime.datetime.fromtimestamp(min_timestamp + 20 * seconds_in_day).strftime('%Y-%m-%d %H:%M:%S')\n",
    "        ))\n",
    "\n",
    "    time_range_split = {\n",
    "        'train': (min_timestamp, seconds_in_day * 18 + min_timestamp),\n",
    "        'test': (seconds_in_day * 18 + min_timestamp, max_timestamp)\n",
    "    }\n",
    "\n",
    "    print(time_range_split)\n",
    "\n",
    "    # 1.2 save ranged data for train\n",
    "    # filtering out train range data and save\n",
    "    train_start, train_end = time_range_split['train']\n",
    "    test_start, test_end = time_range_split['test']\n",
    "    t1 = timer()\n",
    "    train_df = df.filter(\n",
    "        (f.col('tweet_timestamp') >= f.lit(train_start)) & (f.col('tweet_timestamp') < f.lit(train_end)))\n",
    "    train_df = train_df.withColumn(\"fold\", f.round(f.rand(seed=42)*numFolds))\n",
    "    train_df.write.format('parquet').mode('overwrite').save(proc.path_prefix + proc.current_path + train_output)\n",
    "    t2 = timer()\n",
    "    print(\"split to train took %.3f\" % (t2 - t1))\n",
    "    \n",
    "    t1 = timer()\n",
    "    test_df = df.filter(\n",
    "        (f.col('tweet_timestamp') >= f.lit(test_start)) & (f.col('tweet_timestamp') < f.lit(test_end)))\n",
    "    test_df.write.format('parquet').mode('overwrite').save(proc.path_prefix + proc.current_path + test_output)\n",
    "    t2 = timer()\n",
    "    print(\"split to test took %.3f\" % (t2 - t1))\n",
    "    \n",
    "    return (proc.spark.read.parquet(proc.path_prefix + proc.current_path + train_output),\n",
    "            proc.spark.read.parquet(proc.path_prefix + proc.current_path + test_output))\n",
    "\n",
    "\n",
    "def categorifyFeatures(df, proc, output_name=\"train_with_categorified_features\", gen_dict=True, sampleRatio=1):\n",
    "    if gen_dict:\n",
    "        # only call below function when target dicts were not pre-prepared\n",
    "        op_multiItems = GenerateDictionary(\n",
    "            ['present_domains_indicator', 'present_links_indicator', 'hashtags_indicator'], doSplit=True)\n",
    "        op_singleItems = GenerateDictionary(\n",
    "            ['tweet_id_indicator', 'language_indicator', \n",
    "             {'src_cols': ['engaged_with_user_id_indicator', 'engaging_user_id_indicator'], 'col_name': 'user_id'}])\n",
    "        op_tweet = GenerateDictionary(\n",
    "            ['tweet_indicator'], doSplit=True, withCount=True, sep=' ')\n",
    "        \n",
    "        proc.reset_ops([op_multiItems, op_singleItems, op_tweet])\n",
    "        t1 = timer()\n",
    "        dict_dfs = proc.generate_dicts(df)\n",
    "        t2 = timer()\n",
    "        print(\"Generate Dictionary took %.3f\" % (t2 - t1))\n",
    "    else:\n",
    "        # or we can simply load from pre-gened\n",
    "        dict_names = ['hashtags_indicator', 'language_indicator', 'present_domains_indicator',\n",
    "                      'present_links_indicator', 'tweet_id_indicator', 'user_id_indicator', 'tweet_indicator']\n",
    "        dict_dfs = [{'col_name': name, 'dict': proc.spark.read.parquet(\n",
    "            \"%s/%s/%s/%s\" % (proc.path_prefix, proc.current_path, proc.dicts_path, name))} for name in dict_names]\n",
    "\n",
    "    # pre-defined dict\n",
    "    # pre-define\n",
    "    media = {\n",
    "        '': 0,\n",
    "        'GIF': 1,\n",
    "        'GIF_GIF': 2,\n",
    "        'GIF_Photo': 3,\n",
    "        'GIF_Video': 4,\n",
    "        'Photo': 5,\n",
    "        'Photo_GIF': 6,\n",
    "        'Photo_Photo': 7,\n",
    "        'Photo_Video': 8,\n",
    "        'Video': 9,\n",
    "        'Video_GIF': 10,\n",
    "        'Video_Photo': 11,\n",
    "        'Video_Video': 12\n",
    "    }\n",
    "\n",
    "    tweet_type = {'Quote': 0, 'Retweet': 1, 'TopLevel': 2}\n",
    "\n",
    "    media_df = proc.spark.createDataFrame(convert_to_spark_dict(media))\n",
    "    tweet_type_df = proc.spark.createDataFrame(\n",
    "        convert_to_spark_dict(tweet_type))\n",
    "\n",
    "    dict_dfs.append({'col_name': 'present_media_indicator', 'dict': media_df})\n",
    "    dict_dfs.append({'col_name': 'tweet_type_indicator', 'dict': tweet_type_df})\n",
    "\n",
    "    for i in dict_dfs:\n",
    "        dict_name = i['col_name']\n",
    "        dict_df = i['dict']\n",
    "        print(\"%s has numRows as %d\" % (dict_name, dict_df.count()))\n",
    "\n",
    "    ###### 2. define operations and append them to data processor ######\n",
    "\n",
    "    # 1. define operations\n",
    "    # 1.1 filter on tweet dict\n",
    "    i = 0\n",
    "    for dict_df in dict_dfs:\n",
    "        if dict_df['col_name'] == 'tweet':\n",
    "            tweet_dict_df = dict_df['dict']\n",
    "            df_cnt = tweet_dict_df.count()\n",
    "            freqRange = [2, df_cnt * 0.9]\n",
    "            tweet_dict_df = tweet_dict_df.filter((f.col('count') <= f.lit(freqRange[1])) & (f.col('count') >= f.lit(freqRange[0])))\n",
    "            dict_dfs[i]['dict'] = tweet_dict_df\n",
    "        i += 1      \n",
    "\n",
    "    # 1.3 categorify\n",
    "    # since language dict is small, we may use udf to make partition more even\n",
    "    op_categorify_1 = Categorify(\n",
    "        ['present_domains_indicator', 'present_links_indicator', 'hashtags_indicator'], dict_dfs=dict_dfs, doSplit=True, keepMostFrequent=True)\n",
    "    op_categorify_2 = Categorify(['language_indicator', 'present_media_indicator', 'tweet_type_indicator'], dict_dfs=dict_dfs)\n",
    "    op_categorify_3 = Categorify([{'engaged_with_user_id_indicator': 'user_id'}, {'engaging_user_id_indicator': 'user_id'}], dict_dfs=dict_dfs)\n",
    "    op_categorify_4 = Categorify(['tweet_id_indicator'], dict_dfs=dict_dfs)\n",
    "    op_categorify_5 = Categorify(['tweet_indicator'], dict_dfs=dict_dfs, doSplit=True, sep=' ', doSortForArray=True)\n",
    "    #### below are features upon categorified tweet\n",
    "    op_feature_add_tw_word = FeatureAdd({\n",
    "        'tw_first_word': \"f.col('tweet_indicator').getItem(0)\",\n",
    "        'tw_second_word': \"f.col('tweet_indicator').getItem(1)\",\n",
    "        'tw_last_word': \"f.col('tweet_indicator').getItem(f.size(f.col('tweet_indicator')) - 1)\",\n",
    "        'tw_llast_word': \"f.col('tweet_indicator').getItem(f.size(f.col('tweet_indicator')) - 1)\",\n",
    "        'tw_len': \"f.size(f.col('tweet_indicator'))\"\n",
    "    }, op='inline')\n",
    "    ops = [op_categorify_1, op_categorify_2, op_categorify_3, op_categorify_4, op_categorify_5, op_feature_add_tw_word]\n",
    "    proc.reset_ops(ops)\n",
    "\n",
    "    t1 = timer()\n",
    "    if sampleRatio < 1 and sampleRatio > 0:\n",
    "        df = df.sample(sampleRatio)\n",
    "    df = proc.transform(df, name=output_name)\n",
    "    t2 = timer()\n",
    "    print(\"categorify took %.3f\" % (t2 - t1))    \n",
    "    return df, dict_dfs\n",
    "\n",
    "\n",
    "def encodingFeatures(df, proc, output_name, gen_dict, sampleRatio=1):   \n",
    "    targets = ['reply_timestamp', 'retweet_timestamp', 'retweet_with_comment_timestamp', 'like_timestamp']\n",
    "    y_mean_all = []\n",
    "    \n",
    "    t1 = timer()\n",
    "    if gen_dict:\n",
    "        for tgt in targets:\n",
    "            tmp = df.groupBy().mean(tgt).collect()[0]\n",
    "            y_mean = tmp[f\"avg({tgt})\"]\n",
    "            y_mean_all.append(y_mean)\n",
    "        schema = t.StructType([t.StructField(tgt, t.FloatType(), True) for tgt in targets])\n",
    "        y_mean_all_df = proc.spark.createDataFrame([tuple(y_mean_all)], schema)\n",
    "        y_mean_all_df.write.format(\"parquet\").mode(\"overwrite\").save(\n",
    "            \"%s/%s/%s/targets_mean\" % (proc.path_prefix, proc.current_path, proc.dicts_path))\n",
    "    y_mean_all_df = proc.spark.read.parquet(\n",
    "        \"%s/%s/%s/targets_mean\" % (proc.path_prefix, proc.current_path, proc.dicts_path))\n",
    "\n",
    "    te_features = [\n",
    "        'present_media',\n",
    "        'tweet_type',\n",
    "        'language',\n",
    "        'engaged_with_user_id',\n",
    "        'engaging_user_id',\n",
    "        ['present_domains','language','engagee_follows_engager','tweet_type','present_media','engaged_with_user_is_verified'],\n",
    "        ['engaged_with_user_id','tweet_type','language'],\n",
    "        ['tw_first_word','tweet_type','language'],\n",
    "        ['tw_last_word','tweet_type','language'],\n",
    "        ['tw_hash0','tweet_type','language'],\n",
    "        ['tw_hash1','tweet_type','language'],\n",
    "        ['tw_uhash','tweet_type','language'],\n",
    "        ['tw_hash'],\n",
    "        ['present_media','tweet_type','language','engaged_with_user_is_verified','engaging_user_is_verified','engagee_follows_engager'],\n",
    "        ['present_domains','present_media','tweet_type','language'],\n",
    "        ['present_links','present_media','tweet_type','language'],\n",
    "        ['hashtags','present_media','tweet_type','language']\n",
    "        \n",
    "    ]\n",
    "    ce_features = ['present_media', 'tweet_type', 'language', 'engaged_with_user_id', 'engaging_user_id']\n",
    "    fe_features = ['present_media', 'tweet_type', 'language', 'engaged_with_user_id', 'engaging_user_id']\n",
    "    encoding_features = [(\"TE\", te_features), (\"CE\", ce_features), (\"FE\", fe_features)]\n",
    "\n",
    "    te_train_dfs = []\n",
    "    te_test_dfs = []\n",
    "    ce_train_dfs = []\n",
    "    ce_test_dfs = []\n",
    "    fe_train_dfs = []\n",
    "    fe_test_dfs = []\n",
    "    for feature_type, features in encoding_features:\n",
    "        for c in features:\n",
    "            target_tmp = targets\n",
    "            out_name = \"\"\n",
    "            out_col_list = []\n",
    "            for tgt in target_tmp:\n",
    "                if isinstance(c, list):\n",
    "                    out_col_list.append(f'G{feature_type}_' + '_'.join(c) + f'_{tgt}')\n",
    "                    out_name = f'G{feature_type}_' + '_'.join(c)\n",
    "                else:\n",
    "                    out_col_list.append(f'{feature_type}_{c}_{tgt}')\n",
    "                    out_name = f'{feature_type}_{c}'\n",
    "            if gen_dict:\n",
    "                start = timer()\n",
    "                if feature_type == 'TE':\n",
    "                    encoder = TargetEncoder(proc, c, target_tmp, out_col_list, out_name, out_dtype=t.FloatType(), y_mean_list=y_mean_all)\n",
    "                    te_train_df, te_test_df = encoder.transform(df)\n",
    "                    te_train_dfs.append({'col_name': ['fold'] + (c if isinstance(c, list) else [c]), 'dict': te_train_df})\n",
    "                    te_test_dfs.append({'col_name': c, 'dict': te_test_df})\n",
    "                    print(f\"generating target encoding for %s upon %s took %.1f seconds\"%(str(c), str(target_tmp), timer()-start))\n",
    "\n",
    "                elif feature_type == 'CE':\n",
    "                    encoder = CountEncoder(proc, c, target_tmp, out_col_list, out_name)  \n",
    "                    ce_train_df, ce_test_df = encoder.transform(df)\n",
    "                    ce_train_dfs.append({'col_name': c if isinstance(c, list) else [c], 'dict': ce_train_df})\n",
    "                    ce_test_dfs.append({'col_name': c, 'dict': ce_test_df})  \n",
    "                    print(f\"generating count encoding for %s upon %s took %.1f seconds\"%(str(c), str(target_tmp), timer()-start))\n",
    "\n",
    "                elif feature_type == 'FE':\n",
    "                    # For frequency encoding, we don't need to merge with train data\n",
    "                    encoder = FrequencyEncoder(proc, c, target_tmp, out_col_list, out_name) \n",
    "                    fe_train_df, fe_test_df = encoder.transform(df)\n",
    "                    fe_train_dfs.append({'col_name': c if isinstance(c, list) else [c], 'dict': fe_train_df})\n",
    "                    print(f\"generating frequency encoding for %s upon %s took %.1f seconds\"%(str(c), str(target_tmp), timer()-start))\n",
    "\n",
    "            else:\n",
    "                te_train_path = \"%s/%s/%s/train/%s\" % (proc.path_prefix, proc.current_path, proc.dicts_path, out_name)\n",
    "                te_test_path = \"%s/%s/%s/test/%s\" % (proc.path_prefix, proc.current_path, proc.dicts_path, out_name) \n",
    "                if feature_type == 'TE':\n",
    "                    te_train_dfs.append({'col_name': ['fold'] + (c if isinstance(c, list) else [c]), 'dict': proc.spark.read.parquet(te_train_path)})\n",
    "                    te_test_dfs.append({'col_name': c, 'dict': proc.spark.read.parquet(te_test_path)})\n",
    "                if feature_type == 'CE':\n",
    "                    ce_train_dfs.append({'col_name': c if isinstance(c, list) else [c], 'dict': proc.spark.read.parquet(te_train_path)})\n",
    "                    ce_test_dfs.append({'col_name': c, 'dict': proc.spark.read.parquet(te_test_path)})\n",
    "                if feature_type == 'FE':\n",
    "                    # For frequency encoding, we don't need to merge with train data\n",
    "                    fe_train_dfs.append({'col_name': c if isinstance(c, list) else [c], 'dict': proc.spark.read.parquet(te_train_path)})\n",
    "\n",
    "    t2 = timer()\n",
    "    print(\"Generate encoding feature totally took %.3f\" % (t2 - t1))\n",
    "\n",
    "    # merge dicts to original table\n",
    "    if sampleRatio < 1 and sampleRatio > 0:\n",
    "        df = df.sample(sampleRatio)\n",
    "    i = 3\n",
    "    for train_dfs in [te_train_dfs, ce_train_dfs, fe_train_dfs]:\n",
    "        op_merge_to_train = ModelMerge(train_dfs)\n",
    "        proc.reset_ops([op_merge_to_train])\n",
    "        i -= 1\n",
    "        _output_name = output_name if i == 0 else f\"{output_name}_{i}\"\n",
    "\n",
    "        t1 = timer()\n",
    "        df = proc.transform(df, name=_output_name)\n",
    "        t2 = timer()\n",
    "        print(\"encodingFeatures took %.3f\" % (t2 - t1))\n",
    "    \n",
    "    return (df, te_train_dfs, te_test_dfs, y_mean_all_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "path_prefix = \"hdfs://\"\n",
    "current_path = \"/recsys2020_example/\"\n",
    "original_folder = \"/recsys2021_0608/\"\n",
    "dicts_folder = \"recsys_dicts/\"\n",
    "recsysSchema = RecsysSchema()\n",
    "\n",
    "##### 1. Start spark and initialize data processor #####\n",
    "scala_udf_jars = \"/mnt/nvme2/chendi/BlueWhale/recdp/ScalaProcessUtils/target/recdp-scala-extensions-0.1.0-jar-with-dependencies.jar\"\n",
    "\n",
    "t0 = timer()\n",
    "spark = SparkSession.builder.master('yarn')\\\n",
    "    .appName(\"Recsys2021_data_process\")\\\n",
    "    .config(\"spark.executor.memory\", \"20g\")\\\n",
    "    .config(\"spark.executor.memoryOverhead\", \"10g\")\\\n",
    "    .config(\"spark.sql.broadcastTimeout\", \"7200\")\\\n",
    "    .config(\"spark.cleaner.periodicGC.interval\", \"10min\")\\\n",
    "    .config(\"spark.executorEnv.HF_DATASETS_OFFLINE\", \"1\")\\\n",
    "    .config(\"spark.executorEnv.TRANSFORMERS_OFFLINE\", \"1\")\\\n",
    "    .config(\"spark.driver.extraClassPath\", f\"{scala_udf_jars}\")\\\n",
    "    .config(\"spark.executor.extraClassPath\", f\"{scala_udf_jars}\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "schema = recsysSchema.toStructType()\n",
    "\n",
    "# 1.1 prepare dataFrames\n",
    "# 1.2 create RecDP DataProcessor\n",
    "proc = DataProcessor(spark, path_prefix,\n",
    "                     current_path=current_path, dicts_path=dicts_folder, shuffle_disk_capacity=\"1200GB\")\n",
    "df = spark.read.parquet(path_prefix + original_folder)\n",
    "df = df.withColumnRenamed('enaging_user_following_count', 'engaging_user_following_count')\n",
    "df = df.withColumnRenamed('enaging_user_is_verified', 'engaging_user_is_verified')\n",
    "\n",
    "# fast test, comment for full dataset\n",
    "# df.sample(0.01).write.format(\"parquet\").mode(\"overwrite\").save(\"%s/sample_0_0_1\" % current_path)\n",
    "# df = spark.read.parquet(\"%s/sample_0_0_1\" % current_path)\n",
    "\n",
    "# ===============================================\n",
    "# decode tweet_tokens\n",
    "df = decodeBertTokenizerAndExtractFeatures(df, proc, output_name=\"decoded_with_extracted_features\")\n",
    "\n",
    "# ===============================================\n",
    "# splitting and sampling\n",
    "df, test_df = splitByDate(df, proc, train_output=\"train\", test_output=\"test\", numFolds=5)\n",
    "\n",
    "# ===============================================\n",
    "# generate dictionary for categorify indexing\n",
    "df, dict_dfs = categorifyFeatures(df, proc, output_name=\"train_with_categorified_features\", gen_dict=True, sampleRatio=0.03)\n",
    "\n",
    "# ===============================================\n",
    "# encoding features\n",
    "df, te_train_dfs, te_test_dfs, y_mean_all_df = encodingFeatures(df, proc, output_name=\"train_with_features_sample_0_0_3\", gen_dict=True, sampleRatio=0.03)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output with sample as 0.01\n",
    "\n",
    "* recdp-scala-extension is enabled\n",
    "* per core memory size is 5.000 GB and shuffle_disk maximum capacity is 1200.000 GB\n",
    "* BertTokenizer decode and feature extacting took 242.857\n",
    "* min_timestamp is 2021-02-04 08:00:00, max_timestamp is 2021-02-25 07:59:59, 20 days max is 2021-02-24 08:00:00\n",
    "* {'train': (1612396800, 1613952000), 'test': (1613952000, 1614211199)}\n",
    "* split to train took 15.447\n",
    "* split to test took 6.934\n",
    "* Generate Dictionary took 37.361\n",
    "* present_domains_indicator has numRows as 80174\n",
    "* present_links_indicator has numRows as 497560\n",
    "* hashtags_indicator has numRows as 494988\n",
    "* tweet_id_indicator has numRows as 4708592\n",
    "* language_indicator has numRows as 66\n",
    "* user_id has numRows as 5065850\n",
    "* tweet_indicator has numRows as 8332709\n",
    "* present_media_indicator has numRows as 13\n",
    "* tweet_type_indicator has numRows as 3\n",
    "* do scala_udf to present_domains_indicator\n",
    "* do scala_udf to hashtags_indicator\n",
    "* do scala_udf to present_links_indicator\n",
    "* do bhj to tweet_type_indicator\n",
    "* do bhj to present_media_indicator\n",
    "* do bhj to language_indicator\n",
    "* Adding a CodegenSeparator to pure BHJ WSCG case\n",
    "* do bhj to engaged_with_user_id_indicator\n",
    "* do bhj to engaging_user_id_indicator\n",
    "* Adding a CodegenSeparator to pure BHJ WSCG case\n",
    "* do bhj to tweet_id_indicator\n",
    "* Adding a CodegenSeparator to pure BHJ WSCG case\n",
    "* do scala_udf to tweet_indicator\n",
    "* categorify took 281.024\n",
    "* generating target encoding for present_media upon ['reply_timestamp', 'retweet_timestamp', 'retweet_with_comment_timestamp', 'like_timestamp'] took 3.5 seconds\n",
    "* generating target encoding for tweet_type upon ['reply_timestamp', 'retweet_timestamp', 'retweet_with_comment_timestamp', 'like_timestamp'] took 1.5 seconds\n",
    "* generating target encoding for language upon ['reply_timestamp', 'retweet_timestamp', 'retweet_with_comment_timestamp', 'like_timestamp'] took 2.9 seconds\n",
    "* generating target encoding for engaged_with_user_id upon ['reply_timestamp', 'retweet_timestamp', 'retweet_with_comment_timestamp', 'like_timestamp'] took 7.9 seconds\n",
    "* generating target encoding for engaging_user_id upon ['reply_timestamp', 'retweet_timestamp', 'retweet_with_comment_timestamp', 'like_timestamp'] took 5.7 seconds\n",
    "* generating target encoding for ['present_domains', 'language', 'engagee_follows_engager', 'tweet_type', 'present_media', 'engaged_with_user_is_verified'] upon ['reply_timestamp', 'retweet_timestamp', 'retweet_with_comment_timestamp', 'like_timestamp'] took 6.0 seconds\n",
    "* generating target encoding for ['engaged_with_user_id', 'tweet_type', 'language'] upon ['reply_timestamp', 'retweet_timestamp', 'retweet_with_comment_timestamp', 'like_timestamp'] took 7.0 seconds\n",
    "* generating target encoding for ['tw_first_word', 'tweet_type', 'language'] upon ['reply_timestamp', 'retweet_timestamp', 'retweet_with_comment_timestamp', 'like_timestamp'] took 3.8 seconds\n",
    "* generating target encoding for ['tw_last_word', 'tweet_type', 'language'] upon ['reply_timestamp', 'retweet_timestamp', 'retweet_with_comment_timestamp', 'like_timestamp'] took 6.3 seconds\n",
    "* generating target encoding for ['tw_hash0', 'tweet_type', 'language'] upon ['reply_timestamp', 'retweet_timestamp', 'retweet_with_comment_timestamp', 'like_timestamp'] took 5.2 seconds\n",
    "* generating target encoding for ['tw_hash1', 'tweet_type', 'language'] upon ['reply_timestamp', 'retweet_timestamp', 'retweet_with_comment_timestamp', 'like_timestamp'] took 3.7 seconds\n",
    "* generating target encoding for ['tw_uhash', 'tweet_type', 'language'] upon ['reply_timestamp', 'retweet_timestamp', 'retweet_with_comment_timestamp', 'like_timestamp'] took 4.4 seconds\n",
    "* generating target encoding for ['tw_hash'] upon ['reply_timestamp', 'retweet_timestamp', 'retweet_with_comment_timestamp', 'like_timestamp'] took 5.2 seconds\n",
    "* generating target encoding for ['present_media', 'tweet_type', 'language', 'engaged_with_user_is_verified', 'engaging_user_is_verified', 'engagee_follows_engager'] upon ['reply_timestamp', 'retweet_timestamp', 'retweet_with_comment_timestamp', 'like_timestamp'] took 4.5 seconds\n",
    "* generating target encoding for ['present_domains', 'present_media', 'tweet_type', 'language'] upon ['reply_timestamp', 'retweet_timestamp', 'retweet_with_comment_timestamp', 'like_timestamp'] took 6.0 seconds\n",
    "* generating target encoding for ['present_links', 'present_media', 'tweet_type', 'language'] upon ['reply_timestamp', 'retweet_timestamp', 'retweet_with_comment_timestamp', 'like_timestamp'] took 3.7 seconds\n",
    "* generating target encoding for ['hashtags', 'present_media', 'tweet_type', 'language'] upon ['reply_timestamp', 'retweet_timestamp', 'retweet_with_comment_timestamp', 'like_timestamp'] took 4.7 seconds\n",
    "* generating count encoding for present_media upon ['reply_timestamp', 'retweet_timestamp', 'retweet_with_comment_timestamp', 'like_timestamp'] took 1.5 seconds\n",
    "* generating count encoding for tweet_type upon ['reply_timestamp', 'retweet_timestamp', 'retweet_with_comment_timestamp', 'like_timestamp'] took 1.4 seconds\n",
    "* generating count encoding for language upon ['reply_timestamp', 'retweet_timestamp', 'retweet_with_comment_timestamp', 'like_timestamp'] took 2.2 seconds\n",
    "* generating count encoding for engaged_with_user_id upon ['reply_timestamp', 'retweet_timestamp', 'retweet_with_comment_timestamp', 'like_timestamp'] took 3.0 seconds\n",
    "* generating count encoding for engaging_user_id upon ['reply_timestamp', 'retweet_timestamp', 'retweet_with_comment_timestamp', 'like_timestamp'] took 3.0 seconds\n",
    "* generating frequency encoding for present_media upon ['reply_timestamp', 'retweet_timestamp', 'retweet_with_comment_timestamp', 'like_timestamp'] took 1.2 seconds\n",
    "* generating frequency encoding for tweet_type upon ['reply_timestamp', 'retweet_timestamp', 'retweet_with_comment_timestamp', 'like_timestamp'] took 1.1 seconds\n",
    "* generating frequency encoding for language upon ['reply_timestamp', 'retweet_timestamp', 'retweet_with_comment_timestamp', 'like_timestamp'] took 1.2 seconds\n",
    "* generating frequency encoding for engaged_with_user_id upon ['reply_timestamp', 'retweet_timestamp', 'retweet_with_comment_timestamp', 'like_timestamp'] took 1.8 seconds\n",
    "* generating frequency encoding for engaging_user_id upon ['reply_timestamp', 'retweet_timestamp', 'retweet_with_comment_timestamp', 'like_timestamp'] took 1.7 seconds\n",
    "* Generate encoding feature totally took 101.488\n",
    "* do bhj to ['fold', 'tweet_type']\n",
    "* do bhj to ['fold', 'present_media']\n",
    "* do bhj to ['fold', 'language']\n",
    "* do bhj to ['fold', 'tw_first_word', 'tweet_type', 'language']\n",
    "* do bhj to ['fold', 'present_media', 'tweet_type', 'language', 'engaged_with_user_is_verified', 'engaging_user_is_verified', 'engagee_follows_engager']\n",
    "* do bhj to ['fold', 'present_domains', 'present_media', 'tweet_type', 'language']\n",
    "* do bhj to ['fold', 'present_domains', 'language', 'engagee_follows_engager', 'tweet_type', 'present_media', 'engaged_with_user_is_verified']\n",
    "* do bhj to ['fold', 'tw_hash1', 'tweet_type', 'language']\n",
    "* do bhj to ['fold', 'present_links', 'present_media', 'tweet_type', 'language']\n",
    "* do bhj to ['fold', 'hashtags', 'present_media', 'tweet_type', 'language']\n",
    "* do bhj to ['fold', 'tw_hash0', 'tweet_type', 'language']\n",
    "* do bhj to ['fold', 'tw_uhash', 'tweet_type', 'language']\n",
    "* do bhj to ['fold', 'engaged_with_user_id']\n",
    "* do bhj to ['fold', 'engaged_with_user_id', 'tweet_type', 'language']\n",
    "* do bhj to ['fold', 'engaging_user_id']\n",
    "* do bhj to ['fold', 'tw_last_word', 'tweet_type', 'language']\n",
    "* do bhj to ['fold', 'tw_hash']\n",
    "* encodingFeatures took 52.923\n",
    "* do bhj to ['tweet_type']\n",
    "* do bhj to ['present_media']\n",
    "* do bhj to ['language']\n",
    "* do bhj to ['engaged_with_user_id']\n",
    "* do bhj to ['engaging_user_id']\n",
    "* encodingFeatures took 17.919\n",
    "* do bhj to ['tweet_type']\n",
    "* do bhj to ['present_media']\n",
    "* do bhj to ['language']\n",
    "* do bhj to ['engaged_with_user_id']\n",
    "* do bhj to ['engaging_user_id']\n",
    "* encodingFeatures took 19.710"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
