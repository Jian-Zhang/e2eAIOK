{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import init\n",
    "\n",
    "# import findspark\n",
    "# findspark.init()\n",
    "\n",
    "from pyrecdp.data_processor import *\n",
    "from pyrecdp.utils import *\n",
    "\n",
    "from pyspark.sql.types import IntegerType, StringType, StructType, StructField, TimestampType, FloatType, ArrayType, DoubleType\n",
    "import datetime\n",
    "import hashlib\n",
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyspark.sql.functions as F\n",
    "import tensorflow as tf\n",
    "from pyspark import TaskContext\n",
    "from tensorflow_transform.tf_metadata import dataset_metadata\n",
    "from tensorflow_transform.tf_metadata import dataset_schema\n",
    "from tensorflow_transform.tf_metadata import metadata_io\n",
    "from examples.notebooks.wnd.data.feature_description import LABEL_COLUMN, DISPLAY_ID_COLUMN, CATEGORICAL_COLUMNS, \\\n",
    "    DOC_CATEGORICAL_MULTIVALUED_COLUMNS, BOOL_COLUMNS, INT_COLUMNS, FLOAT_COLUMNS, \\\n",
    "    FLOAT_COLUMNS_LOG_BIN_TRANSFORM, FLOAT_COLUMNS_SIMPLE_BIN_TRANSFORM, FLOAT_COLUMNS_NO_TRANSFORM, HASH_BUCKET_SIZES\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_BUCKET_FOLDER = \"/tmp/spark/preprocessed/recdp/\"\n",
    "DATA_BUCKET_FOLDER = \"/outbrain/orig/\"\n",
    "SPARK_TEMP_FOLDER = \"/tmp/spark/spark-temp/\"\n",
    "TENSORFLOW_HADOOP = \"data/tensorflow-hadoop-1.5.0.jar\"\n",
    "scala_udf_jars = \"/root/ht/ML/recdp/ScalaProcessUtils/target/recdp-scala-extensions-0.1.0-jar-with-dependencies.jar\"\n",
    "\n",
    "conf = SparkConf().setMaster('spark://sr112:7077').set('spark.executor.memory', '40g').set('spark.driver.memory', '200g').set('spark.executor.cores', '10')\n",
    "conf.set(\"spark.jars\", TENSORFLOW_HADOOP)\n",
    "conf.set(\"spark.driver.extraClassPath\", f\"{scala_udf_jars}\")\n",
    "conf.set(\"spark.executor.extraClassPath\", f\"{scala_udf_jars}\")\n",
    "\n",
    "\n",
    "sc = SparkContext(conf=conf)\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################  split train/test set #######################\n",
    "events_schema = StructType(\n",
    "    [StructField(\"display_id\", IntegerType(), True),\n",
    "     StructField(\"uuid_event\", StringType(), True),\n",
    "     StructField(\"document_id_event\", IntegerType(), True),\n",
    "     StructField(\"timestamp_event\", IntegerType(), True),\n",
    "     StructField(\"platform_event\", IntegerType(), True),\n",
    "     StructField(\"geo_location_event\", StringType(), True)]\n",
    ")\n",
    "\n",
    "events_df = spark.read.schema(events_schema) \\\n",
    "    .options(header='true', inferschema='false', nullValue='\\\\N') \\\n",
    "    .csv(DATA_BUCKET_FOLDER + \"events.csv\") \\\n",
    "    .withColumn('day_event', (F.col('timestamp_event')/ 1000 / 60 / 60 / 24).cast(IntegerType())) \\\n",
    "    .withColumn('event_country', F.substring('geo_location_event', 0, 2)) \\\n",
    "    .withColumn('event_country_state', F.substring('geo_location_event', 0, 5)) \\\n",
    "    .alias('events')\n",
    "\n",
    "print('Drop rows with empty \"geo_location\"...')\n",
    "events_df = events_df.dropna(subset=\"geo_location_event\")\n",
    "\n",
    "print('Drop rows with empty \"platform\"...')\n",
    "events_df = events_df.dropna(subset=\"platform_event\")\n",
    "\n",
    "promoted_content_schema = StructType(\n",
    "    [StructField(\"ad_id\", IntegerType(), True),\n",
    "     StructField(\"document_id_promo\", IntegerType(), True),\n",
    "     StructField(\"campaign_id\", IntegerType(), True),\n",
    "     StructField(\"advertiser_id\", IntegerType(), True)]\n",
    ")\n",
    "\n",
    "promoted_content_df = spark.read.schema(promoted_content_schema) \\\n",
    "    .options(header='true', inferschema='false', nullValue='\\\\N') \\\n",
    "    .csv(DATA_BUCKET_FOLDER + \"promoted_content.csv\") \\\n",
    "    .alias('promoted_content')\n",
    "\n",
    "clicks_train_schema = StructType(\n",
    "    [StructField(\"display_id\", IntegerType(), True),\n",
    "     StructField(\"ad_id\", IntegerType(), True),\n",
    "     StructField(\"clicked\", IntegerType(), True)]\n",
    ")\n",
    "\n",
    "clicks_train_df = spark.read.schema(clicks_train_schema) \\\n",
    "    .options(header='true', inferschema='false', nullValue='\\\\N') \\\n",
    "    .csv(DATA_BUCKET_FOLDER + \"clicks_train.csv\") \\\n",
    "    .alias('clicks_train')\n",
    "\n",
    "documents_meta_schema = StructType(\n",
    "    [StructField(\"document_id_doc\", IntegerType(), True),\n",
    "     StructField(\"source_id\", IntegerType(), True),\n",
    "     StructField(\"publisher_id\", IntegerType(), True),\n",
    "     StructField(\"publish_time\", TimestampType(), True)]\n",
    ")\n",
    "\n",
    "documents_meta_df = spark.read.schema(documents_meta_schema) \\\n",
    "    .options(header='true', inferschema='false', nullValue='\\\\N') \\\n",
    "    .csv(DATA_BUCKET_FOLDER + \"documents_meta.csv\") \\\n",
    "    .alias('documents_meta')\n",
    "\n",
    "# Drop rows with empty \"source_id\"\n",
    "documents_meta_df = documents_meta_df.dropna(subset=\"source_id\")\n",
    "\n",
    "source_publishers_df = documents_meta_df.select([\"source_id\", \"publisher_id\"]).dropDuplicates()\n",
    "\n",
    "# get list of source_ids without publisher_id\n",
    "rows_no_pub = source_publishers_df.filter(\"publisher_id is NULL\")\n",
    "source_ids_without_publisher = [row['source_id'] for row in rows_no_pub.collect()]\n",
    "\n",
    "# maximum value of publisher_id used so far\n",
    "max_pub = max(source_publishers_df.select([\"publisher_id\"]).dropna().collect())['publisher_id']\n",
    "\n",
    "# rows filled with new publisher_ids\n",
    "new_publishers = [(source, max_pub + 1 + nr) for nr, source in enumerate(source_ids_without_publisher)]\n",
    "new_publishers_df = spark.createDataFrame(new_publishers, (\"source_id\", \"publisher_id\"))\n",
    "\n",
    "# old and new publishers merged\n",
    "fixed_source_publishers_df = source_publishers_df.dropna().union(new_publishers_df)\n",
    "\n",
    "# update documents_meta with bew publishers\n",
    "documents_meta_df = documents_meta_df.drop('publisher_id').join(fixed_source_publishers_df, on='source_id')\n",
    "\n",
    "documents_total = documents_meta_df.count()\n",
    "\n",
    "\n",
    "events_joined_df = events_df.join(documents_meta_df\n",
    "                                  .withColumnRenamed('source_id', 'source_id_doc_event')\n",
    "                                  .withColumnRenamed('publisher_id', 'publisher_doc_event')\n",
    "                                  .withColumnRenamed('publish_time', 'publish_time_doc_event')\n",
    "                                  .withColumnRenamed('document_id_doc', 'document_id_doc_event'),\n",
    "                                  on=F.col(\"document_id_event\") == F.col(\"document_id_doc_event\"), how='left').alias('events')\n",
    "\n",
    "clicks_train_joined_df = clicks_train_df \\\n",
    "    .join(promoted_content_df, on='ad_id', how='left') \\\n",
    "    .join(documents_meta_df, on=F.col(\"promoted_content.document_id_promo\") == F.col(\"documents_meta.document_id_doc\"), how='left') \\\n",
    "    .join(events_joined_df, on='display_id', how='left')\n",
    "\n",
    "clicks_train_joined_df.createOrReplaceTempView('clicks_train_joined')\n",
    "\n",
    "\n",
    "validation_display_ids_df = clicks_train_joined_df.select('display_id', 'day_event').distinct() \\\n",
    "    .sampleBy(\"day_event\", fractions={0: 0.2, 1: 0.2, 2: 0.2, 3: 0.2, 4: 0.2,\n",
    "                                      5: 0.2, 6: 0.2, 7: 0.2, 8: 0.2, 9: 0.2, 10: 0.2, 11: 1.0, 12: 1.0}, seed=0)\n",
    "\n",
    "valid_id = validation_display_ids_df.select('display_id').distinct().createOrReplaceTempView(\"validation_display_ids\")\n",
    "\n",
    "valid_set_df = spark.sql('''\n",
    "SELECT * FROM clicks_train_joined t\n",
    "WHERE EXISTS (SELECT display_id FROM validation_display_ids\n",
    "WHERE display_id = t.display_id)''')\n",
    "\n",
    "s_time = time.time()\n",
    "valid_set_df.write.format('parquet').mode('overwrite').save(OUTPUT_BUCKET_FOLDER + 'valid_set_df')\n",
    "valid_set_df = spark.read.parquet(OUTPUT_BUCKET_FOLDER + 'valid_set_df')\n",
    "print(f'valid_set_df time: {time.time() - s_time}')\n",
    "\n",
    "train_set_df = spark.sql('''\n",
    "SELECT * FROM clicks_train_joined t\n",
    "WHERE NOT EXISTS (SELECT display_id FROM validation_display_ids\n",
    "WHERE display_id = t.display_id)''')\n",
    "\n",
    "s_time = time.time()\n",
    "train_set_df.write.format('parquet').mode('overwrite').save(OUTPUT_BUCKET_FOLDER + 'train_set_df')\n",
    "train_set_df = spark.read.parquet(OUTPUT_BUCKET_FOLDER + 'train_set_df')\n",
    "print(f'train_set_df time: {time.time() - s_time}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_categories_schema = StructType(\n",
    "    [StructField(\"document_id_cat\", IntegerType(), True),\n",
    "     StructField(\"category_id\", IntegerType(), True),\n",
    "     StructField(\"confidence_level_cat\", FloatType(), True)]\n",
    ")\n",
    "\n",
    "documents_categories_df = spark.read.schema(documents_categories_schema) \\\n",
    "    .options(header='true', inferschema='false', nullValue='\\\\N') \\\n",
    "    .csv(DATA_BUCKET_FOLDER + \"documents_categories.csv\") \\\n",
    "    .alias('documents_categories')\n",
    "\n",
    "documents_topics_schema = StructType(\n",
    "    [StructField(\"document_id_top\", IntegerType(), True),\n",
    "     StructField(\"topic_id\", IntegerType(), True),\n",
    "     StructField(\"confidence_level_top\", FloatType(), True)]\n",
    ")\n",
    "\n",
    "documents_topics_df = spark.read.schema(documents_topics_schema) \\\n",
    "    .options(header='true', inferschema='false', nullValue='\\\\N') \\\n",
    "    .csv(DATA_BUCKET_FOLDER + \"documents_topics.csv\") \\\n",
    "    .alias('documents_topics')\n",
    "\n",
    "documents_entities_schema = StructType(\n",
    "    [StructField(\"document_id_ent\", IntegerType(), True),\n",
    "     StructField(\"entity_id\", StringType(), True),\n",
    "     StructField(\"confidence_level_ent\", FloatType(), True)]\n",
    ")\n",
    "\n",
    "documents_entities_df = spark.read.schema(documents_entities_schema) \\\n",
    "    .options(header='true', inferschema='false', nullValue='\\\\N') \\\n",
    "    .csv(DATA_BUCKET_FOLDER + \"documents_entities.csv\") \\\n",
    "    .alias('documents_entities')\n",
    "\n",
    "documents_categories_grouped_df = documents_categories_df.groupBy('document_id_cat') \\\n",
    "    .agg(F.collect_list('category_id').alias('category_id_list'),\n",
    "         F.collect_list('confidence_level_cat').alias('confidence_level_cat_list')) \\\n",
    "    .alias('documents_categories_grouped').cache()\n",
    "\n",
    "documents_topics_grouped_df = documents_topics_df.groupBy('document_id_top') \\\n",
    "    .agg(F.collect_list('topic_id').alias('topic_id_list'),\n",
    "         F.collect_list('confidence_level_top').alias('confidence_level_top_list')) \\\n",
    "    .alias('documents_topics_grouped').cache()\n",
    "\n",
    "documents_entities_grouped_df = documents_entities_df.groupBy('document_id_ent') \\\n",
    "    .agg(F.collect_list('entity_id').alias('entity_id_list'),\n",
    "         F.collect_list('confidence_level_ent').alias('confidence_level_ent_list')) \\\n",
    "    .alias('documents_entities_grouped').cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################## generate dictionary ##################################\n",
    "country_value_cat = events_df.select('event_country').groupBy('event_country').count().filter('event_country is not null and count >= 10')\n",
    "\n",
    "state_value_cal = events_df.select('event_country_state').groupBy('event_country_state').count().filter('event_country_state is not null and count >= 10')\n",
    "\n",
    "geo_location_value_cat = events_df.select('geo_location_event').groupBy('geo_location_event').count().filter('geo_location_event is not null and count >= 10')\n",
    "\n",
    "\n",
    "# ### Average CTR by ad_id\n",
    "ad_id_popularity_df = train_set_df \\\n",
    "    .groupby('ad_id') \\\n",
    "    .agg(F.sum('clicked').alias('clicks'),F.count('*').alias('views')) \\\n",
    "    .withColumn('ctr', F.col('clicks') / F.col('views')) \\\n",
    "    .filter('views > 5').select('ad_id', 'ctr', 'views')\n",
    "\n",
    "# ### Average CTR by document_id (promoted_content)\n",
    "document_id_popularity_df = train_set_df \\\n",
    "    .groupby('document_id_promo') \\\n",
    "    .agg(F.sum('clicked').alias('clicks'), F.count('*').alias('views')) \\\n",
    "    .withColumn('ctr', F.col('clicks') / F.col('views')) \\\n",
    "    .filter('views > 5').select('document_id_promo', 'ctr', 'views')\n",
    "\n",
    "# ### Average CTR by source_id\n",
    "source_id_popularity_df = train_set_df.select('clicked', 'source_id', 'ad_id') \\\n",
    "    .groupby('source_id').agg(F.sum('clicked').alias('clicks'), F.count('*').alias('views')) \\\n",
    "    .withColumn('ctr', F.col('clicks') / F.col('views')) \\\n",
    "    .filter('views > 10 and source_id is not null').select('source_id', 'ctr', 'views')\n",
    "\n",
    "# ### Average CTR by publisher_id\n",
    "publisher_popularity_df = train_set_df.select('clicked', 'publisher_id', 'ad_id') \\\n",
    "    .groupby('publisher_id').agg(F.sum('clicked').alias('clicks'), F.count('*').alias('views')) \\\n",
    "    .withColumn('ctr', F.col('clicks') / F.col('views')) \\\n",
    "    .filter('views > 10 and publisher_id is not null').select('publisher_id', 'ctr', 'views')\n",
    "\n",
    "# ### Average CTR by advertiser_id\n",
    "advertiser_id_popularity_df = train_set_df.select('clicked', 'advertiser_id', 'ad_id') \\\n",
    "    .groupby('advertiser_id').agg(F.sum('clicked').alias('clicks'), F.count('*').alias('views')) \\\n",
    "    .withColumn('ctr', F.col('clicks') / F.col('views')) \\\n",
    "    .filter('views > 10 and advertiser_id is not null').select('advertiser_id', 'ctr', 'views')\n",
    "\n",
    "# ### Average CTR by campaign_id\n",
    "campaign_id_popularity_df = train_set_df.select('clicked', 'campaign_id', 'ad_id') \\\n",
    "    .groupby('campaign_id').agg(F.sum('clicked').alias('clicks'), F.count('*').alias('views')) \\\n",
    "    .withColumn('ctr', F.col('clicks') / F.col('views')) \\\n",
    "    .filter('views > 10 and campaign_id is not null').select('campaign_id', 'ctr', 'views')\n",
    "\n",
    "\n",
    "categories_docs_counts = documents_categories_df.groupBy('category_id').count().rdd.collectAsMap()\n",
    "\n",
    "topics_docs_counts = documents_topics_df.groupBy('topic_id').count().rdd.collectAsMap()\n",
    "\n",
    "entities_docs_counts = documents_entities_df.groupBy('entity_id').count().rdd.collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################### udf used for feature engineering ###############################\n",
    "def cosine_similarity_dicts(dict1, dict2):\n",
    "    dict1_norm = math.sqrt(sum([v ** 2 for v in dict1.values()]))\n",
    "    dict2_norm = math.sqrt(sum([v ** 2 for v in dict2.values()]))\n",
    "\n",
    "    sum_common_aspects = 0.0\n",
    "    intersections = 0\n",
    "    for key in dict1:\n",
    "        if key in dict2:\n",
    "            sum_common_aspects += dict1[key] * dict2[key]\n",
    "            intersections += 1\n",
    "\n",
    "    return sum_common_aspects / (dict1_norm * dict2_norm), intersections\n",
    "\n",
    "\n",
    "def cosine_similarity_doc_event_doc_ad_aspects(doc_event_aspect_ids, doc_event_aspects_confidence,\n",
    "                                               doc_ad_aspect_ids, doc_ad_aspects_confidence,\n",
    "                                               aspect_docs_counts):\n",
    "    if doc_event_aspect_ids is None or len(doc_event_aspect_ids) == 0 \\\n",
    "            or doc_ad_aspect_ids is None or len(doc_ad_aspect_ids) == 0:\n",
    "        return None, None\n",
    "\n",
    "    doc_event_aspects = dict(zip(doc_event_aspect_ids, doc_event_aspects_confidence))\n",
    "    doc_event_aspects_tfidf_confid = {}\n",
    "    for key in doc_event_aspect_ids:\n",
    "        tf = 1.0\n",
    "        idf = math.log(math.log(documents_total / float(aspect_docs_counts[key])))\n",
    "        confidence = doc_event_aspects[key]\n",
    "        doc_event_aspects_tfidf_confid[key] = tf * idf * confidence\n",
    "\n",
    "    doc_ad_aspects = dict(zip(doc_ad_aspect_ids, doc_ad_aspects_confidence))\n",
    "    doc_ad_aspects_tfidf_confid = {}\n",
    "    for key in doc_ad_aspect_ids:\n",
    "        tf = 1.0\n",
    "        idf = math.log(math.log(documents_total / float(aspect_docs_counts[key])))\n",
    "        confidence = doc_ad_aspects[key]\n",
    "        doc_ad_aspects_tfidf_confid[key] = tf * idf * confidence\n",
    "\n",
    "    similarity, intersections = cosine_similarity_dicts(doc_event_aspects_tfidf_confid, doc_ad_aspects_tfidf_confid)\n",
    "\n",
    "    if intersections > 0:\n",
    "        # P(A intersect B)_intersections = P(A)^intersections * P(B)^intersections\n",
    "        random_error = math.pow(len(doc_event_aspect_ids) / float(len(aspect_docs_counts)),\n",
    "                                intersections) * math.pow(len(doc_ad_aspect_ids) / float(len(aspect_docs_counts)),\n",
    "                                                          intersections)\n",
    "    else:\n",
    "        # P(A not intersect B) = 1 - P(A intersect B)\n",
    "        random_error = 1 - ((len(doc_event_aspect_ids) / float(len(aspect_docs_counts))) *\n",
    "                            (len(doc_ad_aspect_ids) / float(len(aspect_docs_counts))))\n",
    "\n",
    "    confidence = 1.0 - random_error\n",
    "\n",
    "    return similarity, confidence\n",
    "\n",
    "def convert_odd_timestamp(timestamp_ms_relative):\n",
    "    TIMESTAMP_DELTA = 1465876799998\n",
    "    return datetime.datetime.fromtimestamp((int(timestamp_ms_relative) + TIMESTAMP_DELTA) // 1000)\n",
    "\n",
    "def timestamp_delta(df, publish_time, timestamp):\n",
    "    def timestamp_delta_udf(publish_time, timestamp):\n",
    "        if timestamp > -1:\n",
    "            dt_timestamp_event = convert_odd_timestamp(timestamp)\n",
    "            if publish_time is not None:\n",
    "                delta_days = (dt_timestamp_event - publish_time).days\n",
    "                if 0 <= delta_days <= 365 * 10:  # 10 years\n",
    "                    return float(delta_days)\n",
    "    udf_inter = F.udf(lambda publish_time, timestamp: timestamp_delta_udf(publish_time, timestamp), DoubleType())\n",
    "    df = df.withColumn(publish_time + '_delta', udf_inter(publish_time, timestamp))\n",
    "    return df\n",
    "\n",
    "# Setting Doc_event-doc_ad CB Similarity fields\n",
    "def get_doc_event_doc_ad_cb_similarity_score_fn(df, doc_event_ids, doc_event_levels, doc_ad_ids, doc_ad_levels, cnt):\n",
    "    udf_inter = F.udf(\n",
    "        lambda doc_event_ids, doc_event_levels, doc_ad_ids, doc_ad_levels: \n",
    "        cosine_similarity_doc_event_doc_ad_aspects(doc_event_ids, doc_event_levels, doc_ad_ids, doc_ad_levels, cnt)[0], DoubleType())\n",
    "    df = df.withColumn(doc_event_ids + '_sim', udf_inter(doc_event_ids, doc_event_levels, doc_ad_ids, doc_ad_levels))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enrich_df(df):\n",
    "    df_enriched = df \\\n",
    "        .join(documents_categories_grouped_df,\n",
    "          on=F.col(\"document_id_promo\") == F.col(\"documents_categories_grouped.document_id_cat\"),\n",
    "          how='left') \\\n",
    "        .join(documents_topics_grouped_df,\n",
    "          on=F.col(\"document_id_promo\") == F.col(\"documents_topics_grouped.document_id_top\"),\n",
    "          how='left') \\\n",
    "        .join(documents_entities_grouped_df,\n",
    "          on=F.col(\"document_id_promo\") == F.col(\"documents_entities_grouped.document_id_ent\"),\n",
    "          how='left') \\\n",
    "        .join(documents_categories_grouped_df\n",
    "          .withColumnRenamed('category_id_list', 'doc_event_category_id_list')\n",
    "          .withColumnRenamed('confidence_level_cat_list', 'doc_event_confidence_level_cat_list')\n",
    "          .alias('documents_event_categories_grouped'),\n",
    "          on=F.col(\"document_id_event\") == F.col(\"documents_event_categories_grouped.document_id_cat\"),\n",
    "          how='left') \\\n",
    "        .join(documents_topics_grouped_df\n",
    "          .withColumnRenamed('topic_id_list', 'doc_event_topic_id_list')\n",
    "          .withColumnRenamed('confidence_level_top_list', 'doc_event_confidence_level_top_list')\n",
    "          .alias('documents_event_topics_grouped'),\n",
    "          on=F.col(\"document_id_event\") == F.col(\"documents_event_topics_grouped.document_id_top\"),\n",
    "          how='left') \\\n",
    "        .join(documents_entities_grouped_df\n",
    "          .withColumnRenamed('entity_id_list', 'doc_event_entity_id_list')\n",
    "          .withColumnRenamed('confidence_level_ent_list', 'doc_event_confidence_level_ent_list')\n",
    "          .alias('documents_event_entities_grouped'),\n",
    "          on=F.col(\"document_id_event\") == F.col(\"documents_event_entities_grouped.document_id_ent\"),\n",
    "          how='left') \\\n",
    "        .select('display_id', 'uuid_event', 'event_country', 'event_country_state', 'platform_event',\n",
    "            'source_id_doc_event', 'publisher_doc_event', 'publish_time_doc_event',\n",
    "            'publish_time', 'ad_id', 'document_id_promo', 'clicked',\n",
    "            'geo_location_event', 'advertiser_id', 'publisher_id',\n",
    "            'campaign_id', 'document_id_event',\n",
    "            F.coalesce(\"doc_event_category_id_list\", F.array())\n",
    "            .alias('doc_event_category_id_list'),\n",
    "            F.coalesce(\"doc_event_confidence_level_cat_list\", F.array())\n",
    "            .alias('doc_event_confidence_level_cat_list'),\n",
    "            F.coalesce(\"doc_event_topic_id_list\", F.array())\n",
    "            .alias('doc_event_topic_id_list'),\n",
    "            F.coalesce(\"doc_event_confidence_level_top_list\", F.array())\n",
    "            .alias('doc_event_confidence_level_top_list'),\n",
    "            F.coalesce(\"doc_event_entity_id_list\", F.array())\n",
    "            .alias('doc_event_entity_id_list'),\n",
    "            F.coalesce(\"doc_event_confidence_level_ent_list\", F.array())\n",
    "            .alias('doc_event_confidence_level_ent_list'),\n",
    "            F.coalesce(\"source_id\", F.lit(-1)).alias('source_id'),\n",
    "            F.coalesce(\"timestamp_event\", F.lit(-1)).alias('timestamp_event'),\n",
    "            F.coalesce(\"category_id_list\", F.array()).alias('category_id_list'),\n",
    "            F.coalesce(\"confidence_level_cat_list\", F.array())\n",
    "            .alias('confidence_level_cat_list'),\n",
    "            F.coalesce(\"topic_id_list\", F.array()).alias('topic_id_list'),\n",
    "            F.coalesce(\"confidence_level_top_list\", F.array())\n",
    "            .alias('confidence_level_top_list'),\n",
    "            F.coalesce(\"entity_id_list\", F.array()).alias('entity_id_list'),\n",
    "            F.coalesce(\"confidence_level_ent_list\", F.array())\n",
    "            .alias('confidence_level_ent_list'))\n",
    "    df_enriched = df_enriched.fillna(-1, subset=['source_id', 'timestamp_event'])\n",
    "    return df_enriched\n",
    "\n",
    "train_set_enriched_df = enrich_df(train_set_df)\n",
    "s_time = time.time()\n",
    "train_set_enriched_df.write.format('parquet').mode('overwrite').save(OUTPUT_BUCKET_FOLDER + 'train_set_enriched_df')\n",
    "train_set_enriched_df = spark.read.parquet(OUTPUT_BUCKET_FOLDER + 'train_set_enriched_df')\n",
    "print(f'train_set_enriched_df time: {time.time() - s_time}')\n",
    "\n",
    "test_set_enriched_df = enrich_df(valid_set_df)\n",
    "s_time = time.time()\n",
    "test_set_enriched_df.write.format('parquet').mode('overwrite').save(OUTPUT_BUCKET_FOLDER + 'test_set_enriched_df')\n",
    "test_set_enriched_df = spark.read.parquet(OUTPUT_BUCKET_FOLDER + 'test_set_enriched_df')\n",
    "print(f'test_set_enriched_df time: {time.time() - s_time}')\n",
    "\n",
    "documents_categories_grouped_df.unpersist()\n",
    "documents_topics_grouped_df.unpersist()\n",
    "documents_entities_grouped_df.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_number(element, name):\n",
    "    if name in BOOL_COLUMNS + CATEGORICAL_COLUMNS:\n",
    "        return element.cast(\"int\")\n",
    "    else:\n",
    "        return element\n",
    "\n",
    "FEAT_CSV_ORDERED_COLUMNS = ['ad_views', 'campaign_id','doc_views',\n",
    "                            'doc_event_days_since_published', 'doc_ad_days_since_published',\n",
    "                            'pop_ad_id', 'pop_document_id', 'pop_publisher_id', 'pop_advertiser_id', 'pop_campain_id',\n",
    "                            'pop_source_id',\n",
    "                            'doc_event_doc_ad_sim_categories', 'doc_event_doc_ad_sim_topics',\n",
    "                            'doc_event_doc_ad_sim_entities', 'ad_advertiser', 'doc_ad_publisher_id',\n",
    "                            'doc_ad_source_id', 'doc_event_publisher_id', 'doc_event_source_id', 'event_country',\n",
    "                            'event_country_state', 'event_geo_location', 'event_platform',\n",
    "                            'traffic_source']\n",
    "feature_vector_labels = ['ad_id_views', 'campaign_id','document_id_promo_views',\n",
    "                            'publish_time_doc_event_delta', 'publish_time_delta', \n",
    "                            'ad_id_ctr', 'document_id_promo_ctr', 'publisher_id_ctr', \n",
    "                            'advertiser_id_ctr', 'campaign_id_ctr', 'source_id_ctr', \n",
    "                            'doc_event_category_id_list_sim', 'doc_event_topic_id_list_sim',\n",
    "                            'doc_event_entity_id_list_sim', \n",
    "                            'advertiser_id', 'publisher_id', 'source_id', 'publisher_doc_event', 'source_id_doc_event', \n",
    "                            'event_country_count', 'event_country_state_count', 'geo_location_event_count', 'platform_event', \n",
    "                            'traffic_source']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorifyFeatures(df, proc, output_name=\"categorify\", gen_dict=False):\n",
    "    categorify_cols = ['ad_id_views', 'document_id_promo_views',\n",
    "                            'ad_id_ctr', 'document_id_promo_ctr', 'publisher_id_ctr', \n",
    "                            'advertiser_id_ctr', 'campaign_id_ctr', 'source_id_ctr',\n",
    "                            'event_country_count', 'event_country_state_count', 'geo_location_event_count']\n",
    "    to_categorify_cols = ['ad_id', 'document_id_promo',\n",
    "                            'ad_id', 'document_id_promo', 'publisher_id', \n",
    "                            'advertiser_id', 'campaign_id', 'source_id',\n",
    "                            'event_country', 'event_country_state', 'geo_location_event']\n",
    "    \n",
    "    # transform dict column name to match recdp\n",
    "    dfs = [ad_id_popularity_df.select(F.col('ad_id').alias('dict_col'), F.col('views').alias('dict_col_id')), \n",
    "            document_id_popularity_df.select(F.col('document_id_promo').alias('dict_col'), F.col('views').alias('dict_col_id')), \n",
    "            ad_id_popularity_df.select(F.col('ad_id').alias('dict_col'), F.col('ctr').alias('dict_col_id')), \n",
    "            document_id_popularity_df.select(F.col('document_id_promo').alias('dict_col'), F.col('ctr').alias('dict_col_id')), \n",
    "            publisher_popularity_df.select(F.col('publisher_id').alias('dict_col'), F.col('ctr').alias('dict_col_id')), \n",
    "            advertiser_id_popularity_df.select(F.col('advertiser_id').alias('dict_col'), F.col('ctr').alias('dict_col_id')), \n",
    "            campaign_id_popularity_df.select(F.col('campaign_id').alias('dict_col'), F.col('ctr').alias('dict_col_id')), \n",
    "            source_id_popularity_df.select(F.col('source_id').alias('dict_col'), F.col('ctr').alias('dict_col_id')), \n",
    "            country_value_cat.select(F.col('event_country').alias('dict_col'), F.col('count').alias('dict_col_id')), \n",
    "            state_value_cal.select(F.col('event_country_state').alias('dict_col'), F.col('count').alias('dict_col_id')), \n",
    "            geo_location_value_cat.select(F.col('geo_location_event').alias('dict_col'), F.col('count').alias('dict_col_id'))]\n",
    "\n",
    "    dict_dfs = [{'col_name': name, 'dict': dfs[index]} for index, name in enumerate(categorify_cols)]\n",
    "    # add new columns since recdp will overwrite cat columns\n",
    "    for index, column in enumerate(categorify_cols):\n",
    "        df = df.withColumn(column, F.col(to_categorify_cols[index]))\n",
    "\n",
    "    op_categorify = Categorify(categorify_cols, dict_dfs=dict_dfs)\n",
    "    op_fillna = FillNA(['event_country_count', 'event_country_state_count', 'geo_location_event_count'], 0)\n",
    "    op_feature_modify = FeatureModification(cols={\"platform_event\": \"f.col('platform_event') - 1\"} , op='inline')\n",
    "    proc.reset_ops([op_categorify, op_fillna, op_feature_modify])\n",
    "    t1 = timer()\n",
    "    df = proc.transform(df, name=output_name)\n",
    "    t2 = timer()\n",
    "    print(\"Categorify took %.3f\" % (t2 - t1))\n",
    "\n",
    "    return df\n",
    "\n",
    "def convertType(df, proc, output_name=\"convertType\"):\n",
    "    op_feature_type_convert = FeatureModification(\n",
    "        cols={\"campaign_id\": \"f.col('campaign_id').cast(spk_type.DoubleType())\", \n",
    "             \"advertiser_id\": \"f.col('advertiser_id').cast(spk_type.DoubleType())\", \n",
    "             \"source_id\": \"f.col('source_id').cast(spk_type.DoubleType())\", \n",
    "             \"publisher_id\": \"f.col('publisher_id').cast(spk_type.DoubleType())\", \n",
    "             \"source_id_doc_event\": \"f.col('source_id_doc_event').cast(spk_type.DoubleType())\", \n",
    "             \"publisher_doc_event\": \"f.col('publisher_doc_event').cast(spk_type.DoubleType())\"\n",
    "             }, op='inline')\n",
    "    proc.reset_ops([op_feature_type_convert])\n",
    "    t1 = timer()\n",
    "    df = proc.transform(df, name=output_name)\n",
    "    t2 = timer()\n",
    "    print(\"Convert type took %.3f\" % (t2 - t1))\n",
    "    \n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################## feature engineer with RecDP ################################\n",
    "path_prefix = \"hdfs://\"\n",
    "current_path = \"/wnd/\"\n",
    "proc = DataProcessor(spark, path_prefix, current_path=current_path, shuffle_disk_capacity=\"1200GB\")\n",
    "\n",
    "######################### trainset feature engineer ###############################\n",
    "train_set_features_df = train_set_enriched_df\n",
    "train_set_features_df = categorifyFeatures(train_set_features_df, proc, output_name=\"train_categorified\", gen_dict=False)\n",
    "train_set_features_df = timestamp_delta(train_set_features_df, 'publish_time', 'timestamp_event')\n",
    "train_set_features_df = timestamp_delta(train_set_features_df, 'publish_time_doc_event', 'timestamp_event')\n",
    "\n",
    "train_set_features_df = get_doc_event_doc_ad_cb_similarity_score_fn(\n",
    "    train_set_features_df, 'doc_event_category_id_list', 'doc_event_confidence_level_cat_list', \n",
    "    'category_id_list', 'confidence_level_cat_list', categories_docs_counts)\n",
    "train_set_features_df = get_doc_event_doc_ad_cb_similarity_score_fn(\n",
    "    train_set_features_df, 'doc_event_topic_id_list', 'doc_event_confidence_level_top_list',\n",
    "    'topic_id_list', 'confidence_level_top_list', topics_docs_counts)\n",
    "train_set_features_df = get_doc_event_doc_ad_cb_similarity_score_fn(\n",
    "    train_set_features_df, 'doc_event_entity_id_list', 'doc_event_confidence_level_ent_list', \n",
    "    'entity_id_list', 'confidence_level_ent_list', entities_docs_counts)\n",
    "\n",
    "train_set_features_df = convertType(train_set_features_df, proc, output_name=\"train_convert_type\")\n",
    "\n",
    "train_set_features_df = train_set_features_df \\\n",
    "    .withColumn('traffic_source', F.lit(0).cast(DoubleType())) \\\n",
    "    .withColumnRenamed('document_id_promo', 'document_id') \\\n",
    "    .withColumnRenamed('clicked', 'label')\n",
    "\n",
    "train_set_features_df = train_set_features_df.fillna(0, subset=feature_vector_labels)\n",
    "\n",
    "train_feature_vectors_integral_csv_rdd_df = train_set_features_df.select(\n",
    "    ['label'] + ['display_id'] + ['ad_id'] + [F.col('document_id').alias('doc_id')] + [F.col('document_id_event').alias('doc_event_id')] + [\n",
    "        format_number(element, FEAT_CSV_ORDERED_COLUMNS[index]).alias(FEAT_CSV_ORDERED_COLUMNS[index]) for\n",
    "        index, element in enumerate([F.col(column) for column in feature_vector_labels])]).replace(\n",
    "    float('nan'), 0)\n",
    "\n",
    "\n",
    "######################### testset feature engineer ###############################\n",
    "test_set_features_df = test_set_enriched_df\n",
    "test_set_features_df = categorifyFeatures(test_set_features_df, proc, output_name=\"test_categorified\", gen_dict=False)\n",
    "test_set_features_df = timestamp_delta(test_set_features_df, 'publish_time', 'timestamp_event')\n",
    "test_set_features_df = timestamp_delta(test_set_features_df, 'publish_time_doc_event', 'timestamp_event')\n",
    "\n",
    "test_set_features_df = get_doc_event_doc_ad_cb_similarity_score_fn(\n",
    "    test_set_features_df, 'doc_event_category_id_list', 'doc_event_confidence_level_cat_list', \n",
    "    'category_id_list', 'confidence_level_cat_list', categories_docs_counts)\n",
    "test_set_features_df = get_doc_event_doc_ad_cb_similarity_score_fn(\n",
    "    test_set_features_df, 'doc_event_topic_id_list', 'doc_event_confidence_level_top_list',\n",
    "    'topic_id_list', 'confidence_level_top_list', topics_docs_counts)\n",
    "test_set_features_df = get_doc_event_doc_ad_cb_similarity_score_fn(\n",
    "    test_set_features_df, 'doc_event_entity_id_list', 'doc_event_confidence_level_ent_list', \n",
    "    'entity_id_list', 'confidence_level_ent_list', entities_docs_counts)\n",
    "\n",
    "test_set_features_df = convertType(test_set_features_df, proc, output_name=\"test_convert_type\")\n",
    "\n",
    "test_set_features_df = test_set_features_df \\\n",
    "    .withColumn('traffic_source', F.lit(0).cast(DoubleType())) \\\n",
    "    .withColumnRenamed('document_id_promo', 'document_id') \\\n",
    "    .withColumnRenamed('clicked', 'label')\n",
    "\n",
    "test_set_features_df = test_set_features_df.fillna(0, subset=feature_vector_labels)\n",
    "\n",
    "test_validation_feature_vectors_integral_csv_rdd_df = test_set_features_df.repartition(40,'display_id').orderBy('display_id').select(\n",
    "    ['label'] + ['display_id'] + ['ad_id'] + [F.col('document_id').alias('doc_id')] + [F.col('document_id_event').alias('doc_event_id')] + [\n",
    "        format_number(element, FEAT_CSV_ORDERED_COLUMNS[index]).alias(FEAT_CSV_ORDERED_COLUMNS[index]) for\n",
    "        index, element in enumerate([F.col(column) for column in feature_vector_labels])]).replace(\n",
    "    float('nan'), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log2_1p(x):\n",
    "    return np.log1p(x) / np.log(2.0)\n",
    "\n",
    "\n",
    "# calculate min and max stats for the given dataframes all in one go\n",
    "def compute_min_max_logs(df):\n",
    "    print(str(datetime.datetime.now()) + '\\tComputing min and max')\n",
    "    min_logs = {}\n",
    "    max_logs = {}\n",
    "    float_expr = []\n",
    "    for name in FLOAT_COLUMNS_LOG_BIN_TRANSFORM + INT_COLUMNS:\n",
    "        float_expr.append(F.min(name))\n",
    "        float_expr.append(F.max(name))\n",
    "    floatDf = all_df.agg(*float_expr).collect()\n",
    "    print(floatDf)\n",
    "    for name in FLOAT_COLUMNS_LOG_BIN_TRANSFORM:\n",
    "        minAgg = floatDf[0][\"min(\" + name + \")\"]\n",
    "        maxAgg = floatDf[0][\"max(\" + name + \")\"]\n",
    "        min_logs[name + '_log_01scaled'] = log2_1p(minAgg * 1000)\n",
    "        max_logs[name + '_log_01scaled'] = log2_1p(maxAgg * 1000)\n",
    "    for name in INT_COLUMNS:\n",
    "        minAgg = floatDf[0][\"min(\" + name + \")\"]\n",
    "        maxAgg = floatDf[0][\"max(\" + name + \")\"]\n",
    "        min_logs[name + '_log_01scaled'] = log2_1p(minAgg)\n",
    "        max_logs[name + '_log_01scaled'] = log2_1p(maxAgg)\n",
    "\n",
    "    return min_logs, max_logs\n",
    "\n",
    "def log_and_norm(df, proc, output_name=\"log_norm\", gen_dict=False):\n",
    "    log_cols = {col + '_log_01': f'f.log1p(\"{col}\")' for col in INT_COLUMNS}\n",
    "    norm_cols = {col + 'scaled': f'(f.col(\"{col}\")-{min_logs[col+\"scaled\"]}) / ({max_logs[col+\"scaled\"]-min_logs[col+\"scaled\"]})' for col in log_cols.keys()}\n",
    "    op_log = FeatureAdd(cols=log_cols, op='inline')\n",
    "    op_norm = FeatureAdd(cols=norm_cols, op='inline')\n",
    "    proc.reset_ops([op_log, op_norm])\n",
    "\n",
    "    op_fillna = FillNA(CATEGORICAL_COLUMNS, 0)\n",
    "    proc.append_ops([op_fillna])\n",
    "\n",
    "    hash_bucket_cols = {col: f'f.col(\"{col}\") % {size}' for col, size in HASH_BUCKET_SIZES.items()}\n",
    "    op_hash_bucket = FeatureModification(cols=hash_bucket_cols, op='inline')\n",
    "    proc.append_ops([op_hash_bucket])\n",
    "    \n",
    "    t1 = timer()\n",
    "    df = proc.transform(df, name=output_name)\n",
    "    t2 = timer()\n",
    "    print(\"Log and norm took %.3f\" % (t2 - t1))\n",
    "\n",
    "    for name, size in HASH_BUCKET_SIZES.items():\n",
    "        df = df.withColumn(name, F.when(F.col(name)<0, F.col(name)+size).otherwise(F.col(name)))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = test_validation_feature_vectors_integral_csv_rdd_df.union(train_feature_vectors_integral_csv_rdd_df)\n",
    "min_logs, max_logs = compute_min_max_logs(all_df)\n",
    "\n",
    "train_feature_norm = log_and_norm(train_feature_vectors_integral_csv_rdd_df, proc, output_name=\"log_norm_train\")\n",
    "test_feature_norm = log_and_norm(test_validation_feature_vectors_integral_csv_rdd_df, proc, output_name=\"log_norm_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_spec(output_dir, batch_size=None):\n",
    "    fixed_shape = [batch_size, 1] if batch_size is not None else []\n",
    "    spec = {}\n",
    "    spec[LABEL_COLUMN] = tf.io.FixedLenFeature(shape=fixed_shape, dtype=tf.int64, default_value=None)\n",
    "    spec[DISPLAY_ID_COLUMN] = tf.io.FixedLenFeature(shape=fixed_shape, dtype=tf.int64, default_value=None)\n",
    "    for name in BOOL_COLUMNS:\n",
    "        spec[name] = tf.io.FixedLenFeature(shape=fixed_shape, dtype=tf.int64, default_value=None)\n",
    "    for name in INT_COLUMNS:\n",
    "        spec[name + '_log_01scaled'] = tf.io.FixedLenFeature(shape=fixed_shape, dtype=tf.float32, default_value=None)\n",
    "    for name in BOOL_COLUMNS + CATEGORICAL_COLUMNS:\n",
    "        spec[name] = tf.io.FixedLenFeature(shape=fixed_shape, dtype=tf.int64, default_value=None)\n",
    "    metadata = dataset_metadata.DatasetMetadata(dataset_schema.from_feature_spec(spec))\n",
    "    metadata_io.write_metadata(metadata, output_dir)\n",
    "\n",
    "\n",
    "def create_tf_example_spark(df):\n",
    "    result = {}\n",
    "    result[LABEL_COLUMN] = tf.train.Feature(int64_list=tf.train.Int64List(value=df[LABEL_COLUMN].to_list()))\n",
    "    result[DISPLAY_ID_COLUMN] = tf.train.Feature(int64_list=tf.train.Int64List(value=df[DISPLAY_ID_COLUMN].to_list()))\n",
    "    for name in FLOAT_COLUMNS:\n",
    "        value = df[name].to_list()\n",
    "        result[name] = tf.train.Feature(float_list=tf.train.FloatList(value=value))\n",
    "    for name in INT_COLUMNS:\n",
    "        nn = name + '_log_01scaled'\n",
    "        value = df[nn].to_list()\n",
    "        result[nn] = tf.train.Feature(float_list=tf.train.FloatList(value=value))\n",
    "    for name in BOOL_COLUMNS + CATEGORICAL_COLUMNS:\n",
    "        value = df[name].to_list()\n",
    "        result[name] = tf.train.Feature(int64_list=tf.train.Int64List(value=value))\n",
    "    tf_example = tf.train.Example(features=tf.train.Features(feature=result))\n",
    "    return tf_example\n",
    "\n",
    "\n",
    "def _transform_to_slices(rdds):\n",
    "    taskcontext = TaskContext.get()\n",
    "    partitionid = taskcontext.partitionId()\n",
    "    csv = pd.DataFrame(list(rdds), columns=columns)\n",
    "    num_rows = len(csv.index)\n",
    "    examples = []\n",
    "    for start_ind in range(0, num_rows, batch_size if batch_size is not None else 1):  # for each batch\n",
    "        if start_ind + batch_size - 1 > num_rows:  # if we'd run out of rows\n",
    "            csv_slice = csv.iloc[start_ind:]\n",
    "            print(\"last Example has: \", len(csv_slice), partitionid)\n",
    "            examples.append((csv_slice, len(csv_slice)))\n",
    "            return examples\n",
    "        else:\n",
    "            csv_slice = csv.iloc[start_ind:start_ind + (batch_size if batch_size is not None else 1)]\n",
    "        examples.append((csv_slice, len(csv_slice)))\n",
    "    return examples\n",
    "\n",
    "\n",
    "def _transform_to_tfrecords_from_slices(rdds):\n",
    "    examples = []\n",
    "    for slice in rdds:\n",
    "        if len(slice[0]) != batch_size:\n",
    "            print(\"slice size is not correct, dropping: \", len(slice[0]))\n",
    "        else:\n",
    "            examples.append(\n",
    "                (bytearray((create_tf_example_spark(slice[0])).SerializeToString()), None))\n",
    "    return examples\n",
    "\n",
    "\n",
    "def _transform_to_tfrecords_from_reslice(rdds):\n",
    "    examples = []\n",
    "    all_dataframes = pd.DataFrame([])\n",
    "    for slice in rdds:\n",
    "        all_dataframes = all_dataframes.append(slice[0])\n",
    "    num_rows = len(all_dataframes.index)\n",
    "    examples = []\n",
    "    for start_ind in range(0, num_rows, batch_size if batch_size is not None else 1):  # for each batch\n",
    "        if start_ind + batch_size - 1 > num_rows:  # if we'd run out of rows\n",
    "            csv_slice = all_dataframes.iloc[start_ind:]\n",
    "            if TEST_SET_MODE:\n",
    "                remain_len = batch_size - len(csv_slice)\n",
    "                (m, n) = divmod(remain_len, len(csv_slice))\n",
    "                print(\"remainder: \", len(csv_slice), remain_len, m, n)\n",
    "                if m:\n",
    "                    for i in range(m):\n",
    "                        csv_slice = csv_slice.append(csv_slice)\n",
    "                csv_slice = csv_slice.append(csv_slice.iloc[:n])\n",
    "                print(\"after fill remainder: \", len(csv_slice))\n",
    "                examples.append(\n",
    "                    (bytearray((create_tf_example_spark(csv_slice)).SerializeToString()), None))\n",
    "                return examples\n",
    "            # drop the remainder\n",
    "            print(\"dropping remainder: \", len(csv_slice))\n",
    "            return examples\n",
    "        else:\n",
    "            csv_slice = all_dataframes.iloc[start_ind:start_ind + (batch_size if batch_size is not None else 1)]\n",
    "            examples.append(\n",
    "                (bytearray((create_tf_example_spark(csv_slice)).SerializeToString()), None))\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### convert data to TFRecods ######################\n",
    "pd.set_option('display.max_columns', 1000)\n",
    "evaluation = True\n",
    "evaluation_verbose = False\n",
    "LOCAL_DATA_TFRECORDS_DIR = \"/outbrain/tfrecords-test/recdp\"\n",
    "train_output_string = '/train'\n",
    "eval_output_string = '/eval'\n",
    "\n",
    "TEST_SET_MODE = False\n",
    "\n",
    "num_train_partitions = 40\n",
    "num_valid_partitions = 40\n",
    "batch_size = 4096\n",
    "# write out tfrecords meta\n",
    "make_spec(LOCAL_DATA_TFRECORDS_DIR + '/transformed_metadata', batch_size=batch_size)\n",
    "\n",
    "columns = train_feature_norm.columns\n",
    "\n",
    "TEST_SET_MODE = False\n",
    "train_features = train_feature_norm.coalesce(30).rdd.mapPartitions(_transform_to_slices)\n",
    "cached_train_features = train_features.cache()\n",
    "train_full = cached_train_features.filter(lambda x: x[1] == batch_size)\n",
    "# split out slies where we don't have a full batch so that we can reslice them so we only drop mininal rows\n",
    "train_not_full = cached_train_features.filter(lambda x: x[1] < batch_size)\n",
    "train_examples_full = train_full.mapPartitions(_transform_to_tfrecords_from_slices)\n",
    "train_left = train_not_full.coalesce(1).mapPartitions(_transform_to_tfrecords_from_reslice)\n",
    "all_train = train_examples_full.union(train_left)\n",
    "\n",
    "all_train.saveAsNewAPIHadoopFile(LOCAL_DATA_TFRECORDS_DIR + train_output_string,\n",
    "                                 \"org.tensorflow.hadoop.io.TFRecordFileOutputFormat\",\n",
    "                                 keyClass=\"org.apache.hadoop.io.BytesWritable\",\n",
    "                                 valueClass=\"org.apache.hadoop.io.NullWritable\")\n",
    "train_features.unpersist()\n",
    "\n",
    "\n",
    "valid_features = test_feature_norm.coalesce(num_valid_partitions).rdd.mapPartitions(_transform_to_slices)\n",
    "cached_valid_features = valid_features.cache()\n",
    "valid_full = cached_valid_features.filter(lambda x: x[1] == batch_size)\n",
    "valid_not_full = cached_valid_features.filter(lambda x: x[1] < batch_size)\n",
    "valid_examples_full = valid_full.mapPartitions(_transform_to_tfrecords_from_slices)\n",
    "valid_left = valid_not_full.coalesce(1).mapPartitions(_transform_to_tfrecords_from_reslice)\n",
    "all_valid = valid_examples_full.union(valid_left)\n",
    "\n",
    "all_valid.saveAsNewAPIHadoopFile(LOCAL_DATA_TFRECORDS_DIR + eval_output_string,\n",
    "                                 \"org.tensorflow.hadoop.io.TFRecordFileOutputFormat\",\n",
    "                                 keyClass=\"org.apache.hadoop.io.BytesWritable\",\n",
    "                                 valueClass=\"org.apache.hadoop.io.NullWritable\")\n",
    "valid_features.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:spark]",
   "language": "python",
   "name": "conda-env-spark-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "metadata": {
   "interpreter": {
    "hash": "11fce9c02c04210b4e393e56cc8683f43e45b3c4b54df7b32543429b3d427785"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}