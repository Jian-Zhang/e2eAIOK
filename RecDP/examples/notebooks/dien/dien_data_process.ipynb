{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import init\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql import *\n",
    "from pyspark import *\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.types import *\n",
    "from timeit import default_timer as timer\n",
    "import logging\n",
    "from pyrecdp.data_processor import *\n",
    "from pyrecdp.utils import *\n",
    "\n",
    "def load_csv(spark, path):\n",
    "    review_id_field = StructField('reviewer_id', StringType())\n",
    "    asin_field = StructField('asin', StringType())\n",
    "    overall_field = StructField('overall', FloatType())\n",
    "    unix_time_field = StructField('unix_review_time', IntegerType())\n",
    "    reviews_info_schema = StructType(\n",
    "        [review_id_field, asin_field, overall_field, unix_time_field])\n",
    "\n",
    "    category_field = StructField('category', StringType())\n",
    "    item_info_schema = StructType([asin_field, category_field])\n",
    "\n",
    "    reviews_info_df = spark.read.schema(reviews_info_schema).option('sep', '\\t').csv(path + \"/reviews-info\")\n",
    "    item_info_df = spark.read.schema(item_info_schema).option('sep', '\\t').csv(path + \"/item-info\")\n",
    "\n",
    "    return reviews_info_df, item_info_df\n",
    "\n",
    "\n",
    "def collapse_by_hist(df, item_info_df, proc, output_name, min_num_hist = 0, max_num_hist = 100):\n",
    "    op_model_merge = ModelMerge([{'col_name': 'asin', 'dict': item_info_df}])\n",
    "    op_collapse_by_hist = CollapseByHist(['asin', 'category'], by = ['reviewer_id'], orderBy = ['unix_review_time', 'asin'], minNumHist = min_num_hist, maxNumHist = 100)\n",
    "    proc.reset_ops([op_model_merge, op_collapse_by_hist])\n",
    "    \n",
    "    t1 = timer()\n",
    "    df = proc.transform(df, output_name)\n",
    "    t2 = timer()\n",
    "    print(f\"Merge with category and collapse hist took {t2 - t1} secs\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_dict_for_asin(df, proc):\n",
    "    dict_dfs = []\n",
    "    dict_names = ['asin']\n",
    "    dict_dfs = [{'col_name': name, 'dict': df.select(spk_func.col(name).alias('dict_col'))} for name in dict_names]\n",
    "    return dict_dfs\n",
    "\n",
    "\n",
    "def add_negative_sample(df, item_info_df, dict_dfs, proc, output_name):\n",
    "    # add negative_sample as new row\n",
    "    op_negative_sample = NegativeSample(['asin'], dict_dfs)\n",
    "    op_drop_category = DropFeature(['category'])\n",
    "    op_add_category = ModelMerge([{'col_name': 'asin', 'dict': item_info_df}])\n",
    "    op_select = SelectFeature(['pos', 'reviewer_id', 'asin', 'category', 'hist_asin', 'hist_category'])\n",
    "    proc.reset_ops([op_negative_sample, op_drop_category, op_add_category, op_select])\n",
    "    t1 = timer()\n",
    "    df = proc.transform(df, output_name)\n",
    "    t2 = timer()    \n",
    "    print(f\"add_negative_sample took {t2 - t1} secs\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_negative_hist_cols(df, item_info_df, dict_dfs, proc, output_name):\n",
    "    # add negative_sample as new row\n",
    "    op_columns_sample = NegativeFeature({'noclk_hist_asin': 'hist_asin'}, dict_dfs, doSplit=True, sep='\\x02', negCnt = 5)\n",
    "    proc.reset_ops([op_columns_sample])\n",
    "    t1 = timer()\n",
    "\n",
    "    df = proc.transform(df, output_name)\n",
    "    t2 = timer()    \n",
    "    print(f\"add_negative_hist_cols took {t2 - t1} secs\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_processed_csv(spark, data_dir):\n",
    "    label_field = StructField('pos', IntegerType())\n",
    "    review_id_field = StructField('reviewer_id', StringType())\n",
    "    asin_field = StructField('asin', StringType())\n",
    "    category_field = StructField('category', StringType())\n",
    "    hist_asin_field = StructField('hist_asin', StringType())\n",
    "    hist_category_field = StructField('hist_category', StringType())\n",
    "    csv_schema = StructType(\n",
    "        [label_field, review_id_field, asin_field, category_field, hist_asin_field, hist_category_field])\n",
    "\n",
    "    return spark.read.schema(csv_schema).option('sep', '\\t').csv(data_dir)\n",
    "\n",
    "\n",
    "def categorify_dien_data(df, user_df, asin_df, cat_df, asin_cat_df, proc, output_name):\n",
    "    df = df.select('pos', 'reviewer_id', 'asin', 'category', 'hist_asin', 'hist_category',\n",
    "                   'noclk_hist_asin', f.expr('noclk_hist_asin as noclk_hist_category'))\n",
    "\n",
    "    dict_dfs = []\n",
    "    dict_dfs.append({'col_name': 'reviewer_id', 'dict': user_df})\n",
    "    dict_dfs.append({'col_name': 'asin', 'dict': asin_df})\n",
    "    dict_dfs.append({'col_name': 'category', 'dict': cat_df})\n",
    "    dict_dfs.append({'col_name': 'hist_asin', 'dict': asin_df})\n",
    "    dict_dfs.append({'col_name': 'hist_category', 'dict': cat_df})\n",
    "    dict_dfs.append({'col_name': 'noclk_hist_asin', 'dict': asin_df})\n",
    "    dict_dfs.append({'col_name': 'noclk_hist_category', 'dict': asin_cat_df})\n",
    "\n",
    "    op_categorify = Categorify(['reviewer_id', 'asin', 'category'], dict_dfs=dict_dfs)\n",
    "    op_categorify_2 = Categorify(['hist_asin', 'hist_category'], dict_dfs=dict_dfs, doSplit=True, sep='\\x02')\n",
    "    op_categorify_3 = Categorify(['noclk_hist_asin', 'noclk_hist_category'], dict_dfs=dict_dfs, doSplit=True, multiLevelSplit=True, multiLevelSep=['|'])\n",
    "    op_fillna = FillNA(['asin', 'category'], 0)\n",
    "    proc.reset_ops([op_categorify, op_categorify_2, op_categorify_3, op_fillna])\n",
    "\n",
    "    t1 = timer()\n",
    "    df = proc.transform(df, output_name)\n",
    "    t2 = timer()\n",
    "    print(f\"categorify took {t2 - t1} secs\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def save_to_voc(df, proc, fmt, cols, default_name, default_v, output_name):\n",
    "    import pickle\n",
    "    col_name = ''\n",
    "    dtypes_list = []\n",
    "    if isinstance(cols, list):\n",
    "        col_name = cols[0]\n",
    "        dtypes_list = df.select(*cols).dtypes\n",
    "    elif isinstance(cols, str):\n",
    "        col_name = cols\n",
    "        dtypes_list = df.select(cols).dtypes\n",
    "    else:\n",
    "        raise ValueError(\"save_to_voc expects cols as String or list of String\")\n",
    "\n",
    "    to_select = []\n",
    "    if len(dtypes_list) == 1 and 'array' not in dtypes_list[0][1]:\n",
    "        to_select.append(f.col(dtypes_list[0][0]))\n",
    "        dict_df = df.select(*to_select)\n",
    "    else:\n",
    "        for name, dtype in dtypes_list:\n",
    "            if 'array' in dtype:\n",
    "                to_select.append(f.col(name))\n",
    "            else:\n",
    "                to_select.append(f.array(f.col(name)))\n",
    "\n",
    "        dict_df = df.withColumn(col_name, f.array_union(*to_select))\n",
    "        dict_df = dict_df.select(f.explode(f.col(col_name)).alias(col_name))\n",
    "    if fmt == 'pkl':\n",
    "        dict_df = dict_df.filter(f\"{col_name} is not null\").groupBy(col_name).count().orderBy(f.desc('count')).select(col_name)\n",
    "        collected = [row[col_name] for row in dict_df.collect()]\n",
    "\n",
    "        voc = {}\n",
    "        voc[default_name] = default_v\n",
    "        voc.update(dict((col_id, col_idx) for (col_id, col_idx) in zip(collected, range(1, len(collected) + 1))))\n",
    "        pickle.dump(voc, open(proc.current_path + f'/{output_name}.pkl', \"wb\"), protocol=0)\n",
    "    else:\n",
    "        op_gen_dict = GenerateDictionary([f\"{col_name}\"])\n",
    "        proc.reset_ops([op_gen_dict])\n",
    "        dict_dfs = proc.generate_dicts(dict_df)\n",
    "        dict_df = dict_dfs[0]['dict']\n",
    "        if fmt == 'adv_pkl':\n",
    "            voc_count = dict_df.count()\n",
    "            pickle.dump(voc_count, open(proc.current_path + f'/{output_name}.pkl', \"wb\"), protocol=0)\n",
    "\n",
    "    return dict_df\n",
    "\n",
    "\n",
    "def save_to_uid_voc(df, proc, fmt = 'pkl'):\n",
    "    # saving (using python)\n",
    "    # build uid_dict, mid_dict and cat_dict\n",
    "    t1 = timer()\n",
    "    dict_df = save_to_voc(df, proc, fmt, ['reviewer_id'], 'A1Y6U82N6TYZPI', 0, 'uid_voc')\n",
    "    t2 = timer()\n",
    "    print(f\"save_to_uid_voc took {t2 - t1} secs\")\n",
    "    return dict_df\n",
    "    \n",
    "\n",
    "def save_to_mid_voc(df, proc, fmt = 'pkl'):\n",
    "    t1 = timer()\n",
    "    dict_df = save_to_voc(df, proc, fmt, ['hist_asin', 'asin'], 'default_mid', 0, 'mid_voc')\n",
    "    t2 = timer()\n",
    "    print(f\"save_to_mid_voc took {t2 - t1} secs\")\n",
    "    return dict_df\n",
    "    \n",
    "    \n",
    "def save_to_cat_voc(df, proc, fmt = 'pkl'):\n",
    "    t1 = timer()\n",
    "    dict_df = save_to_voc(df, proc, fmt, ['hist_category', 'category'], 'default_cat', 0, 'cat_voc')\n",
    "    t2 = timer()\n",
    "    print(f\"save_to_cat_voc took {t2 - t1} secs\")\n",
    "    return dict_df\n",
    "    \n",
    "    \n",
    "def save_to_local_splitByUser(df, proc, output_name):\n",
    "    t1 = timer()\n",
    "    if 'noclk_hist_asin' in df.columns:\n",
    "        dict_df = df.select('pos', 'reviewer_id', 'asin', 'category',\n",
    "                            f.expr(\"concat_ws('\\x02', hist_asin)\"),\n",
    "                            f.expr(\"concat_ws('\\x02', hist_category)\"),\n",
    "                            f.expr(\"concat_ws('\\x02', noclk_hist_asin)\"),\n",
    "                            f.expr(\"concat_ws('\\x02', noclk_hist_category)\"))\n",
    "        collected = [[c1, c2, c3, c4, c5, c6, c7, c8] for (c1, c2, c3, c4, c5, c6, c7, c8) in dict_df.collect()]\n",
    "    else:\n",
    "        dict_df = df.select('pos', 'reviewer_id', 'asin', 'category',\n",
    "                            f.expr(\"concat_ws('\\x02', hist_asin)\"),\n",
    "                            f.expr(\"concat_ws('\\x02', hist_category)\"))\n",
    "        collected = [[c1, c2, c3, c4, c5, c6] for (c1, c2, c3, c4, c5, c6) in dict_df.collect()]\n",
    "    user_map = {}\n",
    "    for items in collected:\n",
    "        if items[1] not in user_map:\n",
    "            user_map[items[1]] = []\n",
    "        user_map[items[1]].append(items)\n",
    "    with open(proc.current_path + f\"/{output_name}\", 'w') as fp:\n",
    "        for user, r in user_map.items():\n",
    "            positive_sorted = sorted(r, key=lambda x: x[0])\n",
    "            for items in positive_sorted:\n",
    "                print('\\t'.join([str(x) for x in items]), file=fp)\n",
    "\n",
    "    t2 = timer()\n",
    "    print(f\"save_to_local_splitByUser took {t2 - t1} secs\")\n",
    "    return dict_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "option = '--basic'\n",
    "\n",
    "if option == '--basic':\n",
    "    fmt = 'pkl'\n",
    "elif option == '--advanced':\n",
    "    fmt = 'adv_pkl'\n",
    "else:\n",
    "    raise NotImplementedError(f'{option} is not recognized.')\n",
    "\n",
    "path_prefix = \"file://\"\n",
    "current_path = \"/home/vmagent/app/recdp/examples/python_tests/dien/output/\"\n",
    "original_folder = \"/home/vmagent/app/recdp/examples/python_tests/dien/j2c_test/\"\n",
    "scala_udf_jars = \"/home/vmagent/app/recdp/ScalaProcessUtils/target/recdp-scala-extensions-0.1.0-jar-with-dependencies.jar\"\n",
    "\n",
    "##### 1. Start spark and initialize data processor #####\n",
    "t0 = timer()\n",
    "spark = SparkSession.builder.master('local[104]')\\\n",
    "        .appName(\"dien_data_process\")\\\n",
    "        .config(\"spark.driver.memory\", \"480G\")\\\n",
    "        .config(\"spark.driver.memoryOverhead\", \"20G\")\\\n",
    "        .config(\"spark.executor.cores\", \"104\")\\\n",
    "        .config(\"spark.driver.extraClassPath\", f\"{scala_udf_jars}\")\\\n",
    "        .getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "    \n",
    "# 1.1 prepare dataFrames\n",
    "# 1.2 create RecDP DataProcessor\n",
    "proc = DataProcessor(spark, path_prefix, current_path=current_path, shuffle_disk_capacity=\"1200GB\", spark_mode='local')\n",
    "\n",
    "# ===============================================\n",
    "# basic: Do categorify for all columns\n",
    "reviews_info_df, item_info_df = load_csv(spark, path_prefix + original_folder)\n",
    "\n",
    "# 1. join records with its category and then collapse history \n",
    "df = reviews_info_df\n",
    "dict_dfs = get_dict_for_asin(df, proc)\n",
    "df = collapse_by_hist(df, item_info_df, proc, \"collapsed\", min_num_hist = 2, max_num_hist = 100)\n",
    "\n",
    "# 2. add negative sample to records\n",
    "df = add_negative_sample(df, item_info_df, dict_dfs, proc, \"records_with_negative_sample\")\n",
    "\n",
    "\n",
    "if fmt == 'adv_pkl':\n",
    "    dict_dfs[0]['col_name'] = 'hist_asin'\n",
    "    df = add_negative_hist_cols(df, item_info_df, dict_dfs, proc, \"records_with_negative_hists\")\n",
    "\n",
    "uid_dict_df = save_to_uid_voc(df, proc, fmt = fmt)\n",
    "mid_dict_df = save_to_mid_voc(df, proc, fmt = fmt)\n",
    "cat_dict_df = save_to_cat_voc(df, proc, fmt = fmt)\n",
    "if fmt == 'adv_pkl':\n",
    "    # for create noclk_hist_category, we should create a dict to mapping from asin to its cat_id\n",
    "    asin_df = item_info_df.withColumnRenamed('asin', 'dict_col')\n",
    "    cat_dict_for_merge_df = cat_dict_df.withColumnRenamed('dict_col', 'category')\n",
    "    op_asin_to_cat_id = ModelMerge([{'col_name': 'category', 'dict': cat_dict_for_merge_df}])\n",
    "    op_select = SelectFeature(['dict_col', 'dict_col_id', 'count'])\n",
    "    proc.reset_ops([op_asin_to_cat_id, op_select])\n",
    "    asin_cat_df = proc.apply(asin_df)\n",
    "\n",
    "    df = categorify_dien_data(df, uid_dict_df, mid_dict_df, cat_dict_df, asin_cat_df, proc, \"local_train_splitByUser.parquet\")\n",
    "    df = save_to_local_splitByUser(df, proc, 'local_train_splitByUser')\n",
    "\n",
    "    test_df = load_processed_csv(spark, path_prefix + original_folder + \"/local_test_splitByUser\")\n",
    "    test_df = add_negative_hist_cols(test_df, item_info_df, dict_dfs, proc, \"test_records_with_negative_hists\")\n",
    "    test_df = categorify_dien_data(test_df, uid_dict_df, mid_dict_df, cat_dict_df, asin_cat_df, proc, \"local_test_splitByUser.parquet\")\n",
    "    test_df = save_to_local_splitByUser(test_df, proc, 'local_test_splitByUser')\n",
    "else:\n",
    "    train_df = save_to_local_splitByUser(df, proc, 'local_train_splitByUser')\n",
    "t1 = timer()\n",
    "\n",
    "print(f\"Total process time is {(t1 - t0)} secs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* recdp-scala-extension is enabled\n",
    "* per core memory size is 4.500 GB and shuffle_disk maximum capacity is 1200.000 GB\n",
    "* \n",
    "* Merge with category and collapse hist took 274.15958739898633 secs\n",
    "* Generate Dictionary took 2.550 secs\n",
    "* add_negative_sample took 9.529635827988386 secs\n",
    "* save_to_uid_voc took 10.749223954975605 secs\n",
    "* save_to_mid_voc took 13.278015322983265 secs\n",
    "* save_to_cat_voc took 1.4347403410356492 secs\n",
    "* save_to_local_train_splitByUser took 29.36967344605364 secs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
