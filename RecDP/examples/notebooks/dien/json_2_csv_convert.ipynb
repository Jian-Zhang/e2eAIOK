{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import init\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql import *\n",
    "from pyspark import *\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.types import *\n",
    "from timeit import default_timer as timer\n",
    "import logging\n",
    "from pyrecdp.data_processor import *\n",
    "from pyrecdp.utils import *\n",
    "import shutil\n",
    "\n",
    "def load_csv(spark, path):\n",
    "    review_id_field = StructField('reviewerID', StringType())\n",
    "    asin_field = StructField('asin', StringType())\n",
    "    overall_field = StructField('overall', FloatType())\n",
    "    unix_time_field = StructField('unixReviewTime', IntegerType())\n",
    "    reviews_info_schema = StructType(\n",
    "        [review_id_field, asin_field, overall_field, unix_time_field])\n",
    "\n",
    "    category_field = StructField('categories', StringType())\n",
    "    item_info_schema = StructType([asin_field, category_field])\n",
    "\n",
    "    reviews_info_df = spark.read.schema(reviews_info_schema).option('sep', '\\t').csv(path + \"/reviews-info\")\n",
    "    item_info_df = spark.read.schema(item_info_schema).option('sep', '\\t').csv(path + \"/item-info\")\n",
    "\n",
    "    return reviews_info_df, item_info_df\n",
    "\n",
    "def process_meta(file, o_path):\n",
    "    fi = open(file, \"r\")\n",
    "    fo = open(\"%s/item-info\" % o_path, \"w\")\n",
    "    for line in fi:\n",
    "        obj = eval(line)\n",
    "        cat = obj[\"categories\"][0][-1]\n",
    "        print(obj[\"asin\"] + \"\\t\" + cat, file=fo)\n",
    "\n",
    "\n",
    "def list_dir(path):   \n",
    "    source_path_dict = {}\n",
    "    dirs = os.listdir(path)\n",
    "    for files in dirs:\n",
    "        try:\n",
    "            sub_dirs = os.listdir(path + \"/\" + files)\n",
    "            for file_name in sub_dirs:\n",
    "                if (file_name.endswith('parquet') or file_name.endswith('csv')):\n",
    "                    source_path_dict[files] = os.path.join(\n",
    "                        path, files, file_name)\n",
    "        except:\n",
    "            source_path_dict[files] = os.path.join(path, files)\n",
    "    return source_path_dict\n",
    "\n",
    "\n",
    "def result_rename_or_convert(fpath):\n",
    "    source_path_dict = list_dir(fpath)\n",
    "    fix = \"-spark\"\n",
    "    try:\n",
    "        os.rename(source_path_dict[\"reviews-info\" + fix], fpath + 'reviews-info')\n",
    "        shutil.rmtree(source_path_dict[\"reviews-info\" + fix], ignore_errors=True)\n",
    "        # os.rename(source_path_dict[\"item-info\" + fix], fpath + 'item-info')\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "def compare_with_expected(spark, path, records_df, item_info_df):\n",
    "    records_expected_df, item_info_expected_df = load_csv(spark, path)\n",
    "    cmp_res_records = records_expected_df.join(records_df, ['reviewerID', 'asin', 'overall', 'unixReviewTime'], 'anti')\n",
    "    cmp_res_items = item_info_expected_df.join(item_info_df, ['categories', 'asin'], 'anti')\n",
    "    error_parsed_records_len = cmp_res_records.count()\n",
    "    error_parsed_items_len = cmp_res_items.count()\n",
    "    print(f\"records_df error_parsed_len is {error_parsed_records_len}, example as below:\")\n",
    "    cmp_res_records.show()\n",
    "    print(f\"item_info_df error_parsed_len is {error_parsed_items_len}, example as below:\")\n",
    "    cmp_res_items.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_prefix = \"file://\"\n",
    "current_path = \"/home/vmagent/app/recdp/examples/python_tests/dien/output/\"\n",
    "original_folder = \"/home/vmagent/app/recdp/examples/python_tests/dien/\"\n",
    "\n",
    "scala_udf_jars = \"/home/vmagent/app/recdp/ScalaProcessUtils/target/recdp-scala-extensions-0.1.0-jar-with-dependencies.jar\"\n",
    "\n",
    "##### 1. Start spark and initialize data processor #####\n",
    "t0 = timer()\n",
    "spark = SparkSession.builder.master('local[104]')\\\n",
    "    .appName(\"dien_data_ingestion\")\\\n",
    "    .config(\"spark.driver.memory\", \"480G\")\\\n",
    "    .config(\"spark.driver.memoryOverhead\", \"20G\")\\\n",
    "    .config(\"spark.executor.cores\", \"104\")\\\n",
    "    .config(\"spark.driver.extraClassPath\", f\"{scala_udf_jars}\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "# 1.1 prepare dataFrames\n",
    "# 1.2 create RecDP DataProcessor\n",
    "proc = DataProcessor(spark, path_prefix, current_path=current_path, shuffle_disk_capacity=\"1200GB\", spark_mode='local')\n",
    "\n",
    "records_df = spark.read.json(\"%s/%s/raw_data/reviews_Books.json\" % (path_prefix, original_folder))\n",
    "records_df = records_df.select('reviewerID', 'asin', 'overall', 'unixReviewTime')\n",
    "records_df.repartition(1).write.format(\"csv\").option('sep', '\\t').mode(\"overwrite\").save(\"%s/%s/j2c_test/reviews-info-spark\" % (path_prefix, original_folder))\n",
    "process_meta('%s/raw_data/meta_Books.json' % original_folder, \"%s/j2c_test\" % original_folder)\n",
    "result_rename_or_convert(\"%s/j2c_test/\" % (original_folder))\n",
    "\n",
    "t1 = timer()\n",
    "\n",
    "print(f\"Total process time is {(t1 - t0)} secs\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
