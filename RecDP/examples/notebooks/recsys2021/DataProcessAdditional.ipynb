{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import init\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql import *\n",
    "from pyspark import *\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.types import *\n",
    "import time\n",
    "from pyrecdp.data_processor import *\n",
    "from pyrecdp.encoder import *\n",
    "from pyrecdp.utils import *\n",
    "\n",
    "spark = SparkSession.builder.master('yarn')\\\n",
    "        .appName(\"Recsys2021_data_process\")\\\n",
    "        .getOrCreate()\n",
    "path_prefix = \"hdfs://\"\n",
    "dicts_folder = \"recsys_dicts/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "valid = spark.read.parquet(\"/recsys2021_0608_processed/sample_0_3_20days_eric_features/validate_with_features\")\n",
    "#train = spark.read.parquet(\"/recsys2021_0608_processed/sample_0_3_20days_eric_features/train_with_features\")\n",
    "train = spark.read.parquet(\"/recsys2021_0608_processed/sample_0_3_20days_eric_features/train_with_features_with_word_bucket_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "valid.printSchema()\n",
    "train.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f\n",
    "\n",
    "def tweet_word_to_max_bucketid(df, output):\n",
    "    t1 = timer()\n",
    "    tweet_dict_df = spark.read.parquet(\"/recsys2021_0608_processed/recsys_dicts/tweet_word_with_bucketid\")\n",
    "    df_with_id = df.withColumn('row_id', f.monotonically_increasing_id())\n",
    "    df_with_id.write.format('parquet').mode('overwrite').save(\"file:///mnt/nvme2/chendi/BlueWhale/sample_0_3/stage4_20days/tmp_df_with_rowid\")\n",
    "    df_with_id = spark.read.parquet(\"file:///mnt/nvme2/chendi/BlueWhale/sample_0_3/stage4_20days/tmp_df_with_rowid\")\n",
    "    \n",
    "    tmp_df = df_with_id\\\n",
    "               .select('row_id', 'tweet')\\\n",
    "               .withColumn('tweet_word', f.explode(f.split(f.col('tweet'), ' ')))\\\n",
    "               .join(tweet_dict_df.withColumnRenamed('dict_col', 'tweet_word').hint('shuffle_hash'), 'tweet_word', 'left')\\\n",
    "               .select('row_id', 'tweet_word', 'bucket_id')\n",
    "    tmp_df.write.format('parquet').mode('overwrite').save(\"file:///mnt/nvme2/chendi/BlueWhale/sample_0_3/stage4_20days/tmp_valid_joined_tweet_word\")\n",
    "    tmp_df = spark.read.parquet(\"file:///mnt/nvme2/chendi/BlueWhale/sample_0_3/stage4_20days/tmp_valid_joined_tweet_word\")\n",
    "    most_used_wordbucketid_df = tmp_df\\\n",
    "           .groupby('row_id', 'bucket_id').agg(f.count(f.lit(1)).alias('wordcnt_by_bucketid'))\\\n",
    "           .withColumn('word_bucket_rank_id', f.row_number().over(Window.partitionBy('row_id').orderBy(f.desc('wordcnt_by_bucketid'))))\\\n",
    "           .where((f.col('word_bucket_rank_id') == 1) | (f.col('word_bucket_rank_id') == 2))\\\n",
    "           .select('row_id', 'bucket_id', 'wordcnt_by_bucketid', 'word_bucket_rank_id')\n",
    "    most_used_wordbucketid_df.write.format('parquet').mode('overwrite').save(\"file:///mnt/nvme2/chendi/BlueWhale/sample_0_3/stage4_20days/tmp_most_uder_word_bucket_id_by_tweet\")\n",
    "    most_used_wordbucketid_df = spark.read.parquet(\"file:///mnt/nvme2/chendi/BlueWhale/sample_0_3/stage4_20days/tmp_most_uder_word_bucket_id_by_tweet\")\n",
    "    #most_used_wordbucketid_df.show()\n",
    "    df = df_with_id.join(most_used_wordbucketid_df.filter(\"word_bucket_rank_id == 1\"), 'row_id', 'left').drop('wordcnt_by_bucketid').drop('word_bucket_rank_id').withColumnRenamed('bucket_id', 'most_used_word_bucket_id')\n",
    "    df = df.join(most_used_wordbucketid_df.filter(\"word_bucket_rank_id == 2\"), 'row_id', 'left').drop('row_id').drop('wordcnt_by_bucketid').drop('word_bucket_rank_id').withColumnRenamed('bucket_id', 'second_used_word_bucket_id')\n",
    "    df.write.format('parquet').mode('overwrite').save(output)\n",
    "    \n",
    "    t2 = timer()\n",
    "    print(\"Feature Engineering for tweet text: encoded tweet column took %.3f\" % (t2 - t1))\n",
    "    return df\n",
    "\n",
    "#train = spark.read.parquet(\"/recsys2021_0608_processed/sample_0_3_20days_eric_features/train_with_features\")\n",
    "valid = spark.read.parquet(\"/recsys2021_0608_processed/sample_0_3_20days_eric_features/validate_with_features\")\n",
    "tweet_word_to_max_bucketid(\n",
    "    valid,\n",
    "    \"/recsys2021_0608_processed/sample_0_3_20days_eric_features/valid_with_features_with_word_bucket_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import *\n",
    "\n",
    "path_prefix = \"hdfs://\"\n",
    "dicts_folder = \"recsys_dicts/\"\n",
    "current_path = \"/recsys2021_0608_processed/sample_0_3_20days_eric_features/\"\n",
    "proc = DataProcessor(spark, path_prefix,\n",
    "                     current_path=current_path, dicts_path=dicts_folder)\n",
    "\n",
    "label_names = ['reply_timestamp', 'retweet_timestamp', 'retweet_with_comment_timestamp', 'like_timestamp']\n",
    "\n",
    "te_dicts_train = OrderedDict() # value should be tuple, (input_cols, output_col, y_mean)\n",
    "te_dicts_valid = OrderedDict() # value should be tuple, (input_cols, output_col, y_mean)\n",
    "encoding_columns = []\n",
    "train = spark.read.parquet(\"/recsys2021_0608_processed/sample_0_3_20days_eric_features/train_with_features_chendi\")\n",
    "#############################################################################################\n",
    "### 1. Target Encoding ###\n",
    "begin = time.time()\n",
    "y_mean_all = []\n",
    "y_mean_all_dict = {}\n",
    "for t in ['reply_timestamp', 'retweet_timestamp', 'retweet_with_comment_timestamp', 'like_timestamp']:\n",
    "    y_mean = np.array(train.groupBy().mean(t).collect())[0][0]\n",
    "    y_mean_all.append(y_mean)\n",
    "    y_mean_all_dict[t] = y_mean\n",
    "    for c in ['most_used_word_bucket_id', 'second_used_word_bucket_id', 'mentioned_count', 'mentioned_bucket_id',\n",
    "             ['has_mention', 'engaging_user_id'], ['mentioned_count', 'engaging_user_id'], ['mentioned_bucket_id', 'engaging_user_id']]:\n",
    "        start = time.time()\n",
    "        if isinstance(c, str):\n",
    "            out_col = f'TE_{c}_{t}'\n",
    "        elif isinstance(c, list):\n",
    "            out_col = 'TE_'+'_'.join(c)+f'_{t}'\n",
    "        encoder = TargetEncoder(proc, c, t, out_col, y_mean, out_dtype=FloatType())\n",
    "        te_dicts_train[out_col], te_dicts_valid[out_col] = encoder.transform(train, valid, train_only=True)      \n",
    "        print(out_col,\" y_mean is %f, %.1f seconds\"%(y_mean, time.time()-start))\n",
    "print(F\"Target encoding #1 total time:{time.time()-begin}\")\n",
    "# #############################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup_table_path = \"%s/train\" % current_path\n",
    "def merge_features_to_same_input(lookup_list):\n",
    "    for c in lookup_list:\n",
    "        to_join = []\n",
    "        for t in ['reply_timestamp', 'retweet_timestamp', 'retweet_with_comment_timestamp', 'like_timestamp']:\n",
    "            if isinstance(c, str):\n",
    "                feature_name = f'TE_{c}_{t}'\n",
    "                join_cols = ['fold', c]\n",
    "            elif isinstance(c, list):\n",
    "                feature_name = 'TE_'+'_'.join(c)+f'_{t}'\n",
    "                join_cols = ['fold'] + c\n",
    "            to_join.append(spark.read.parquet(\"%s/%s\" % (lookup_table_path, feature_name)))\n",
    "        df = to_join[0]\n",
    "        for l in to_join[1:]:\n",
    "            df = df.join(l, join_cols, 'left')\n",
    "        output = \"lookup_table_%s\" % \"_\".join(join_cols)\n",
    "        df.write.format('parquet').mode('overwrite').save(\"%s/train_lookup/%s\" % (current_path, output))\n",
    "    \n",
    "def merge_features_to_main(train, lookup_list, output):\n",
    "    for c in lookup_list:\n",
    "        for t in ['reply_timestamp', 'retweet_timestamp', 'retweet_with_comment_timestamp', 'like_timestamp']:\n",
    "            if isinstance(c, str):\n",
    "                feature_name = f'TE_{c}_{t}'\n",
    "                join_cols = ['fold', c]\n",
    "            elif isinstance(c, list):\n",
    "                feature_name = 'TE_'+'_'.join(c)+f'_{t}'\n",
    "                join_cols = ['fold'] + c\n",
    "        input_name = \"lookup_table_%s\" % \"_\".join(join_cols)\n",
    "        lookup_df = spark.read.parquet(\"%s/train_lookup/%s\" % (current_path, input_name))\n",
    "        train = train.join(lookup_df, join_cols, 'left')\n",
    "    train.write.format('parquet').mode('overwrite').save(\"%s/%s\" % (current_path, output))\n",
    "\n",
    "lookup_list_1 = ['most_used_word_bucket_id', 'second_used_word_bucket_id', 'mentioned_count', 'mentioned_bucket_id', ['has_mention', 'engaging_user_id']]\n",
    "lookup_list_2 = [['mentioned_count', 'engaging_user_id'], ['mentioned_bucket_id', 'engaging_user_id']]\n",
    "#merge_features_to_same_input(lookup_list_1)\n",
    "#merge_features_to_same_input(lookup_list_2)\n",
    "train = spark.read.parquet(\"/recsys2021_0608_processed/sample_0_3_20days_eric_features/train_with_features_chendi\")\n",
    "merge_features_to_main(train, lookup_list_1, \"train_with_features_chendi_te_1\")\n",
    "train = spark.read.parquet(\"%s/%s\" % (current_path, \"train_with_features_chendi_te_1\"))\n",
    "merge_features_to_main(train, lookup_list_2, \"train_with_features_chendi_te\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_valid_lookup_from_train(lookup_list):\n",
    "    for c in lookup_list:\n",
    "        feature_list = []\n",
    "        for t in ['reply_timestamp', 'retweet_timestamp', 'retweet_with_comment_timestamp', 'like_timestamp']:\n",
    "            if isinstance(c, str):\n",
    "                feature_name = f'TE_{c}_{t}'\n",
    "                join_cols = ['fold', c]\n",
    "                output = \"lookup_table_%s\" % c\n",
    "            elif isinstance(c, list):\n",
    "                feature_name = 'TE_'+'_'.join(c)+f'_{t}'\n",
    "                join_cols = ['fold'] + c\n",
    "                output = \"lookup_table_%s\" % \"_\".join(c)\n",
    "            feature_list.append(feature_name)\n",
    "        input_name = \"lookup_table_%s\" % \"_\".join(join_cols)\n",
    "        #output = \"lookup_table_%s\" % c\n",
    "        lookup_df = spark.read.parquet(\"%s/train_lookup/%s\" % (current_path, input_name))\n",
    "        valid_lookup_df = lookup_df.groupby(c).agg(*[f.mean(c).alias(c) for c in feature_list])\n",
    "        valid_lookup_df.write.format('parquet').mode('overwrite').save(\"%s/valid_lookup/%s\" % (current_path, output))\n",
    "\n",
    "def merge_features_to_main(valid, lookup_list, output):\n",
    "    for c in lookup_list:\n",
    "        for t in ['reply_timestamp', 'retweet_timestamp', 'retweet_with_comment_timestamp', 'like_timestamp']:\n",
    "            if isinstance(c, str):\n",
    "                feature_name = f'TE_{c}_{t}'\n",
    "                join_cols = [c]\n",
    "            elif isinstance(c, list):\n",
    "                feature_name = 'TE_'+'_'.join(c)+f'_{t}'\n",
    "                join_cols = c\n",
    "        input_name = \"lookup_table_%s\" % \"_\".join(join_cols)\n",
    "        lookup_df = spark.read.parquet(\"%s/valid_lookup/%s\" % (current_path, input_name))\n",
    "        valid = valid.join(lookup_df, join_cols, 'left')\n",
    "    valid.write.format('parquet').mode('overwrite').save(\"%s/%s\" % (current_path, output))\n",
    "\n",
    "lookup_list_1 = ['most_used_word_bucket_id', 'second_used_word_bucket_id', 'mentioned_count', 'mentioned_bucket_id', ['has_mention', 'engaging_user_id']]\n",
    "lookup_list_2 = [['mentioned_count', 'engaging_user_id'], ['mentioned_bucket_id', 'engaging_user_id']]\n",
    "\n",
    "valid = spark.read.parquet(\"%s/%s\" % (current_path, \"validate_with_features_chendi\"))\n",
    "gen_valid_lookup_from_train(lookup_list_1 + lookup_list_2)\n",
    "merge_features_to_main(valid, lookup_list_1 + lookup_list_2, \"validate_with_features_chendi_te_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mentioned_bucket_id\n",
      "mentioned_count\n",
      "has_mention\n",
      "second_used_word_bucket_id\n",
      "most_used_word_bucket_id\n",
      "mention\n",
      "has_photo\n",
      "has_video\n",
      "has_gif\n",
      "has_links\n",
      "has_hashtags\n",
      "TE_most_used_word_bucket_id_reply_timestamp\n",
      "TE_most_used_word_bucket_id_retweet_timestamp\n",
      "TE_most_used_word_bucket_id_retweet_with_comment_timestamp\n",
      "TE_most_used_word_bucket_id_like_timestamp\n",
      "TE_second_used_word_bucket_id_reply_timestamp\n",
      "TE_second_used_word_bucket_id_retweet_timestamp\n",
      "TE_second_used_word_bucket_id_retweet_with_comment_timestamp\n",
      "TE_second_used_word_bucket_id_like_timestamp\n",
      "TE_mentioned_count_reply_timestamp\n",
      "TE_mentioned_count_retweet_timestamp\n",
      "TE_mentioned_count_retweet_with_comment_timestamp\n",
      "TE_mentioned_count_like_timestamp\n",
      "TE_mentioned_bucket_id_reply_timestamp\n",
      "TE_mentioned_bucket_id_retweet_timestamp\n",
      "TE_mentioned_bucket_id_retweet_with_comment_timestamp\n",
      "TE_mentioned_bucket_id_like_timestamp\n",
      "TE_has_mention_engaging_user_id_reply_timestamp\n",
      "TE_has_mention_engaging_user_id_retweet_timestamp\n",
      "TE_has_mention_engaging_user_id_retweet_with_comment_timestamp\n",
      "TE_has_mention_engaging_user_id_like_timestamp\n",
      "TE_mentioned_count_engaging_user_id_reply_timestamp\n",
      "TE_mentioned_count_engaging_user_id_retweet_timestamp\n",
      "TE_mentioned_count_engaging_user_id_retweet_with_comment_timestamp\n",
      "TE_mentioned_count_engaging_user_id_like_timestamp\n",
      "TE_mentioned_bucket_id_engaging_user_id_reply_timestamp\n",
      "TE_mentioned_bucket_id_engaging_user_id_retweet_timestamp\n",
      "TE_mentioned_bucket_id_engaging_user_id_retweet_with_comment_timestamp\n",
      "TE_mentioned_bucket_id_engaging_user_id_like_timestamp\n",
      "-RECORD 0------------------------------\n",
      " reply_timestamp                | 0    \n",
      " retweet_timestamp              | 0    \n",
      " retweet_with_comment_timestamp | 0    \n",
      " like_timestamp                 | 1    \n",
      " most_used_word_bucket_id       | 20.0 \n",
      " second_used_word_bucket_id     | 3.0  \n",
      " mentioned_count                | 1    \n",
      " mentioned_bucket_id            | 1.0  \n",
      "-RECORD 1------------------------------\n",
      " reply_timestamp                | 0    \n",
      " retweet_timestamp              | 0    \n",
      " retweet_with_comment_timestamp | 0    \n",
      " like_timestamp                 | 0    \n",
      " most_used_word_bucket_id       | 20.0 \n",
      " second_used_word_bucket_id     | 1.0  \n",
      " mentioned_count                | 1    \n",
      " mentioned_bucket_id            | 1.0  \n",
      "-RECORD 2------------------------------\n",
      " reply_timestamp                | 0    \n",
      " retweet_timestamp              | 0    \n",
      " retweet_with_comment_timestamp | 0    \n",
      " like_timestamp                 | 1    \n",
      " most_used_word_bucket_id       | 20.0 \n",
      " second_used_word_bucket_id     | 10.0 \n",
      " mentioned_count                | 1    \n",
      " mentioned_bucket_id            | 1.0  \n",
      "-RECORD 3------------------------------\n",
      " reply_timestamp                | 0    \n",
      " retweet_timestamp              | 0    \n",
      " retweet_with_comment_timestamp | 0    \n",
      " like_timestamp                 | 1    \n",
      " most_used_word_bucket_id       | 20.0 \n",
      " second_used_word_bucket_id     | 1.0  \n",
      " mentioned_count                | 1    \n",
      " mentioned_bucket_id            | 1.0  \n",
      "-RECORD 4------------------------------\n",
      " reply_timestamp                | 0    \n",
      " retweet_timestamp              | 0    \n",
      " retweet_with_comment_timestamp | 0    \n",
      " like_timestamp                 | 0    \n",
      " most_used_word_bucket_id       | 20.0 \n",
      " second_used_word_bucket_id     | 1.0  \n",
      " mentioned_count                | 1    \n",
      " mentioned_bucket_id            | 1.0  \n",
      "-RECORD 5------------------------------\n",
      " reply_timestamp                | 0    \n",
      " retweet_timestamp              | 0    \n",
      " retweet_with_comment_timestamp | 0    \n",
      " like_timestamp                 | 1    \n",
      " most_used_word_bucket_id       | 20.0 \n",
      " second_used_word_bucket_id     | 1.0  \n",
      " mentioned_count                | 1    \n",
      " mentioned_bucket_id            | 1.0  \n",
      "-RECORD 6------------------------------\n",
      " reply_timestamp                | 0    \n",
      " retweet_timestamp              | 0    \n",
      " retweet_with_comment_timestamp | 0    \n",
      " like_timestamp                 | 0    \n",
      " most_used_word_bucket_id       | 20.0 \n",
      " second_used_word_bucket_id     | 14.0 \n",
      " mentioned_count                | 1    \n",
      " mentioned_bucket_id            | 1.0  \n",
      "-RECORD 7------------------------------\n",
      " reply_timestamp                | 0    \n",
      " retweet_timestamp              | 0    \n",
      " retweet_with_comment_timestamp | 0    \n",
      " like_timestamp                 | 0    \n",
      " most_used_word_bucket_id       | 20.0 \n",
      " second_used_word_bucket_id     | 1.0  \n",
      " mentioned_count                | 1    \n",
      " mentioned_bucket_id            | 1.0  \n",
      "-RECORD 8------------------------------\n",
      " reply_timestamp                | 0    \n",
      " retweet_timestamp              | 0    \n",
      " retweet_with_comment_timestamp | 0    \n",
      " like_timestamp                 | 1    \n",
      " most_used_word_bucket_id       | 20.0 \n",
      " second_used_word_bucket_id     | 3.0  \n",
      " mentioned_count                | 1    \n",
      " mentioned_bucket_id            | 1.0  \n",
      "-RECORD 9------------------------------\n",
      " reply_timestamp                | 0    \n",
      " retweet_timestamp              | 0    \n",
      " retweet_with_comment_timestamp | 0    \n",
      " like_timestamp                 | 0    \n",
      " most_used_word_bucket_id       | 20.0 \n",
      " second_used_word_bucket_id     | 1.0  \n",
      " mentioned_count                | 1    \n",
      " mentioned_bucket_id            | 1.0  \n",
      "-RECORD 10-----------------------------\n",
      " reply_timestamp                | 0    \n",
      " retweet_timestamp              | 0    \n",
      " retweet_with_comment_timestamp | 0    \n",
      " like_timestamp                 | 1    \n",
      " most_used_word_bucket_id       | 20.0 \n",
      " second_used_word_bucket_id     | 1.0  \n",
      " mentioned_count                | 1    \n",
      " mentioned_bucket_id            | 1.0  \n",
      "-RECORD 11-----------------------------\n",
      " reply_timestamp                | 0    \n",
      " retweet_timestamp              | 0    \n",
      " retweet_with_comment_timestamp | 0    \n",
      " like_timestamp                 | 0    \n",
      " most_used_word_bucket_id       | 20.0 \n",
      " second_used_word_bucket_id     | 3.0  \n",
      " mentioned_count                | 1    \n",
      " mentioned_bucket_id            | 1.0  \n",
      "-RECORD 12-----------------------------\n",
      " reply_timestamp                | 0    \n",
      " retweet_timestamp              | 0    \n",
      " retweet_with_comment_timestamp | 0    \n",
      " like_timestamp                 | 0    \n",
      " most_used_word_bucket_id       | 20.0 \n",
      " second_used_word_bucket_id     | 2.0  \n",
      " mentioned_count                | 1    \n",
      " mentioned_bucket_id            | 1.0  \n",
      "-RECORD 13-----------------------------\n",
      " reply_timestamp                | 0    \n",
      " retweet_timestamp              | 0    \n",
      " retweet_with_comment_timestamp | 0    \n",
      " like_timestamp                 | 0    \n",
      " most_used_word_bucket_id       | 20.0 \n",
      " second_used_word_bucket_id     | 19.0 \n",
      " mentioned_count                | 1    \n",
      " mentioned_bucket_id            | 1.0  \n",
      "-RECORD 14-----------------------------\n",
      " reply_timestamp                | 0    \n",
      " retweet_timestamp              | 0    \n",
      " retweet_with_comment_timestamp | 0    \n",
      " like_timestamp                 | 0    \n",
      " most_used_word_bucket_id       | 20.0 \n",
      " second_used_word_bucket_id     | 6.0  \n",
      " mentioned_count                | 1    \n",
      " mentioned_bucket_id            | 1.0  \n",
      "-RECORD 15-----------------------------\n",
      " reply_timestamp                | 0    \n",
      " retweet_timestamp              | 0    \n",
      " retweet_with_comment_timestamp | 0    \n",
      " like_timestamp                 | 0    \n",
      " most_used_word_bucket_id       | 20.0 \n",
      " second_used_word_bucket_id     | 12.0 \n",
      " mentioned_count                | 1    \n",
      " mentioned_bucket_id            | 1.0  \n",
      "-RECORD 16-----------------------------\n",
      " reply_timestamp                | 0    \n",
      " retweet_timestamp              | 0    \n",
      " retweet_with_comment_timestamp | 0    \n",
      " like_timestamp                 | 0    \n",
      " most_used_word_bucket_id       | 20.0 \n",
      " second_used_word_bucket_id     | 1.0  \n",
      " mentioned_count                | 1    \n",
      " mentioned_bucket_id            | 1.0  \n",
      "-RECORD 17-----------------------------\n",
      " reply_timestamp                | 0    \n",
      " retweet_timestamp              | 0    \n",
      " retweet_with_comment_timestamp | 0    \n",
      " like_timestamp                 | 1    \n",
      " most_used_word_bucket_id       | 20.0 \n",
      " second_used_word_bucket_id     | 2.0  \n",
      " mentioned_count                | 1    \n",
      " mentioned_bucket_id            | 1.0  \n",
      "-RECORD 18-----------------------------\n",
      " reply_timestamp                | 0    \n",
      " retweet_timestamp              | 0    \n",
      " retweet_with_comment_timestamp | 0    \n",
      " like_timestamp                 | 0    \n",
      " most_used_word_bucket_id       | 20.0 \n",
      " second_used_word_bucket_id     | 6.0  \n",
      " mentioned_count                | 1    \n",
      " mentioned_bucket_id            | 1.0  \n",
      "-RECORD 19-----------------------------\n",
      " reply_timestamp                | 0    \n",
      " retweet_timestamp              | 0    \n",
      " retweet_with_comment_timestamp | 0    \n",
      " like_timestamp                 | 0    \n",
      " most_used_word_bucket_id       | 20.0 \n",
      " second_used_word_bucket_id     | 5.0  \n",
      " mentioned_count                | 1    \n",
      " mentioned_bucket_id            | 1.0  \n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#valid = spark.read.parquet(\"/recsys2021_0608_processed/sample_0_3_20days_eric_features/validate_with_features_chendi\")\n",
    "#valid.printSchema()\n",
    "#valid.select('reply_timestamp', 'retweet_timestamp', 'retweet_with_comment_timestamp', 'like_timestamp',\n",
    "#             'most_used_word_bucket_id', 'second_used_word_bucket_id', 'mentioned_count', 'mentioned_bucket_id').show(vertical=True)\n",
    "\n",
    "train = spark.read.parquet(\"/recsys2021_0608_processed/sample_0_3_20days_eric_features/train_with_features_chendi_te\")\n",
    "#train.printSchema()\n",
    "for c in train.columns:\n",
    "    if \"has_rt\" not in c and (\"has_\" in c or \"mention\" in c or \"word_bucket_id\" in c):\n",
    "        print(c)\n",
    "train.select('reply_timestamp', 'retweet_timestamp', 'retweet_with_comment_timestamp', 'like_timestamp',\n",
    "             'most_used_word_bucket_id', 'second_used_word_bucket_id', 'mentioned_count', 'mentioned_bucket_id').show(vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train = spark.read.parquet(\"/recsys2021_0608_processed/sample_0_3_20days_eric_features/train_with_features_chendi_te\")\n",
    "train.drop(\"tweet_nortsign\").sample(0.5, 3).write.format('parquet').mode('overwrite').save(\"/recsys2021_0608_processed/sample_0_3_20days_eric_features/train_with_features_chendi_0.15\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "word_dict_df = spark.read.parquet(\"/recsys2021_0608_processed/recsys_dicts/tweet_word_with_bucketid\")\n",
    "mention_dict_df = spark.read.parquet(\"/recsys2021_0608_processed/recsys_dicts/mention_with_bucketid\")\n",
    "train = spark.read.parquet(\"/recsys2021_0608_processed/sample_0_3_20days_eric_features/train_with_features_with_word_bucket_id\")\n",
    "train = train.withColumn('has_photo', F.col('present_media').contains('Photo').cast(IntegerType()))\\\n",
    "             .withColumn('has_video', F.col('present_media').contains('Vedio').cast(IntegerType()))\\\n",
    "             .withColumn('has_gif', F.col('present_media').contains('GIF').cast(IntegerType()))\\\n",
    "             .withColumn('has_links', (F.col('present_links') != \"\").cast(IntegerType()))\\\n",
    "             .withColumn('has_hashtags', (F.col('hashtags') != \"\").cast(IntegerType()))\\\n",
    "             .withColumn('mention', F.regexp_extract(F.col('tweet'), r\"[^RT]\\s@(\\S+)\", 1))\\\n",
    "             .withColumn('has_mention', (F.col('mention')!= \"\").cast(IntegerType()))\\\n",
    "             .withColumn('has_rt', (F.col('has_rt').cast(IntegerType())))\n",
    "train = train.drop('text_tokens').drop('tokens').drop('tweet_nortsign')\n",
    "train = train.join(mention_dict_df.hint('merge'), 'mention', 'left').withColumnRenamed('count', 'mentioned_count').withColumnRenamed('bucket_id', 'mentioned_bucket_id')\n",
    "train.write.format('parquet').mode('overwrite').save(\"/recsys2021_0608_processed/sample_0_3_20days_eric_features/train_with_features_chendi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "mention_dict_df = spark.read.parquet(\"/recsys2021_0608_processed/recsys_dicts/mention_with_bucketid\")\n",
    "valid = spark.read.parquet(\"/recsys2021_0608_processed/sample_0_3_20days_eric_features/valid_with_features_with_word_bucket_id\")\n",
    "valid = valid.withColumn('has_photo', F.col('present_media').contains('Photo').cast(IntegerType()))\\\n",
    "             .withColumn('has_video', F.col('present_media').contains('Vedio').cast(IntegerType()))\\\n",
    "             .withColumn('has_gif', F.col('present_media').contains('GIF').cast(IntegerType()))\\\n",
    "             .withColumn('has_links', (F.col('present_links') != \"\").cast(IntegerType()))\\\n",
    "             .withColumn('has_hashtags', (F.col('hashtags') != \"\").cast(IntegerType()))\\\n",
    "             .withColumn('mention', F.regexp_extract(F.col('tweet'), r\"[^RT]\\s@(\\S+)\", 1))\\\n",
    "             .withColumn('has_mention', (F.col('mention')!= \"\").cast(IntegerType()))\\\n",
    "             .withColumn('has_rt', (F.col('has_rt').cast(IntegerType())))\n",
    "valid = valid.drop('text_tokens').drop('tokens').drop('tweet_nortsign')\n",
    "valid = valid.join(mention_dict_df.hint('merge'), 'mention', 'left').withColumnRenamed('count', 'mentioned_count').withColumnRenamed('bucket_id', 'mentioned_bucket_id')\n",
    "valid.write.format('parquet').mode('overwrite').save(\"/recsys2021_0608_processed/sample_0_3_20days_eric_features/validate_with_features_chendi\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import *\n",
    "df = spark.read.parquet(\"/recsys2021_0608_processed/recsys_dicts/tweet\")\n",
    "df.printSchema()\n",
    "qd = QuantileDiscretizer(numBuckets=100, inputCol=\"count\", outputCol='bucket_id')\n",
    "result  = qd.fit(df).transform(df)\n",
    "#result.groupby('bucket_id').agg(F.min('count'), F.count(F.lit(1))).show()\n",
    "result.write.format('parquet').mode('overwrite').save('/recsys2021_0608_processed/recsys_dicts/tweet_word_with_bucketid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = spark.read.parquet(\"/recsys2021_0608_processed/recsys_dicts/tweet_word_with_bucketid\")\n",
    "df.printSchema()\n",
    "df.groupby('bucket_id').agg(F.min('count'), F.count(F.lit(1))).show(50)\n",
    "df = spark.read.parquet(\"/recsys2021_0608_processed/recsys_dicts/mention_with_bucketid\")\n",
    "df.printSchema()\n",
    "df.groupby('bucket_id').agg(F.min('count'), F.count(F.lit(1))).show(50)\n",
    "train = spark.read.parquet(\"/recsys2021_0608_processed/sample_0_3_20days_eric_features/train_with_features\")\n",
    "train.printSchema()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
