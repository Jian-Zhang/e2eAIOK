{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/datac/recsys2021/miniconda3/envs/recsys/lib/python3.7/site-packages/ipykernel_launcher.py:21: DeprecationWarning: pyarrow.hdfs.connect is deprecated as of 2.0.0, please use pyarrow.fs.HadoopFileSystem instead.\n"
     ]
    }
   ],
   "source": [
    "# import stuff\n",
    "#  %%time\n",
    "\n",
    "%config IPCompleter.greedy=True\n",
    "\n",
    "import os\n",
    "os.environ['http_proxy']=''\n",
    "os.environ['https_proxy']=''\n",
    "\n",
    "#import modin.config\n",
    "#modin.config.Backend.put('omnisci')\n",
    "\n",
    "#import modin.pandas as pd\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "import timeit\n",
    "hdfs = pa.hdfs.connect()\n",
    "# hdfs = pa.fs.HadoopFileSystem(host=\"localhost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = hdfs.read_parquet(\"/updated_parquet/updated_parquet/part-00000.parquet\").to_pandas()\n",
    "#df = pq.read_table(\"/updated_parquet/updated_parquet/part-00000.parquet\", filesystem = hdfs).to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-02-03 16:00:00\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, timedelta\n",
    "dstart = datetime.strptime(\"2021-02-03 16:00:00\", \"%Y-%m-%d %H:%M:%S\")\n",
    "print(dstart)\n",
    "import numpy as np\n",
    "valend = dstart + timedelta(days=2)\n",
    "testend = dstart + timedelta(days=18)\n",
    "trainend = valend + timedelta(days=18)\n",
    "\n",
    "trains = []\n",
    "vals = []\n",
    "tests = []\n",
    "\n",
    "file_idxs = list(range(20)) + list(range(180, 199))\n",
    "for idx in file_idxs:\n",
    "    idxs = str(idx).zfill(3)\n",
    "    df = hdfs.read_parquet(f\"/updated_parquet/updated_parquet/part-00{idxs}.parquet\").to_pandas()\n",
    "    df['tweet_datetime'] = pd.to_datetime(df['tweet_timestamp'], unit='s')\n",
    "    \n",
    "    val_mask = df['tweet_datetime'] < valend\n",
    "    test_mask = df['tweet_datetime'] > testend\n",
    "    train_mask = np.logical_and((df['tweet_datetime'] > valend), df['tweet_datetime'] < trainend)\n",
    "    \n",
    "    trains.append(df[train_mask])\n",
    "    vals.append(df[val_mask])\n",
    "    tests.append(df[test_mask])\n",
    "    \n",
    "\n",
    "train = pd.concat(trains)\n",
    "test = pd.concat(tests)\n",
    "val = pd.concat(vals)\n",
    "\n",
    "# \"/updated_parquet/updated_parquet/part-00000.parquet\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.shape, train.shape, val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# non-estimator functions.  Thanks to Visilij for porting.\n",
    "def prep_tsv_columns(df):\n",
    "    media = df['present_media'].fillna('')\n",
    "    df['has_photo'] = media.str.contains('Photo').astype('int8')\n",
    "    df['has_video'] = media.str.contains('Video').astype('int8')\n",
    "    df['has_gif'] = media.str.contains('GIF').astype('int8')\n",
    "    return df\n",
    "\n",
    "def get_rates(df):\n",
    "    df['engaged_user_rate'] = df[\"engaged_with_user_following_count\"] / df[\"engaged_with_user_follower_count\"]\n",
    "    df['engaging_user_rate'] = df[\"engaging_user_following_count\"] / df[\"engaging_user_follower_count\"]\n",
    "    return df\n",
    "\n",
    "def prep_bool_cols(df):\n",
    "    bool_columns = ['engagee_follows_engager', \"engaged_with_user_is_verified\", 'engaging_user_is_verified']\n",
    "    for c in bool_columns:\n",
    "        df[f'{c}_indicator'] = df[c].astype('int8')\n",
    "        \n",
    "    return df\n",
    "        \n",
    "def prep_datetime_columns(df):\n",
    "    df['tweet_datetime'] = pd.to_datetime(df['tweet_timestamp'], unit='s')\n",
    "    df['tweet_hour'] = df['tweet_datetime'].dt.hour\n",
    "    df['tweet_dow'] = df['tweet_datetime'].dt.dayofweek\n",
    "    return df\n",
    "        \n",
    "def feature_generation(df):\n",
    "    df['both_verified'] = (df[\"engaging_user_is_verified\"] & df[\"engaged_with_user_is_verified\"]).astype('int8')\n",
    "    return df\n",
    "\n",
    "def prep_response(df):\n",
    "    cols = ['reply_timestamp', 'retweet_timestamp', 'retweet_with_comment_timestamp', 'like_timestamp']\n",
    "    for c in cols:\n",
    "        df[f'{c}_indicator'] = df[c].notnull().astype('int8')\n",
    "    return df\n",
    "        \n",
    "def most_common_value(df, index, cat_column):\n",
    "    w = Window.partitionBy(index, cat_column)\n",
    "    df_wrn = df.withColumn(\"count\", F.count(\"*\").over(w)).withColumn('row_number', F.row_number().over(w.orderBy(F.desc(\"count\"))))\n",
    "    df_wrn = df_wrn.filter(\"row_number = 1\")\n",
    "    return df_wrn\n",
    "\n",
    "def language_indicator(df):\n",
    "    dfagg = df.groupby('engaging_user_id').agg({'language': ['first']})\n",
    "    dfagg.columns = dfagg.columns.to_flat_index()\n",
    "    dfagg = dfagg.rename(columns={('language', 'first'): 'engaging_user_language'})\n",
    "\n",
    "    df = df.join(dfagg, on='engaging_user_id', how='inner')\n",
    "    df['same_languages'] = (df['engaging_user_language'] == df['language']).astype('int8')\n",
    "    return df\n",
    "\n",
    "def all_prep(df):\n",
    "    df['engaging_user_is_verified'] = df['enaging_user_is_verified']\n",
    "    df['engaging_user_following_count'] = df['enaging_user_following_count']\n",
    "    df = prep_tsv_columns(df)\n",
    "    df = prep_bool_cols(df)\n",
    "    df = feature_generation(df)\n",
    "    df = prep_datetime_columns(df)\n",
    "    df = get_rates(df)\n",
    "    # df = language_indicator(df)\n",
    "    df = prep_response(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model object stub to make joins easier\n",
    "\n",
    "\n",
    "def intersect(x1, x2):\n",
    "\n",
    "    values = []\n",
    "    for idx in range(len(x1)):\n",
    "        if x1[idx] is None or x2[idx] is None:\n",
    "            values.append(0)\n",
    "        else:\n",
    "            xset = set(str(x1[idx]).split(\"\\t\"))\n",
    "            row_intersect = xset.intersection(x2[idx])\n",
    "\n",
    "            values.append(row_intersect)\n",
    "    \n",
    "    return values\n",
    "    \n",
    "class Model():\n",
    "    def __init__(self, name, join_cols=None):\n",
    "        \n",
    "        #jsons = list(Path(f\"training/{name}.json\").glob(f\"**/*.json\"))\n",
    "\n",
    "        #there should only be one json file... coalesced in spark from one row\n",
    "#         with open(jsons[0]) as json_file:\n",
    "#             self.params = json.load(json_file)\n",
    "        \n",
    "        # self.df_agg = pq.read_table(f\"training/{name}.parquet\").to_pandas().drop_duplicates()\n",
    "        # self.df_agg = pq.read_table(f\"training/{name}.parquet\", filesystem=hdfs).to_pandas()\n",
    "\n",
    "        self.df_agg = pq.ParquetDataset(f\"/training/{name}.parquet\", filesystem = hdfs).read().to_pandas()\n",
    "        self.join_cols = join_cols\n",
    "        \n",
    "    def set_join_cols(cols):\n",
    "        self.join_cols = cols\n",
    "        \n",
    "    def transform(self, data):\n",
    "        return pd.merge(data, self.df_agg, on=self.join_cols, how='left')\n",
    "    \n",
    "#Model object stub to make joins easier\n",
    "class CPD_Model():\n",
    "    def __init__(self, name, inputCol = None, outputCol=None, join_cols=None):\n",
    "        \n",
    "        jsons = list(Path(f\"training/{name}.json\").glob(f\"**/*.json\"))\n",
    "\n",
    "        #there should only be one json file... coalesced in spark from one row\n",
    "        with open(jsons[0]) as json_file:\n",
    "            self.params = json.load(json_file)\n",
    "        \n",
    "        self.df_agg = hdfs.read_table(f\"training/{name}.parquet\").to_pandas()\n",
    "        \n",
    "        self.join_cols = join_cols\n",
    "        self.inputCol = inputCol\n",
    "        self.outputCol = outputCol\n",
    "        \n",
    "    def set_join_cols(cols):\n",
    "        self.join_cols = cols\n",
    "        \n",
    "    def transform(self, data):\n",
    "        alldf = pd.merge(data, self.df_agg, on=self.join_cols, how='left')\n",
    "        alldf[f'CPD_{self.inputCol}_{self.outputCol}'] = alldf.apply(lambda x: intersect(alldf[f\"{self.inputCol}_{self.inputCol}_{self.outputCol}_intersection_unique\"], alldf[self.inputCol]))\n",
    "    \n",
    "    \n",
    "    \n",
    "    #merge the stored data with new data and calucate the intersection\n",
    "#         dataset = dataset.join(self.agg_all.select(self.indexCol, f\"{self.inputCol}_{self.outputCol}_unique\"), on=[self.indexCol], how='left')\n",
    "#         dataset = dataset.withColumn(self.outputCol, F.size(F.array_intersect(f\"{self.inputCol}_{self.outputCol}_unique\", F.split(F.col(self.inputCol), '\\t'))))\n",
    "#         dataset = dataset.fillna({self.outputCol: 0.0})\n",
    "#         return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gte_tweet_type = Model(\"GMTE_tweet_type_engaged_with_user_id\", join_cols=['engaging_user_id', 'tweet_type'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gte_tweet_type.df_agg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insantiate the Model objects.  They will load data from \"./training\" directory\n",
    "te_engaging_user = Model(\"TE_engaging_user_id\", join_cols=['engaging_user_id'])\n",
    "te_language = Model(\"TE_language\", join_cols=['language'])\n",
    "te_tweet_type = Model(\"TE_tweet_type\", join_cols=['tweet_type'])\n",
    "te_engaged_user = Model(\"TE_engaged_with_user_id\", join_cols=['engaged_with_user_id'])\n",
    "te_tweet_dow = Model(\"TE_tweet_dow\", join_cols=['tweet_dow'])\n",
    "te_tweet_hour = Model(\"TE_tweet_hour\", join_cols=['tweet_hour'])\n",
    "te_engaging_user = Model(\"TE_engaging_user_id\", join_cols = ['engaging_user_id'])\n",
    "\n",
    "\n",
    "\n",
    "model_list = [te_engaging_user, te_language, te_tweet_type, te_engaged_user, te_tweet_dow, te_tweet_hour, te_engaging_user] \n",
    "#              gte_engaged_with_user_id, gte_language, gte_tweet_dow, gte_tweet_hour, gte_tweet_type]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gte_engaged_with_user_id = Model(\"GMTE_engaged_with_user_id\", join_cols=['engaging_user_id', 'engaged_with_user_id'])\n",
    "# gte_language = Model(\"GMTE_language\", join_cols = ['engaging_user_id', 'language'])\n",
    "# gte_tweet_dow = Model(\"GMTE_tweet_dow\", join_cols=['engaging_user_id', 'tweet_dow'])\n",
    "# gte_tweet_hour = Model(\"GMTE_tweet_hour\", join_cols=['engaging_user_id', 'tweet_hour'])\n",
    "# gte_tweet_type = Model(\"GMTE_tweet_type\", join_cols=['engaging_user_id', 'tweet_type'])\n",
    "\n",
    "gte_engaged_with_user_id_euid = Model(\"GMTE_engaged_with_user_id_engaging_user_id\", join_cols = ['engaged_with_user_id', 'engaging_user_id'])\n",
    "# gte_has_rt_engaged_with_user_id = Model(\"GMTE_has_rt_engaged_with_user_id\", join_cols = ['has_rt', 'engaged_with_user_id'])\n",
    "gte_language_engaged_with_user_id = Model(\"GMTE_language_engaged_with_user_id\", join_cols = ['language', 'engaged_with_user_id'])\n",
    "gte_language_engaging_user_id = Model(\"GMTE_language_engaging_user_id\", join_cols = ['language', 'engaging_user_id'])\n",
    "gte_tweet_dow_ewuid = Model(\"GMTE_tweet_dow_engaged_with_user_id\", join_cols = ['tweet_dow', 'engaged_with_user_id'])\n",
    "gte_tweet_dow_euid = Model(\"GMTE_tweet_dow_engaging_user_id\", join_cols = ['tweet_dow', 'engaging_user_id'])\n",
    "gte_tweet_hour_engaged_with_user_id = Model(\"GMTE_tweet_hour_engaged_with_user_id\", join_cols = ['tweet_hour', 'engaged_with_user_id'])\n",
    "gte_tweet_hour_euid = Model(\"GMTE_tweet_hour_engaging_user_id\", join_cols = ['tweet_hour', 'engaging_user_id'])\n",
    "gte_tweet_type_ewuid = Model(\"GMTE_tweet_type_engaged_with_user_id\", join_cols = ['tweet_type', 'engaged_with_user_id'])\n",
    "gte_tweet_type_euid = Model(\"GMTE_tweet_type_engaging_user_id\", join_cols = ['tweet_type', 'engaging_user_id'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = model_list + [gte_engaged_with_user_id_euid,gte_language_engaged_with_user_id,\n",
    "                          gte_language_engaging_user_id,gte_tweet_dow_ewuid,gte_tweet_dow_euid,gte_tweet_hour_engaged_with_user_id,\n",
    "                          gte_tweet_hour_euid,gte_tweet_type_ewuid,gte_tweet_type_euid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#call transformer code\n",
    "prep_start = timeit.default_timer()\n",
    "train = all_prep(train)\n",
    "test = all_prep(test)\n",
    "val = all_prep(val)\n",
    "prep_end = timeit.default_timer()\n",
    "\n",
    "print(f\"prep rate {train.shape[0]/(prep_end - prep_start)} rows/second\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now perform target embedding lookup\n",
    "start = timeit.default_timer()\n",
    "# feature_engineered_df = df_transformed\n",
    "print(train.shape)\n",
    "for m in model_list:\n",
    "    train = m.transform(train)\n",
    "    test = m.transform(test)\n",
    "    val = m.transform(val)\n",
    "    # feature_engineered_df = m.transform(feature_engineered_df)\n",
    "    print(train.shape)\n",
    "\n",
    "end = timeit.default_timer()\n",
    "print(f\"merge took {end - start} seconds\")\n",
    "     \n",
    "print(feature_engineered_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#next will be xgboost modelling.  To be completed after the rest of the pipeline is complete.\n",
    "for c in train.columns:\n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = train.columns\n",
    "idx_start = train.columns.get_loc(\"engaging_user_following_count\")\n",
    "idx_end = train.columns.get_loc(\"engaging_user_rate\")\n",
    "fcols = cols[idx_start:idx_end+1].to_list()\n",
    "\n",
    "idx_final = cols.get_loc(\"TE_engaging_user_id_reply_timestamp_indicator_x\")\n",
    "\n",
    "fcols = fcols + cols[idx_final:].to_list()\n",
    "\n",
    "#fcols.remove('engaging_user_language')\n",
    "#fcols.remove(\"tweet_datetime\")\n",
    "print(fcols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I am going to fit an XGBoost model in this stage.  In the real inference case we will load a serialized model from disk.\n",
    "# *NOTE* disregard the scores... they are only to evaluate if optimization contaminates the feature and not relevent to the whole moodel.\n",
    "response_columns = ['retweet_timestamp_indicator', 'reply_timestamp_indicator',\n",
    "       'retweet_with_comment_timestamp_indicator', 'like_timestamp_indicator']\n",
    "from xgboost import XGBClassifier\n",
    "import timeit\n",
    "import numpy as np\n",
    "from sklearn.metrics import average_precision_score, log_loss\n",
    "model_dict = {}\n",
    "X = train.loc[:, fcols]\n",
    "Xv = val.loc[:, fcols]\n",
    "Xt = test.loc[:, fcols]\n",
    "for response in response_columns:\n",
    "    xgb = XGBClassifier(max_depth=6, n_estimators=250, learning_rate = 0.1, n_jobs=8, num_parallel_tree  = 1, \n",
    "                        tree_method='hist', subsample = 0.8, reg_alpha = 0.1, reg_lambda = 0.01, colsample_bytree=0.7)\n",
    "    # mask = np.random.choice([True, False], size=feature_engineered_df.shape[0], p=[0.8, 0.2])\n",
    "\n",
    "    y = train[response]\n",
    "    \n",
    "    Xtrain = X\n",
    "    ytrain = y\n",
    "    \n",
    "    Xtest = Xv\n",
    "    ytest = test[response]\n",
    "    \n",
    "    xgb.fit(Xtrain, ytrain, eval_metric=\"aucpr\", eval_set=[(Xv, val[response])], early_stopping_rounds=10)\n",
    "    model_dict[response] = xgb\n",
    "    \n",
    "    inf_start = timeit.default_timer()\n",
    "    pred = xgb.predict_proba(Xt)\n",
    "    inf_end = timeit.default_timer()\n",
    "    print(f\"Inference rate on {response} is {len(pred)/(inf_end - inf_start)} samples/sec\")\n",
    "    print(f\"AP score of {response} is {average_precision_score(ytest, pred[:, 1])}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_m = np.zeros(shape=X.shape[0])\n",
    "retweet_mask = feature_engineered_df['retweet_timestampe_indicator'] == 1\n",
    "y_m[retweet_mask] = 1\n",
    "retweetwc_mask = feature_engineered_df['retweet_with_comment_timestamp_indicator'] == 1\n",
    "y_m[retweetwc_mask] = 2\n",
    "\n",
    "xgb = XGBClassifier(num_class = 3, max_depth=3, n_estimators=250, learning_rate = 0.1, n_jobs=8, objective = 'multi:softmax', tree_method='hist', parallel_trees=10)\n",
    "mask = np.random.choice([True, False], size=feature_engineered_df.shape[0], p=[0.2, 0.8])\n",
    "\n",
    "Xtrain = X.iloc[mask, :]\n",
    "ytrain = y_m[mask]\n",
    "\n",
    "Xtest = X.iloc[~mask, :]\n",
    "ytest = y_m[~mask]\n",
    "\n",
    "xgb.fit(Xtrain, ytrain.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = xgb.predict(Xtest)\n",
    "print(np.unique(pred))\n",
    "# print(f\"{average_precision_score(ytest, pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_f = pred.astype(int) == 1\n",
    "y_f = y_f.astype(np.int)\n",
    "print(np.unique(y_f))\n",
    "average_precision_score(feature_engineered_df['retweet_timestampe_indicator'][~mask], y_f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
