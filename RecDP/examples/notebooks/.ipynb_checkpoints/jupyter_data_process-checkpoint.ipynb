{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 203)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<tokenize>\"\u001b[0;36m, line \u001b[0;32m203\u001b[0m\n\u001b[0;31m    'bert-base-multilingual-cased', do_lower_case=False)\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "#!/env/bin/python\n",
    "\n",
    "import init\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql import *\n",
    "from pyspark import *\n",
    "import pyspark.sql.functions as f\n",
    "from timeit import default_timer as timer\n",
    "import logging\n",
    "from RecsysSchema import RecsysSchema\n",
    "from pyrecdp.data_processor import *\n",
    "from pyrecdp.utils import *\n",
    "import hashlib\n",
    "\n",
    "def categorifyAllFeatures(df, proc, output_name=\"categorified\", gen_dict=False):\n",
    "    dict_dfs = []\n",
    "    if gen_dict:\n",
    "        # only call below function when target dicts were not pre-prepared\n",
    "        op_multiItems = GenerateDictionary(\n",
    "            ['present_domains', 'present_links', 'hashtags'], doSplit=True)\n",
    "        op_singleItems = GenerateDictionary(['tweet_id', 'language', {'src_cols': [\n",
    "                                            'engaged_with_user_id', 'enaging_user_id'], 'col_name': 'user_id'}])\n",
    "        proc.reset_ops([op_multiItems, op_singleItems])\n",
    "        t1 = timer()\n",
    "        dict_dfs = proc.generate_dicts(df)\n",
    "        t2 = timer()\n",
    "        print(\"Generate Dictionary took %.3f\" % (t2 - t1))\n",
    "    else:\n",
    "        # or we can simply load from pre-gened\n",
    "        dict_names = ['hashtags', 'language', 'present_domains',\n",
    "                      'present_links', 'tweet_id', 'user_id']\n",
    "        dict_dfs = [{'col_name': name, 'dict': proc.spark.read.parquet(\n",
    "            \"%s/%s/%s/%s\" % (proc.path_prefix, proc.current_path, proc.dicts_path, name))} for name in dict_names]\n",
    "\n",
    "    # Below codes are for inference\n",
    "    # op_multiItems = GenerateDictionary(\n",
    "    #    ['present_domains', 'present_links', 'hashtags'], doSplit=True)\n",
    "    # op_singleItems = GenerateDictionary(['tweet_id', 'language', {'src_cols': [\n",
    "    #                                     'engaged_with_user_id', 'enaging_user_id'], 'col_name': 'user_id'}])\n",
    "    # proc.reset_ops([op_multiItems, op_singleItems])\n",
    "    # t1 = timer()\n",
    "    # dict_dfs = proc.merge_dicts(df, dict_dfs)\n",
    "    # t2 = timer()\n",
    "    # print(\"Merge Dictionary took %.3f\" % (t2 - t1))\n",
    "    # ###############################\n",
    "\n",
    "    # pre-defined dict\n",
    "    # pre-define\n",
    "    media = {\n",
    "        '': 0,\n",
    "        'GIF': 1,\n",
    "        'GIF_GIF': 2,\n",
    "        'GIF_Photo': 3,\n",
    "        'GIF_Video': 4,\n",
    "        'Photo': 5,\n",
    "        'Photo_GIF': 6,\n",
    "        'Photo_Photo': 7,\n",
    "        'Photo_Video': 8,\n",
    "        'Video': 9,\n",
    "        'Video_GIF': 10,\n",
    "        'Video_Photo': 11,\n",
    "        'Video_Video': 12\n",
    "    }\n",
    "\n",
    "    tweet_type = {'Quote': 0, 'Retweet': 1, 'TopLevel': 2}\n",
    "\n",
    "    media_df = proc.spark.createDataFrame(convert_to_spark_dict(media))\n",
    "    tweet_type_df = proc.spark.createDataFrame(\n",
    "        convert_to_spark_dict(tweet_type))\n",
    "\n",
    "    dict_dfs.append({'col_name': 'present_media', 'dict': media_df})\n",
    "    dict_dfs.append({'col_name': 'tweet_type', 'dict': tweet_type_df})\n",
    "\n",
    "    for i in dict_dfs:\n",
    "        dict_name = i['col_name']\n",
    "        dict_df = i['dict']\n",
    "        print(\"%s has numRows as %d\" % (dict_name, dict_df.count()))\n",
    "\n",
    "    ###### 2. define operations and append them to data processor ######\n",
    "\n",
    "    # 1. define operations\n",
    "    # 1.1 fill na and features\n",
    "    op_fillna_str = FillNA(\n",
    "        ['present_domains', 'present_links', 'hashtags', 'present_media', 'tweet_id'], \"\")\n",
    "    op_feature_modification_type_convert = FeatureModification(cols=['tweet_timestamp',\n",
    "                                                                     'engaged_with_user_follower_count',\n",
    "                                                                     'engaged_with_user_following_count',\n",
    "                                                                     'engaged_with_user_account_creation',\n",
    "                                                                     'enaging_user_follower_count',\n",
    "                                                                     'enaging_user_following_count',\n",
    "                                                                     'enaging_user_account_creation'], op='toInt')\n",
    "    op_feature_modification_present_media_replace = FeatureModification(\n",
    "        cols={'present_media': \"f.concat_ws('_', f.slice(f.split(f.col('present_media'),'\\t'), 1, 2))\"}, op='inline')\n",
    "    op_feature_add_len_hashtags = FeatureAdd(\n",
    "        cols={'len_hashtags': \"f.when(f.col('hashtags') == '', f.lit(0)).otherwise(f.size(f.split(f.col('hashtags'), '\\t')))\"}, op='inline')\n",
    "    op_feature_add_len_domains = FeatureAdd(\n",
    "        cols={'len_domains': \"f.when(f.col('present_domains') == '', f.lit(0)).otherwise(f.size(f.split(f.col('present_domains'), '\\t')))\"}, op='inline')\n",
    "    op_feature_add_len_links = FeatureAdd(\n",
    "        cols={'len_links': \"f.when(f.col('present_links') == '', f.lit(0)).otherwise(f.size(f.split(f.col('present_links'), '\\t')))\"}, op='inline')\n",
    "    op_new_feature_dt_dow = FeatureAdd(cols={\n",
    "        \"dt_dow\": \"f.dayofweek(f.from_unixtime(f.col('tweet_timestamp'))).cast(t.IntegerType())\",\n",
    "        \"dt_hour\": \"f.hour(f.from_unixtime(f.col('tweet_timestamp'))).cast(t.IntegerType())\",\n",
    "        \"dt_minute\": \"f.minute(f.from_unixtime(f.col('tweet_timestamp'))).cast(t.IntegerType())\",\n",
    "        \"dt_second\": \"f.second(f.from_unixtime(f.col('tweet_timestamp'))).cast(t.IntegerType())\"}, op='inline')\n",
    "    op_new_feature_origin = FeatureAdd(\n",
    "        cols={\"origin_tweet_id\": \"f.col('tweet_id')\", \"origin_engaging_user_id\": \"f.col('enaging_user_id')\"}, op='inline')\n",
    "    op_fillna_num = FillNA(['tweet_timestamp'], -1)\n",
    "    ops = [op_fillna_str,\n",
    "           op_feature_modification_type_convert, op_feature_modification_present_media_replace,\n",
    "           op_feature_add_len_hashtags, op_feature_add_len_domains, op_feature_add_len_links,\n",
    "           op_new_feature_dt_dow, op_new_feature_origin, op_fillna_num]\n",
    "    proc.reset_ops(ops)\n",
    "\n",
    "    # 1.3 categorify\n",
    "    # since language dict is small, we may use udf to make partition more even\n",
    "    op_categorify_multi = Categorify(\n",
    "        ['present_domains', 'present_links', 'hashtags'], dict_dfs=dict_dfs, doSplit=True, keepMostFrequent=True)\n",
    "    op_categorify = Categorify(['language', 'tweet_id', 'present_media', 'tweet_type', {\n",
    "        'engaged_with_user_id': 'user_id'}, {'enaging_user_id': 'user_id'}], dict_dfs=dict_dfs)\n",
    "\n",
    "    op_fillna_for_categorified = FillNA(['present_domains', 'present_links', 'hashtags', 'language',\n",
    "                                         'tweet_id', 'present_media', 'tweet_type', 'engaged_with_user_id', 'enaging_user_id'], -1)\n",
    "    ops_1 = [op_categorify_multi, op_categorify, op_fillna_for_categorified]\n",
    "    proc.append_ops(ops_1)\n",
    "\n",
    "    ##### 3. do data transform(data frame materialize) #####\n",
    "    t1 = timer()\n",
    "    df = proc.transform(df, output_name)\n",
    "    t2 = timer()\n",
    "    print(\"Data Process and udf categorify took %.3f\" % (t2 - t1))\n",
    "    return df\n",
    "\n",
    "\n",
    "def categorifyTweetText(df, proc, output_name=\"tweet_text_categorified_20days\", gen_dict=False):\n",
    "    dict_dfs = []\n",
    "    if gen_dict:\n",
    "        # only call below function when target dicts were not pre-prepared\n",
    "        op_multiItems = GenerateDictionary(\n",
    "            ['tweet'], doSplit=True, withCount=True, sep=' ')\n",
    "        proc.reset_ops([op_multiItems])\n",
    "        ##### transform #####\n",
    "        t1 = timer()\n",
    "        dict_dfs = proc.generate_dicts(df)\n",
    "        t2 = timer()\n",
    "        print(\"Generate Dictionary took %.3f\" % (t2 - t1))\n",
    "    else:\n",
    "        # or we can simply load from pre-gened\n",
    "        name = \"tweet\"\n",
    "        tweet_dict_df = proc.spark.read.parquet(\n",
    "            \"%s/%s/%s/%s\" % (proc.path_prefix, proc.current_path, proc.dicts_path, name))\n",
    "        dict_dfs = [{'col_name': 'tweet', 'dict': tweet_dict_df}]\n",
    "\n",
    "    tweet_dict_df = dict_dfs[0]['dict']\n",
    "    freqRange = [2, 100000]\n",
    "    tweet_dict_df = tweet_dict_df.filter((f.col('count') <= f.lit(\n",
    "        freqRange[1])) & (f.col('count') >= f.lit(freqRange[0])))\n",
    "    op_fillNA = FillNA(['tweet'], \"\")\n",
    "    op_rename = FeatureAdd(cols={\"original_tweet\": \"f.col('tweet')\"}, op='inline')\n",
    "    op_categorify = Categorify(['tweet'], dict_dfs=dict_dfs, doSplit=True, sep=' ', doSortForArray=True)\n",
    "    proc.reset_ops([op_fillNA, op_rename, op_categorify])\n",
    "    t1 = timer()\n",
    "    df = proc.transform(df, name=output_name)\n",
    "    t2 = timer()\n",
    "    print(\"Categorify tweet took %.3f\" % (t2 - t1))\n",
    "    return df    \n",
    "\n",
    "\n",
    "def categorifyTweetHash(df, proc, output_name=\"tweet_text_processed_20days\", gen_dict=False):\n",
    "    dict_dfs = []\n",
    "    if gen_dict:\n",
    "        # only call below function when target dicts were not pre-prepared\n",
    "        op_gen_dict = GenerateDictionary(['tw_hash'])\n",
    "        proc.reset_ops([op_gen_dict])\n",
    "        ##### transform #####\n",
    "        t1 = timer()\n",
    "        dict_dfs = proc.generate_dicts(df)\n",
    "        t2 = timer()\n",
    "        print(\"Generate Dictionary took %.3f\" % (t2 - t1))\n",
    "    else:\n",
    "        # or we can simply load from pre-gened\n",
    "        name = \"tw_hash\"\n",
    "        tw_hash_dict_df = proc.spark.read.parquet(\n",
    "            \"%s/%s/%s/%s\" % (proc.path_prefix, proc.current_path, proc.dicts_path, name))\n",
    "        dict_dfs = [{'col_name': 'tw_hash', 'dict': tw_hash_dict_df}]\n",
    "\n",
    "    op_categorify = Categorify(['tw_hash'], dict_dfs=dict_dfs)\n",
    "    proc.reset_ops([op_categorify])\n",
    "    t1 = timer()\n",
    "    df = proc.transform(df, name=output_name)\n",
    "    t2 = timer()\n",
    "    print(\"Categorify tw_hash took %.3f\" % (t2 - t1))\n",
    "    return df\n",
    "\n",
    "\n",
    "def decodeBertTokenizer(df, proc, output_name=\"data_all_with_text\"):\n",
    "    from transformers import BertTokenizer\n",
    "    tokenizer = BertTokenizer.from_pretrained(\n",
    "        'bert-base-multilingual-cased', do_lower_case=False)\n",
    "\n",
    "    # define UDF\n",
    "    tokenizer_decode = f.udf(lambda x: tokenizer.decode(\n",
    "        [int(n) for n in x.split('\\t')]))\n",
    "    format_url = f.udf(lambda x: x.replace(\n",
    "        'https : / / t. co / ', 'https://t.co/').replace('@ ', '@'))\n",
    "\n",
    "    # define operations\n",
    "    op_feature_modification_tokenizer_decode = FeatureAdd(\n",
    "        cols={'tweet': 'text_tokens'}, udfImpl=tokenizer_decode)\n",
    "    op_feature_modification_format_url = FeatureModification(\n",
    "        cols=['tweet'], udfImpl=format_url)\n",
    "\n",
    "    # execute\n",
    "    proc.reset_ops([op_feature_modification_tokenizer_decode,\n",
    "                    op_feature_modification_format_url])\n",
    "    t1 = timer()\n",
    "    df = proc.transform(df)\n",
    "    t2 = timer()\n",
    "    print(\"BertTokenizer decode and format took %.3f\" % (t2 - t1))\n",
    "\n",
    "    def extract_hash(text, split_text='@', no=0):\n",
    "        text = text.lower()\n",
    "        uhash = ''\n",
    "        text_split = text.split('@')\n",
    "        if len(text_split) > (no+1):\n",
    "            text_split = text_split[no+1].split(' ')\n",
    "            cl_loop = True\n",
    "            uhash += clean_text(text_split[0])\n",
    "            while cl_loop:\n",
    "                if len(text_split) > 1:\n",
    "                    if text_split[1] in ['_']:\n",
    "                        uhash += clean_text(text_split[1]) + \\\n",
    "                            clean_text(text_split[2])\n",
    "                        text_split = text_split[2:]\n",
    "                    else:\n",
    "                        cl_loop = False\n",
    "                else:\n",
    "                    cl_loop = False\n",
    "        hash_object = hashlib.md5(uhash.encode('utf-8'))\n",
    "        return hash_object.hexdigest()\n",
    "\n",
    "    def clean_text(text):\n",
    "        if len(text) > 1:\n",
    "            if text[-1] in ['!', '?', ':', ';', '.', ',']:\n",
    "                return(text[:-1])\n",
    "        return(text)\n",
    "\n",
    "    # features upon tweet\n",
    "    to_notsign = f.udf(lambda x: x.replace('\\[CLS\\] RT @', ''))\n",
    "    count_space = f.udf(lambda x: x.count(' '))\n",
    "    count_text_length = f.udf(lambda x: len(x))\n",
    "    user_defined_hash = f.udf(\n",
    "        lambda x: extract_hash(x, split_text='RT @', no=0))\n",
    "    # features upon tweet_nortsign\n",
    "    count_at = f.udf(lambda x: x.count('@'))\n",
    "    user_define_hash_1 = f.udf(lambda x: extract_hash(x))\n",
    "    user_define_hash_2 = f.udf(lambda x: extract_hash(x, no=1))\n",
    "\n",
    "    # features upon tweet\n",
    "    op_feature_add_tweet_nortsign = FeatureAdd(\n",
    "        cols={'tweet_nortsign': 'tweet'}, udfImpl=to_notsign)\n",
    "    op_feature_add_count_words = FeatureAdd(\n",
    "        cols={'count_words': 'tweet'}, udfImpl=count_space)\n",
    "    op_feature_add_count_char = FeatureAdd(\n",
    "        cols={'count_char': 'tweet'}, udfImpl=count_text_length)\n",
    "    op_feature_add_tw_uhash = FeatureAdd(\n",
    "        cols={'tw_uhash': 'tweet'}, udfImpl=user_defined_hash)\n",
    "    op_feature_add_tw_hash = FeatureAdd(\n",
    "        cols={'tw_hash': \"hash(col('tweet'))%1000000000\"}, op='inline')\n",
    "    # features upon tweet_nortsign\n",
    "    op_feature_add_count_at = FeatureAdd(\n",
    "        cols={'count_ats': 'tweet_nortsign'}, udfImpl=count_at)\n",
    "    op_feature_add_tw_uhash0 = FeatureAdd(\n",
    "        cols={'tw_hash0': 'tweet_nortsign'}, udfImpl=user_define_hash_1)\n",
    "    op_feature_add_tw_uhash1 = FeatureAdd(\n",
    "        cols={'tw_hash1': 'tweet_nortsign'}, udfImpl=user_define_hash_2)\n",
    "\n",
    "    # execute\n",
    "    proc.reset_ops([op_feature_add_tweet_nortsign, op_feature_add_count_words, op_feature_add_count_char,\n",
    "                    op_feature_add_tw_uhash, op_feature_add_tw_hash,\n",
    "                    op_feature_add_count_at, op_feature_add_tw_uhash0, op_feature_add_tw_uhash1])\n",
    "    t1 = timer()\n",
    "    df = proc.transform(df, output_name)\n",
    "    t2 = timer()\n",
    "    print(\"Adding Feature upon tweet and tweet_nortsign column took %.3f\" % (t2 - t1))\n",
    "    # expect to spend about 1000secs\n",
    "    return df\n",
    "\n",
    "\n",
    "def tweetFeatureEngineer(df, proc, output_name=\"tweet_feature_engineer_20days\"):\n",
    "    op_fillna_for_tweet = FillNA(['tweet'], \"\")\n",
    "    op_feature_add_tw_hash = FeatureAdd(\n",
    "        cols={'tw_hash': \"f.hash(f.col('original_tweet'))%1000000000\"}, op='inline')\n",
    "    op_feature_add_tw_first_word = FeatureAdd(\n",
    "        {'tw_first_word': \"f.col('tweet').getItem(0)\"}, op='inline')\n",
    "    op_feature_add_tw_second_word = FeatureAdd(\n",
    "        {'tw_second_word': \"f.col('tweet').getItem(1)\"}, op='inline')\n",
    "    op_feature_add_tw_last_word = FeatureAdd(\n",
    "        {'tw_last_word': \"f.col('tweet').getItem(f.size(f.col('tweet')) - 1)\"}, op='inline')\n",
    "    op_feature_add_tw_second_last_word = FeatureAdd(\n",
    "        {'tw_llast_word': \"f.col('tweet').getItem(f.size(f.col('tweet')) - 1)\"}, op='inline')\n",
    "    op_feature_add_tw_word_len = FeatureAdd(\n",
    "        {'tw_len': \"f.size(f.col('tweet'))\"}, op='inline')\n",
    "    op_feature_modification_fillna = FillNA(\n",
    "        ['tw_hash', 'tw_first_word', 'tw_second_word', 'tw_last_word', 'tw_llast_word', 'tw_len'], -1)\n",
    "\n",
    "    proc.reset_ops([op_fillna_for_tweet, op_feature_add_tw_hash, op_feature_add_tw_first_word, op_feature_add_tw_second_word,\n",
    "                    op_feature_add_tw_last_word, op_feature_add_tw_second_last_word, op_feature_add_tw_word_len,\n",
    "                    op_feature_modification_fillna])\n",
    "    t1 = timer()\n",
    "    df = proc.transform(df, name=output_name)\n",
    "    t2 = timer()\n",
    "    print(\"feature engineering upon Frequency encoded tweet column took %.3f\" % (t2 - t1))\n",
    "    return df\n",
    "\n",
    "def get_train_data_with_amount_of_days(df, proc, num_of_day = 20):\n",
    "    categorified_with_text_df = df\n",
    "    categorified_with_text_df.cache()\n",
    "    # 1.1 get timestamp range\n",
    "    import datetime\n",
    "    min_timestamp = categorified_with_text_df.select('tweet_timestamp').agg({'tweet_timestamp': 'min'}).collect()[0]['min(tweet_timestamp)']\n",
    "    max_timestamp = categorified_with_text_df.select('tweet_timestamp').agg({'tweet_timestamp': 'max'}).collect()[0]['max(tweet_timestamp)']\n",
    "    seconds_in_day = 3600 * 24\n",
    "\n",
    "    print(\n",
    "        \"min_timestamp is %s, max_timestamp is %s, %d days max is %s\" % (\n",
    "            datetime.datetime.fromtimestamp(min_timestamp).strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            datetime.datetime.fromtimestamp(max_timestamp).strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            num_of_day,\n",
    "            datetime.datetime.fromtimestamp(min_timestamp + num_of_day * seconds_in_day).strftime('%Y-%m-%d %H:%M:%S')\n",
    "        ))\n",
    "\n",
    "    time_range_split = {\n",
    "        'target': (min_timestamp, seconds_in_day * num_of_day + min_timestamp)\n",
    "    }\n",
    "\n",
    "    print(time_range_split)\n",
    "\n",
    "    # 1.2 save ranged data for train\n",
    "    # filtering out train range data and save\n",
    "    train_start, train_end = time_range_split['target']\n",
    "    df = categorified_with_text_df.filter(\n",
    "        (f.col('tweet_timestamp') >= f.lit(train_start)) & (f.col('tweet_timestamp') < f.lit(train_end)))\n",
    "    output_path = \"%s/%s/data_splitted_by_%ddays\" % (proc.path_prefix, proc.current_path, num_of_day)\n",
    "    df.write.format('parquet').mode('overwrite').save(output_path)\n",
    "    return proc.spark.read.parquet(output_path)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_timestamp is 2021-02-04 08:00:00, max_timestamp is 2021-02-25 07:59:59, 5 days max is 2021-02-09 08:00:00\n",
      "{'target': (1612396800, 1612828800), 'validate': (1612396800, 1612569600), 'train': (1612569600, 1612483200), 'test': (1613952000, 1614124800)}\n",
      "generate_dict_dfs withCount = False\n",
      "generate_dict_dfs withCount = False\n",
      "generate_dict_dfs withCount = False\n",
      "generate_dict_dfs withCount = False\n",
      "generate_dict_dfs withCount = False\n",
      "generate_dict_dfs withCount = False\n",
      "Generate Dictionary took 235.714\n",
      "present_domains has numRows as 369189\n",
      "present_links has numRows as 3786346\n",
      "hashtags has numRows as 2433894\n",
      "tweet_id has numRows as 76389619\n",
      "language has numRows as 66\n",
      "user_id has numRows as 30514583\n",
      "present_media has numRows as 13\n",
      "tweet_type has numRows as 3\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o758.save.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:231)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:188)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:132)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:131)\n\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:989)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:989)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:438)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:415)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:293)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Cannot broadcast the table that is larger than 8GB: 12 GB\n\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.$anonfun$relationFuture$1(BroadcastExchangeExec.scala:144)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$1(SQLExecution.scala:185)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-2a05ee279dee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sep'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'\\x01'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_prefix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0moriginal_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_train_data_with_amount_of_days\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcategorifyAllFeatures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m# ===============================================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-67fbc7fb6bef>\u001b[0m in \u001b[0;36mcategorifyAllFeatures\u001b[0;34m(df, proc, output_name, gen_dict)\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;31m##### 3. do data transform(data frame materialize) #####\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0mt1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m     \u001b[0mt2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Data Process and udf categorify took %.3f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mt2\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/nvme2/chendi/BlueWhale/frameworks.bigdata.bluewhale/RecDP/pyrecdp/data_processor.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, df, name)\u001b[0m\n\u001b[1;32m    666\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mop\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    667\u001b[0m             \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspark\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 668\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaterialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    669\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgenerate_dicts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/nvme2/chendi/BlueWhale/frameworks.bigdata.bluewhale/RecDP/pyrecdp/data_processor.py\u001b[0m in \u001b[0;36mmaterialize\u001b[0;34m(self, df, df_name, method)\u001b[0m\n\u001b[1;32m    660\u001b[0m                 save_path = \"%s/%s/%s\" % (self.path_prefix,\n\u001b[1;32m    661\u001b[0m                                           self.current_path, df_name)\n\u001b[0;32m--> 662\u001b[0;31m             \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'parquet'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'overwrite'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    663\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    664\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/hadoop/spark/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m   1107\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1108\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1109\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/hadoop/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/hadoop/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/hadoop/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o758.save.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:231)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:188)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:132)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:131)\n\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:989)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:989)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:438)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:415)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:293)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Cannot broadcast the table that is larger than 8GB: 12 GB\n\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.$anonfun$relationFuture$1(BroadcastExchangeExec.scala:144)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$1(SQLExecution.scala:185)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "current_path = \"/recsys2021/1day\"\n",
    "path_prefix = \"hdfs://\"\n",
    "original_folder = \"/recsys2021/decompress\"\n",
    "dicts_folder = \"recsys_dicts/\"\n",
    "recsysSchema = RecsysSchema()\n",
    "\n",
    "##### 1. Start spark and initialize data processor #####\n",
    "t0 = timer()\n",
    "spark = SparkSession.builder.master('yarn')\\\n",
    "    .appName(\"Recsys2021_data_process\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "schema = recsysSchema.toStructType()\n",
    "\n",
    "# 1.1 prepare dataFrames\n",
    "# 1.2 create RecDP DataProcessor\n",
    "proc = DataProcessor(spark, path_prefix,\n",
    "                     current_path=current_path, dicts_path=dicts_folder)\n",
    "\n",
    "# ===============================================\n",
    "# basic: Do categorify for all columns for xgboost\n",
    "# df = spark.read.schema(schema).option('sep', '\\x01').csv(path_prefix + original_folder)\n",
    "# df = get_train_data_with_amount_of_days(df, proc, 5) # 5 days will use 1 day for train dataset\n",
    "df = spark.read.parquet(\"%s/data_splitted_by_5days/\" % current_path)\n",
    "df = categorifyAllFeatures(df, proc, gen_dict=False)\n",
    "\n",
    "# ===============================================\n",
    "# optional: do bert decode\n",
    "# df = spark.read.parquet(\"%s/categorfied/\" % current_path)\n",
    "# df = decodeBertTokenizer(df, proc)\n",
    "\n",
    "# ===============================================\n",
    "# optional: do tweet text feature engineering\n",
    "# step1: categorify tweet text\n",
    "# df = spark.read.parquet(\"%s/processed_for_20days/\" % current_path)\n",
    "# df = categorifyTweetText(df, proc, gen_dict=False)\n",
    "\n",
    "# step2: add new feature with categorified tweet\n",
    "# df = spark.read.parquet(\"%s/tweet_text_categorified_20days/\" % current_path)\n",
    "# df = tweetFeatureEngineer(df, proc)\n",
    "\n",
    "# step3: categorify tweet hash\n",
    "# df = spark.read.parquet(\"%s/tweet_feature_engineer_20days/\" % current_path)\n",
    "# df = categorifyTweetHash(df, proc, gen_dict=True)\n",
    "\n",
    "# ==============================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      " tweet_id                           | 19996776                                                                                                                                                                                                   \n",
      " text_tokens                        | 101\t779\t33442\t10429\t788\t45316\t10461\t59901\t15470\t20884\t785\t104230\t11294\t788\t45935\t58200\t10582\t76216\t10700\t788\t11626\t41484\t69861\t11852\t14556\t791\t10700\t11626\t41484\t71604\t50695\t10429\t781\t37560\t14146\t100\t102 \n",
      " hashtags                           | 0                                                                                                                                                                                                          \n",
      " present_media                      | 0                                                                                                                                                                                                          \n",
      " present_links                      | 0                                                                                                                                                                                                          \n",
      " present_domains                    | 0                                                                                                                                                                                                          \n",
      " tweet_type                         | 2                                                                                                                                                                                                          \n",
      " language                           | 7                                                                                                                                                                                                          \n",
      " tweet_timestamp                    | 1612444554                                                                                                                                                                                                 \n",
      " engaged_with_user_id               | 376429                                                                                                                                                                                                     \n",
      " engaged_with_user_follower_count   | 8904                                                                                                                                                                                                       \n",
      " engaged_with_user_following_count  | 7997                                                                                                                                                                                                       \n",
      " engaged_with_user_is_verified      | false                                                                                                                                                                                                      \n",
      " engaged_with_user_account_creation | 1434108231                                                                                                                                                                                                 \n",
      " enaging_user_id                    | 65236                                                                                                                                                                                                      \n",
      " enaging_user_follower_count        | 832                                                                                                                                                                                                        \n",
      " enaging_user_following_count       | 560                                                                                                                                                                                                        \n",
      " enaging_user_is_verified           | false                                                                                                                                                                                                      \n",
      " enaging_user_account_creation      | 1396221377                                                                                                                                                                                                 \n",
      " engagee_follows_engager            | true                                                                                                                                                                                                       \n",
      " reply_timestamp                    | 0                                                                                                                                                                                                          \n",
      " retweet_timestamp                  | 0                                                                                                                                                                                                          \n",
      " retweet_with_comment_timestamp     | 0                                                                                                                                                                                                          \n",
      " like_timestamp                     | 1                                                                                                                                                                                                          \n",
      " len_hashtags                       | 0                                                                                                                                                                                                          \n",
      " len_domains                        | 0                                                                                                                                                                                                          \n",
      " len_links                          | 0                                                                                                                                                                                                          \n",
      " engage_time                        | 0                                                                                                                                                                                                          \n",
      " dt_dow                             | 5                                                                                                                                                                                                          \n",
      " dt_hour                            | 21                                                                                                                                                                                                         \n",
      " dt_minute                          | 15                                                                                                                                                                                                         \n",
      " dt_second                          | 54                                                                                                                                                                                                         \n",
      " original_tweet                     | [CLS] طبعا محبي الصيف قاعدين مضايقين انهم مش بيعرقوا ومش بيشممونا عرقهم [UNK] [SEP]                                                                                                                        \n",
      " tweet                              | [2, 3, 5, 2338, 23304, 35590, 45272, 78236, 85008, 294356, 995387, 2243190, 3615464, 3850914]                                                                                                              \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.parquet(\"/recsys2021/1day/tweet_text_categorified_20days/\")\n",
    "df.show(1, vertical=True, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_timestamp is 2021-02-04 08:00:00, max_timestamp is 2021-02-25 07:59:59, 20 days max is 2021-02-24 08:00:00\n",
      "{'20days': (1612396800, 1612483200), 'validate': (1612396800, 1612569600), 'train': (1612569600, 1613952000), 'test': (1613952000, 1614124800)}\n"
     ]
    }
   ],
   "source": [
    "# optional: adding new features upon tweet text\n",
    "categorified_with_text_df = spark.read.parquet(\"/recsys2021/data_all_with_text\")\n",
    "# 1.1 get timestamp range\n",
    "import datetime\n",
    "min_timestamp = categorified_with_text_df.select('tweet_timestamp').agg({'tweet_timestamp': 'min'}).collect()[0]['min(tweet_timestamp)']\n",
    "max_timestamp = categorified_with_text_df.select('tweet_timestamp').agg({'tweet_timestamp': 'max'}).collect()[0]['max(tweet_timestamp)']\n",
    "seconds_in_day = 3600 * 24\n",
    "\n",
    "print(\n",
    "    \"min_timestamp is %s, max_timestamp is %s, 20 days max is %s\" % (\n",
    "        datetime.datetime.fromtimestamp(min_timestamp).strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        datetime.datetime.fromtimestamp(max_timestamp).strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        datetime.datetime.fromtimestamp(min_timestamp + 20 * seconds_in_day).strftime('%Y-%m-%d %H:%M:%S')\n",
    "    ))\n",
    "\n",
    "time_range_split = {\n",
    "    '20days': (min_timestamp, seconds_in_day * 20 + min_timestamp),\n",
    "    'validate': (min_timestamp, seconds_in_day * 2 + min_timestamp),\n",
    "    'train': (seconds_in_day * 2 + min_timestamp, seconds_in_day * 18 + min_timestamp),\n",
    "    'test': (seconds_in_day * 18 + min_timestamp, seconds_in_day * 20 + min_timestamp)\n",
    "}\n",
    "\n",
    "print(time_range_split)\n",
    "\n",
    "# 1.2 save ranged data for train\n",
    "# filtering out train range data and save\n",
    "train_start, train_end = time_range_split['20days']\n",
    "df = categorified_with_text_df.filter(\n",
    "    (f.col('tweet_timestamp') >= f.lit(train_start)) & (f.col('tweet_timestamp') < f.lit(train_end)))\n",
    "df.cache()\n",
    "train_data_processed = \"/recsys2021/1day/processed_for_20days\"\n",
    "df.write.format('parquet').mode('overwrite').save(path_prefix + train_data_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hashtags has numRows as 6653969, maximun is 6653968\n",
      "language has numRows as 66, maximun is 65\n",
      "present_domains has numRows as 900139, maximun is 900138\n",
      "present_links has numRows as 15025550, maximun is 15025549\n",
      "tweet_id has numRows as 323033151, maximun is 323033150\n",
      "user_id has numRows as 46809755, maximun is 46809754\n",
      "tweet has numRows as 64809002, maximun is 64809001\n"
     ]
    }
   ],
   "source": [
    "dict_names = ['hashtags', 'language', 'present_domains','present_links', 'tweet_id', 'user_id', 'tweet']\n",
    "dict_dfs = [{'col_name': name, 'dict': spark.read.parquet(\"%s/recsys2021/recsys_dicts/%s\" % (path_prefix, name))} for name in dict_names]\n",
    "for i in dict_dfs:\n",
    "    dict_name = i['col_name']\n",
    "    dict_df = i['dict']\n",
    "    print(\"%s has numRows as %d, maximun is %d\" % (dict_name, dict_df.count(), dict_df.agg({'dict_col_id': \"max\"}).collect()[0]['max(dict_col_id)']))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
