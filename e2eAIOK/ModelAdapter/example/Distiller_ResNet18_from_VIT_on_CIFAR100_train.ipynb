{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b92539e",
   "metadata": {},
   "source": [
    "# Train Resnet18 from VIT with Distiller on CIFAR100\n",
    "Distiller can transfer knowledge from a heavy model (teacher) to a light one (student) with different structure.\n",
    "\n",
    "* Teacher is a large model pretrained on specific dataset, which contains sufficient knowledge for this task, while the student model has much smaller structure. Distiller trains the student not only on the dataset, but also with the help of teacherâ€™s knowledge.\n",
    "* Distiller can take use of the knowledge from the existing pretrained large models but use much less training time. It can also significantly improve the converge  speed and predicting accuracy of a small model, which is very helpful for inference.\n",
    "![Distiller](../doc/imgs/distiller.png)\n",
    "\n",
    "In this notebook, we will do distillation from VIT to ResNet18 to show the basic usage of Model Adapter Distiller."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad2fc58",
   "metadata": {},
   "source": [
    "# Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f878badd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms,datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import timm\n",
    "import transformers\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5646a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/vmagent/app/TLK/frameworks.bigdata.AIDK/AIDK/\")\n",
    "from TransferLearningKit.src.engine_core.transferrable_model import make_transferrable_with_knowledge_distillation\n",
    "from TransferLearningKit.src.engine_core.distiller import KD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec28cbf4",
   "metadata": {},
   "source": [
    "# Prepare Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3893b32d",
   "metadata": {},
   "source": [
    "### Define Data Preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c016312d",
   "metadata": {},
   "outputs": [],
   "source": [
    "CIFAR100_TRAIN_MEAN = (0.5070751592371323, 0.48654887331495095, 0.4409178433670343) # mean for 3 channels\n",
    "CIFAR100_TRAIN_STD = (0.2673342858792401, 0.2564384629170883, 0.27615047132568404)  # std for 3 channels\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "  transforms.RandomCrop(32, padding=4),\n",
    "  transforms.RandomHorizontalFlip(),\n",
    "  transforms.Resize(224),  # pretrained model is trained on large imgage size, scale 32x32 to 112x112\n",
    "  transforms.ToTensor(),\n",
    "  transforms.Normalize(CIFAR100_TRAIN_MEAN, CIFAR100_TRAIN_STD)\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "  transforms.RandomCrop(32, padding=4),\n",
    "  transforms.Resize(224),  # pretrained model is trained on large imgage size, scale 32x32 to 112x112\n",
    "  transforms.ToTensor(),\n",
    "  transforms.Normalize(CIFAR100_TRAIN_MEAN, CIFAR100_TRAIN_STD)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85fb033",
   "metadata": {},
   "source": [
    "### Prepare dataset and dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "69896736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "num_workers = 1 # data worker\n",
    "data_folder='./dataset' # dataset location\n",
    "train_set = datasets.CIFAR100(root=data_folder, train=True, download=True, transform=train_transform)\n",
    "test_set = datasets.CIFAR100(root=data_folder, train=False, download=True, transform=test_transform)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_set, batch_size=batch_size, shuffle=True, num_workers=1, drop_last=False)\n",
    "validate_loader = DataLoader(dataset=test_set, batch_size=batch_size, shuffle=True, num_workers=1, drop_last=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9eeba42",
   "metadata": {},
   "source": [
    "# Create Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06ea401",
   "metadata": {},
   "source": [
    "### Create Backbone model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40b8a7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = timm.create_model('resnet18', pretrained=False, num_classes=100).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6353b6d3",
   "metadata": {},
   "source": [
    "### Define Distiller \n",
    "When define distiller, we need to define teacher_type with a name start with \"huggingface\" if the teacher model comes from hugging face. Otherwise, don't need to set it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c86c941",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "teacher_model = transformers.ViTForImageClassification.from_pretrained('edumunozsala/vit_base-224-in21k-ft-cifar100')\n",
    "distiller= KD(teacher_model,teacher_type=\"huggingface_vit_base-224-in21k-ft-cifar100\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c1bd41",
   "metadata": {},
   "source": [
    "### Make Model transferrable with distiller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e1b2074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.02 s, sys: 1.97 s, total: 9.99 s\n",
      "Wall time: 21.1 s\n"
     ]
    }
   ],
   "source": [
    "model = make_transferrable_with_knowledge_distillation(model, loss_fn,distiller)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa226f93",
   "metadata": {},
   "source": [
    "# create optimizer and scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c9561a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "################# create optimizer #################\n",
    "init_lr = 0.01\n",
    "weight_decay = 0.005\n",
    "momentum = 0.9\n",
    "optimizer = optim.SGD(model.parameters(),lr=init_lr, weight_decay=weight_decay,momentum=momentum)\n",
    "################# create scheduler #################\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e633199",
   "metadata": {},
   "source": [
    "# Create Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "046525dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epoch = 1 # max 1 epoch\n",
    "print_interval = 10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db9fbc44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output,label):\n",
    "    pred = output.data.cpu().max(1)[1]\n",
    "    label = label.data.cpu()\n",
    "    if label.shape == output.shape:\n",
    "        label = label.max(1)[1]\n",
    "    return torch.mean((pred == label).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "874fb6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, model, optimizer, scheduler):\n",
    "        self._model = model\n",
    "        self._optimizer = optimizer\n",
    "        self._scheduler = scheduler\n",
    "        \n",
    "    def train(self, train_dataloader, valid_dataloader, max_epoch):\n",
    "        ''' \n",
    "        :param train_dataloader: train dataloader\n",
    "        :param valid_dataloader: validation dataloader\n",
    "        :param max_epoch: steps per epoch\n",
    "        '''\n",
    "        for epoch in range(0, max_epoch):\n",
    "            ################## train #####################\n",
    "            model.train()  # set training flag\n",
    "            for (cur_step,(data, label)) in enumerate(train_dataloader):\n",
    "                data = data.to(device)\n",
    "                label = label.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                output = model(data)\n",
    "                # loss_value = loss_fn(output, label)\n",
    "                loss_value = model.loss(output, label) # use model.loss\n",
    "                loss_value.backward()       \n",
    "                if cur_step%print_interval == 0:\n",
    "                    # batch_acc = accuracy(output,label)\n",
    "                    batch_acc = accuracy(output.backbone_output,label) # use output.backbone_output\n",
    "                    dt = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S') # date time\n",
    "                    # print(\"[{}] epoch {} step {} : training batch loss {:.4f}, training batch acc {:.4f}\".format(\n",
    "                    #   dt, epoch, cur_step, loss_value.item(), batch_acc.item()))\n",
    "                    print(\"[{}] epoch {} step {} : training batch loss {:.4f}, training batch acc {:.4f}\".format(\n",
    "                      dt, epoch, cur_step, loss_value.backbone_loss.item(), batch_acc.item())) # use loss_value.backbone_loss\n",
    "                self._optimizer.step()\n",
    "            self._scheduler.step()\n",
    "            ################## evaluate ######################\n",
    "            self.evaluate(model, valid_dataloader, epoch)\n",
    "            \n",
    "    def evaluate(self, model, valid_dataloader, epoch):\n",
    "        with torch.no_grad():\n",
    "            model.eval()  \n",
    "            backbone = model.backbone # use backbone in evaluation\n",
    "            loss_cum = 0.0\n",
    "            sample_num = 0\n",
    "            acc_cum = 0.0\n",
    "            for (cur_step,(data, label)) in enumerate(valid_dataloader):\n",
    "                data = data.to(device)\n",
    "                label = label.to(device)\n",
    "                # output = model(data)\n",
    "                output = backbone(data)[0] # use backbone in evaluation and backbone has multi output\n",
    "                batch_size = data.size(0)\n",
    "                sample_num += batch_size\n",
    "                loss_cum += loss_fn(output, label).item() * batch_size\n",
    "                acc_cum += accuracy(output, label).item() * batch_size\n",
    "            dt = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S') # date time\n",
    "            if sample_num > 0:\n",
    "                loss_value = loss_cum/sample_num\n",
    "                acc_value = acc_cum/sample_num\n",
    "            else:\n",
    "                loss_value = 0.0\n",
    "                acc_value = 0.0\n",
    "\n",
    "            print(\"[{}] epoch {} : evaluation loss {:.4f}, evaluation acc {:.4f}\".format(\n",
    "                dt, epoch, loss_value, acc_value))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc734272",
   "metadata": {},
   "source": [
    "# Train and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bae383b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-11-15 05:00:14] epoch 0 step 0 : training batch loss 4.6086, training batch acc 0.0234\n",
      "[2022-11-15 05:00:48] epoch 0 step 10 : training batch loss 4.5780, training batch acc 0.0078\n",
      "[2022-11-15 05:01:22] epoch 0 step 20 : training batch loss 4.5429, training batch acc 0.0156\n",
      "[2022-11-15 05:01:56] epoch 0 step 30 : training batch loss 4.4996, training batch acc 0.0391\n",
      "[2022-11-15 05:02:30] epoch 0 step 40 : training batch loss 4.4805, training batch acc 0.0391\n",
      "[2022-11-15 05:03:05] epoch 0 step 50 : training batch loss 4.4295, training batch acc 0.0547\n",
      "[2022-11-15 05:03:39] epoch 0 step 60 : training batch loss 4.4019, training batch acc 0.0781\n",
      "[2022-11-15 05:04:11] epoch 0 step 70 : training batch loss 4.3581, training batch acc 0.1016\n",
      "[2022-11-15 05:04:43] epoch 0 step 80 : training batch loss 4.3802, training batch acc 0.0391\n",
      "[2022-11-15 05:05:15] epoch 0 step 90 : training batch loss 4.3280, training batch acc 0.0703\n",
      "[2022-11-15 05:05:47] epoch 0 step 100 : training batch loss 4.2740, training batch acc 0.0781\n",
      "[2022-11-15 05:06:21] epoch 0 step 110 : training batch loss 4.3292, training batch acc 0.0469\n",
      "[2022-11-15 05:06:53] epoch 0 step 120 : training batch loss 4.3331, training batch acc 0.0547\n",
      "[2022-11-15 05:07:24] epoch 0 step 130 : training batch loss 4.3350, training batch acc 0.0547\n",
      "[2022-11-15 05:07:57] epoch 0 step 140 : training batch loss 4.3267, training batch acc 0.0703\n",
      "[2022-11-15 05:08:31] epoch 0 step 150 : training batch loss 4.2813, training batch acc 0.0469\n",
      "[2022-11-15 05:09:03] epoch 0 step 160 : training batch loss 4.3114, training batch acc 0.0625\n",
      "[2022-11-15 05:09:36] epoch 0 step 170 : training batch loss 4.3577, training batch acc 0.0547\n",
      "[2022-11-15 05:10:08] epoch 0 step 180 : training batch loss 4.2358, training batch acc 0.0547\n",
      "[2022-11-15 05:10:43] epoch 0 step 190 : training batch loss 4.2748, training batch acc 0.0547\n",
      "[2022-11-15 05:11:15] epoch 0 step 200 : training batch loss 4.2574, training batch acc 0.0781\n",
      "[2022-11-15 05:11:49] epoch 0 step 210 : training batch loss 4.3230, training batch acc 0.0547\n",
      "[2022-11-15 05:12:20] epoch 0 step 220 : training batch loss 4.2437, training batch acc 0.0781\n",
      "[2022-11-15 05:12:51] epoch 0 step 230 : training batch loss 4.2984, training batch acc 0.0703\n",
      "[2022-11-15 05:13:24] epoch 0 step 240 : training batch loss 4.2375, training batch acc 0.0938\n",
      "[2022-11-15 05:13:55] epoch 0 step 250 : training batch loss 4.2850, training batch acc 0.0547\n",
      "[2022-11-15 05:14:27] epoch 0 step 260 : training batch loss 4.1828, training batch acc 0.1250\n",
      "[2022-11-15 05:14:58] epoch 0 step 270 : training batch loss 4.1403, training batch acc 0.1172\n",
      "[2022-11-15 05:15:30] epoch 0 step 280 : training batch loss 4.1938, training batch acc 0.0625\n",
      "[2022-11-15 05:16:04] epoch 0 step 290 : training batch loss 4.1534, training batch acc 0.0781\n",
      "[2022-11-15 05:16:37] epoch 0 step 300 : training batch loss 4.3715, training batch acc 0.0156\n",
      "[2022-11-15 05:17:12] epoch 0 step 310 : training batch loss 4.1721, training batch acc 0.0781\n",
      "[2022-11-15 05:17:46] epoch 0 step 320 : training batch loss 4.2561, training batch acc 0.0703\n",
      "[2022-11-15 05:18:19] epoch 0 step 330 : training batch loss 4.1972, training batch acc 0.0781\n",
      "[2022-11-15 05:18:50] epoch 0 step 340 : training batch loss 4.1955, training batch acc 0.0938\n",
      "[2022-11-15 05:19:23] epoch 0 step 350 : training batch loss 4.2317, training batch acc 0.0547\n",
      "[2022-11-15 05:19:55] epoch 0 step 360 : training batch loss 4.1798, training batch acc 0.0469\n",
      "[2022-11-15 05:20:28] epoch 0 step 370 : training batch loss 4.1547, training batch acc 0.0938\n",
      "[2022-11-15 05:20:59] epoch 0 step 380 : training batch loss 4.3232, training batch acc 0.0547\n",
      "[2022-11-15 05:21:30] epoch 0 step 390 : training batch loss 4.2376, training batch acc 0.0500\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'logits'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_14410/1972702821.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, train_dataloader, valid_dataloader, max_epoch)\u001b[0m\n\u001b[1;32m     43\u001b[0m                     \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m                     \u001b[0;31m# output = model(data)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m                     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackbone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m \u001b[0;31m# use backbone in evaluation and backbone has multi output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m                     \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m                     \u001b[0msample_num\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'logits'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "trainer = Trainer(model, optimizer, scheduler)\n",
    "trainer.train(train_loader,validate_loader,max_epoch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
