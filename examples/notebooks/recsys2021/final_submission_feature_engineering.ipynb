{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/env/bin/python\n",
    "\n",
    "import init\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql import *\n",
    "from pyspark import *\n",
    "import pyspark.sql.functions as f\n",
    "import pyspark.sql.types as t\n",
    "from timeit import default_timer as timer\n",
    "import logging\n",
    "from RecsysSchema import RecsysSchema\n",
    "from pyrecdp.data_processor import *\n",
    "from pyrecdp.encoder import *\n",
    "from pyrecdp.utils import *\n",
    "import hashlib\n",
    "\n",
    "target_list = [\n",
    " 'reply_timestamp',\n",
    " 'retweet_timestamp',\n",
    " 'retweet_with_comment_timestamp',\n",
    " 'like_timestamp'\n",
    "]\n",
    "\n",
    "final_feature_list = [\n",
    " 'engaged_with_user_follower_count',\n",
    " 'engaged_with_user_following_count',\n",
    " 'engaged_with_user_is_verified',\n",
    " 'engaging_user_follower_count',\n",
    " 'engaging_user_following_count',\n",
    " 'engaging_user_is_verified',\n",
    " 'has_photo',\n",
    " 'has_video',\n",
    " 'has_gif',\n",
    " 'a_ff_rate',\n",
    " 'b_ff_rate', \n",
    " 'dt_hour',\n",
    " 'dt_dow',\n",
    " 'has_mention',  \n",
    " 'mentioned_bucket_id',    \n",
    " 'mentioned_count',    \n",
    " 'most_used_word_bucket_id',\n",
    " 'second_used_word_bucket_id',\n",
    " 'TE_tweet_type_reply_timestamp',\n",
    " 'TE_tweet_type_retweet_timestamp',\n",
    " 'TE_dt_dow_retweet_timestamp',\n",
    " 'TE_most_used_word_bucket_id_reply_timestamp',\n",
    " 'TE_most_used_word_bucket_id_retweet_timestamp',\n",
    " 'TE_most_used_word_bucket_id_retweet_with_comment_timestamp',\n",
    " 'TE_most_used_word_bucket_id_like_timestamp',\n",
    " 'TE_second_used_word_bucket_id_reply_timestamp',\n",
    " 'TE_second_used_word_bucket_id_retweet_timestamp',\n",
    " 'TE_second_used_word_bucket_id_retweet_with_comment_timestamp',\n",
    " 'TE_second_used_word_bucket_id_like_timestamp',\n",
    " 'TE_mentioned_bucket_id_retweet_timestamp',\n",
    " 'TE_mentioned_bucket_id_retweet_with_comment_timestamp',\n",
    " 'TE_mentioned_bucket_id_like_timestamp',\n",
    " 'TE_mentioned_bucket_id_reply_timestamp',\n",
    " 'TE_language_reply_timestamp',\n",
    " 'TE_language_retweet_timestamp',\n",
    " 'TE_language_retweet_with_comment_timestamp',\n",
    " 'TE_language_like_timestamp',\n",
    " 'TE_mentioned_count_reply_timestamp',\n",
    " 'TE_mentioned_count_retweet_timestamp',\n",
    " 'TE_mentioned_count_retweet_with_comment_timestamp',\n",
    " 'TE_mentioned_count_like_timestamp',\n",
    " 'TE_engaged_with_user_id_reply_timestamp',\n",
    " 'TE_engaged_with_user_id_retweet_timestamp',\n",
    " 'TE_engaged_with_user_id_retweet_with_comment_timestamp',\n",
    " 'TE_engaged_with_user_id_like_timestamp',\n",
    " 'GTE_language_engaged_with_user_id_reply_timestamp',\n",
    " 'GTE_language_engaged_with_user_id_retweet_timestamp',\n",
    " 'GTE_language_engaged_with_user_id_retweet_with_comment_timestamp',\n",
    " 'GTE_language_engaged_with_user_id_like_timestamp',\n",
    " 'GTE_tweet_type_engaged_with_user_id_reply_timestamp',\n",
    " 'GTE_tweet_type_engaged_with_user_id_retweet_timestamp',\n",
    " 'GTE_tweet_type_engaged_with_user_id_retweet_with_comment_timestamp',\n",
    " 'GTE_tweet_type_engaged_with_user_id_like_timestamp',\n",
    " 'GTE_has_mention_engaging_user_id_reply_timestamp',\n",
    " 'GTE_has_mention_engaging_user_id_retweet_timestamp',\n",
    " 'GTE_has_mention_engaging_user_id_retweet_with_comment_timestamp',\n",
    " 'GTE_has_mention_engaging_user_id_like_timestamp',\n",
    " 'GTE_mentioned_bucket_id_engaging_user_id_reply_timestamp',\n",
    " 'GTE_mentioned_bucket_id_engaging_user_id_retweet_timestamp',\n",
    " 'GTE_mentioned_bucket_id_engaging_user_id_retweet_with_comment_timestamp',\n",
    " 'GTE_mentioned_bucket_id_engaging_user_id_like_timestamp',\n",
    " 'GTE_language_engaging_user_id_reply_timestamp',\n",
    " 'GTE_language_engaging_user_id_retweet_timestamp',\n",
    " 'GTE_language_engaging_user_id_retweet_with_comment_timestamp',\n",
    " 'GTE_language_engaging_user_id_like_timestamp',\n",
    " 'GTE_tweet_type_engaging_user_id_reply_timestamp',\n",
    " 'GTE_tweet_type_engaging_user_id_retweet_timestamp',\n",
    " 'GTE_tweet_type_engaging_user_id_retweet_with_comment_timestamp',\n",
    " 'GTE_tweet_type_engaging_user_id_like_timestamp',\n",
    " 'GTE_dt_dow_engaged_with_user_id_reply_timestamp',\n",
    " 'GTE_dt_dow_engaged_with_user_id_retweet_timestamp',\n",
    " 'GTE_dt_dow_engaged_with_user_id_retweet_with_comment_timestamp',\n",
    " 'GTE_dt_dow_engaged_with_user_id_like_timestamp',\n",
    " 'GTE_mentioned_count_engaging_user_id_reply_timestamp',\n",
    " 'GTE_mentioned_count_engaging_user_id_retweet_timestamp',\n",
    " 'GTE_mentioned_count_engaging_user_id_retweet_with_comment_timestamp',\n",
    " 'GTE_mentioned_count_engaging_user_id_like_timestamp',\n",
    " 'GTE_dt_hour_engaged_with_user_id_reply_timestamp',\n",
    " 'GTE_dt_hour_engaged_with_user_id_retweet_timestamp',\n",
    " 'GTE_dt_hour_engaged_with_user_id_retweet_with_comment_timestamp',\n",
    " 'GTE_dt_hour_engaged_with_user_id_like_timestamp',\n",
    " 'GTE_dt_dow_engaging_user_id_reply_timestamp',\n",
    " 'GTE_dt_dow_engaging_user_id_retweet_timestamp',\n",
    " 'GTE_dt_dow_engaging_user_id_retweet_with_comment_timestamp',\n",
    " 'GTE_dt_dow_engaging_user_id_like_timestamp',\n",
    " 'GTE_dt_hour_engaging_user_id_reply_timestamp',\n",
    " 'GTE_dt_hour_engaging_user_id_retweet_timestamp',\n",
    " 'GTE_dt_hour_engaging_user_id_retweet_with_comment_timestamp',\n",
    " 'GTE_dt_hour_engaging_user_id_like_timestamp'\n",
    "]\n",
    "\n",
    "def decodeBertTokenizerAndExtractFeatures(df, proc, output_name):\n",
    "    from transformers import BertTokenizer\n",
    "    tokenizer = BertTokenizer.from_pretrained(\n",
    "        'bert-base-multilingual-cased', do_lower_case=False)\n",
    "\n",
    "    # define UDF\n",
    "    tokenizer_decode = f.udf(lambda x: tokenizer.decode(\n",
    "        [int(n) for n in x.split('\\t')]))\n",
    "    format_url = f.udf(lambda x: x.replace(\n",
    "        'https : / / t. co / ', 'https://t.co/').replace('@ ', '@'))\n",
    "\n",
    "    # define decode udf operations\n",
    "    op_feature_modification_tokenizer_decode = FeatureAdd(\n",
    "        cols={'tweet': 'text_tokens'}, udfImpl=tokenizer_decode)\n",
    "    op_feature_modification_format_url = FeatureModification(\n",
    "        cols=['tweet'], udfImpl=format_url)\n",
    "    \n",
    "    op_feature_target_classify = FeatureModification(cols={\n",
    "        \"reply_timestamp\": \"f.when(f.col('reply_timestamp') > 0, 1).otherwise(0)\",\n",
    "        \"retweet_timestamp\": \"f.when(f.col('retweet_timestamp') > 0, 1).otherwise(0)\",\n",
    "        \"retweet_with_comment_timestamp\": \"f.when(f.col('retweet_with_comment_timestamp') > 0, 1).otherwise(0)\",\n",
    "        \"like_timestamp\": \"f.when(f.col('like_timestamp') > 0, 1).otherwise(0)\"}, op='inline')\n",
    "    \n",
    "    # define new features\n",
    "    op_feature_from_original = FeatureAdd(\n",
    "        cols={\"has_photo\": \"f.col('present_media').contains('Photo').cast(t.IntegerType())\",\n",
    "              \"has_video\": \"f.col('present_media').contains('Vedio').cast(t.IntegerType())\",\n",
    "              \"has_gif\": \"f.col('present_media').contains('GIF').cast(t.IntegerType())\",             \n",
    "              \"a_ff_rate\": \"f.col('engaged_with_user_following_count')/f.col('engaged_with_user_follower_count')\",\n",
    "              \"b_ff_rate\": \"f.col('engaging_user_following_count') /f.col('engaging_user_follower_count')\",\n",
    "              \"dt_dow\": \"f.dayofweek(f.from_unixtime(f.col('tweet_timestamp'))).cast(t.IntegerType())\",\n",
    "              \"dt_hour\": \"f.hour(f.from_unixtime(f.col('tweet_timestamp'))).cast(t.IntegerType())\",           \n",
    "              \"mention\": \"f.regexp_extract(f.col('tweet'), r'[^RT]\\s@(\\S+)', 1)\",\n",
    "              \"has_mention\": \"(f.col('mention')!= '').cast(t.IntegerType())\"\n",
    "        }, op='inline')\n",
    "\n",
    "    # execute\n",
    "    proc.reset_ops([op_feature_modification_tokenizer_decode,\n",
    "                    op_feature_modification_format_url,\n",
    "                    op_feature_target_classify,\n",
    "                    op_feature_from_original])\n",
    "    t1 = timer()\n",
    "    df = proc.transform(df, name=output_name)\n",
    "    t2 = timer()\n",
    "    print(\"BertTokenizer decode and feature extacting took %.3f\" % (t2 - t1))\n",
    "\n",
    "    return df\n",
    "\n",
    "def categorifyFeatures(df, proc, output_name, gen_dict, sampleRatio=1):\n",
    "    # 1. prepare dictionary\n",
    "    dict_dfs = []\n",
    "    if gen_dict:\n",
    "        # only call below function when target dicts were not pre-prepared\n",
    "        op_gen_dict_multiItems = GenerateDictionary(['tweet'], doSplit=True, sep=' ', bucketSize=100)\n",
    "        op_gen_dict_singleItems = GenerateDictionary(['mention'], bucketSize=100)\n",
    "        proc.reset_ops([op_gen_dict_multiItems, op_gen_dict_singleItems])\n",
    "        t1 = timer()\n",
    "        dict_dfs = proc.generate_dicts(df)\n",
    "        t2 = timer()\n",
    "        print(\"Generate Dictionary took %.3f\" % (t2 - t1))\n",
    "    else:\n",
    "        dict_names = ['tweet', 'mention']\n",
    "        dict_dfs = [{'col_name': name, 'dict': proc.spark.read.parquet(\n",
    "            \"%s/%s/%s/%s\" % (proc.path_prefix, proc.current_path, proc.dicts_path, name))} for name in dict_names]\n",
    "    # 2. since we need both mentioned_bucket_id and mentioned_count, add two mention id dict_dfs\n",
    "    for dict_df in dict_dfs:\n",
    "        if dict_df['col_name'] == 'mention':\n",
    "            dict_dfs.append({'col_name': 'mentioned_bucket_id', 'dict':dict_df['dict']})\n",
    "            dict_dfs.append({'col_name': 'mentioned_count', 'dict':dict_df['dict'].drop('dict_col_id').withColumnRenamed('count', 'dict_col_id')})\n",
    "    op_feature_add = FeatureAdd({\"mentioned_bucket_id\": \"f.col('mention')\", \"mentioned_count\": \"f.col('mention')\"}, op='inline')\n",
    "    \n",
    "    # 3. categorify\n",
    "    op_categorify_multiItems = Categorify([{'bucketized_tweet_word': 'tweet'}], dict_dfs=dict_dfs, doSplit=True, sep=' ')\n",
    "    op_categorify_singleItem = Categorify(['mentioned_bucket_id', 'mentioned_count'], dict_dfs=dict_dfs)\n",
    "    proc.reset_ops([op_feature_add, op_categorify_multiItems, op_categorify_singleItem])\n",
    "    \n",
    "    # 4. get most and second used bucketized_tweet_word\n",
    "    op_feature_add_sorted_bucketized_tweet_word = FeatureAdd(\n",
    "        cols={'sorted_bucketized_tweet_word': \"f.expr('sortIntArrayByFrequency(bucketized_tweet_word)')\"}, op='inline')\n",
    "    op_feature_add_convert = FeatureAdd(\n",
    "        cols={'most_used_word_bucket_id': \"f.when(f.size(f.col('sorted_bucketized_tweet_word'))>0, f.col('sorted_bucketized_tweet_word').getItem(0)).otherwise(np.nan)\",\n",
    "             'second_used_word_bucket_id': \"f.when(f.size(f.col('sorted_bucketized_tweet_word'))>1, f.col('sorted_bucketized_tweet_word').getItem(1)).otherwise(np.nan)\"}, op='inline')\n",
    "    proc.append_ops([op_feature_add_sorted_bucketized_tweet_word, op_feature_add_convert])\n",
    "\n",
    "    # 5. transform\n",
    "    t1 = timer()\n",
    "    if sampleRatio != 1:\n",
    "        df = df.sample(sampleRatio)\n",
    "    df = proc.transform(df, name=output_name)\n",
    "    t2 = timer()\n",
    "    print(\"categorify and getMostAndSecondUsedWordBucketId took %.3f\" % (t2 - t1))\n",
    "    return (df, dict_dfs)\n",
    "\n",
    "\n",
    "def encodingFeatures(df, proc, output_name, gen_dict, sampleRatio=1):   \n",
    "    targets = ['reply_timestamp', 'retweet_timestamp', 'retweet_with_comment_timestamp', 'like_timestamp']\n",
    "    y_mean_all = []\n",
    "    \n",
    "    t1 = timer()\n",
    "    if gen_dict:\n",
    "        for tgt in targets:\n",
    "            tmp = df.groupBy().mean(tgt).collect()[0]\n",
    "            y_mean = tmp[f\"avg({tgt})\"]\n",
    "            y_mean_all.append(y_mean)\n",
    "        schema = t.StructType([t.StructField(tgt, t.FloatType(), True) for tgt in targets])\n",
    "        y_mean_all_df = proc.spark.createDataFrame([tuple(y_mean_all)], schema)\n",
    "        y_mean_all_df.write.format(\"parquet\").mode(\"overwrite\").save(\n",
    "            \"%s/%s/%s/targets_mean\" % (proc.path_prefix, proc.current_path, proc.dicts_path))\n",
    "    y_mean_all_df = proc.spark.read.parquet(\n",
    "        \"%s/%s/%s/targets_mean\" % (proc.path_prefix, proc.current_path, proc.dicts_path))\n",
    "\n",
    "    features = [\n",
    "            'engaged_with_user_id',\n",
    "            'language',\n",
    "            'dt_dow',\n",
    "            'tweet_type',\n",
    "            'most_used_word_bucket_id',\n",
    "            'second_used_word_bucket_id',\n",
    "            'mentioned_count',\n",
    "            'mentioned_bucket_id',\n",
    "            ['has_mention', 'engaging_user_id'],\n",
    "            ['mentioned_count', 'engaging_user_id'],\n",
    "            ['mentioned_bucket_id', 'engaging_user_id'],\n",
    "            ['language', 'engaged_with_user_id'],\n",
    "            ['language', 'engaging_user_id'],\n",
    "            ['dt_dow', 'engaged_with_user_id'],\n",
    "            ['dt_dow', 'engaging_user_id'],\n",
    "            ['dt_hour', 'engaged_with_user_id'],\n",
    "            ['dt_hour', 'engaging_user_id'],\n",
    "            ['tweet_type', 'engaged_with_user_id'],\n",
    "            ['tweet_type', 'engaging_user_id']\n",
    "    ]\n",
    "    excludes = {'dt_dow': ['reply_timestamp', 'retweet_with_comment_timestamp', 'like_timestamp'],\n",
    "               'tweet_type': ['like_timestamp', 'retweet_with_comment_timestamp']\n",
    "              }\n",
    "\n",
    "    te_train_dfs = []\n",
    "    te_test_dfs = []\n",
    "    for c in features:\n",
    "        target_tmp = targets\n",
    "        out_name = \"\"\n",
    "        if str(c) in excludes:\n",
    "            target_tmp = []\n",
    "            for tgt in targets:\n",
    "                if tgt not in excludes[c]:\n",
    "                    target_tmp.append(tgt)\n",
    "        out_col_list = []\n",
    "        for tgt in target_tmp:\n",
    "            if isinstance(c, list):\n",
    "                out_col_list.append('GTE_'+'_'.join(c)+'_'+tgt)\n",
    "                out_name = 'GTE_'+'_'.join(c)\n",
    "            else:\n",
    "                out_col_list.append(f'TE_{c}_{tgt}')\n",
    "                out_name = f'TE_{c}'\n",
    "        if gen_dict:\n",
    "            start = timer()\n",
    "            encoder = TargetEncoder(proc, c, target_tmp, out_col_list, out_name, out_dtype=t.FloatType(), y_mean_list=y_mean_all)\n",
    "            te_train_df, te_test_df = encoder.transform(df)\n",
    "            te_train_dfs.append({'col_name': ['fold'] + (c if isinstance(c, list) else [c]), 'dict': te_train_df})\n",
    "            te_test_dfs.append({'col_name': c, 'dict': te_test_df})\n",
    "            print(f\"generating target encoding for %s upon %s took %.1f seconds\"%(str(c), str(target_tmp), timer()-start))\n",
    "        else:\n",
    "            te_train_path = \"%s/%s/%s/train/%s\" % (proc.path_prefix, proc.current_path, proc.dicts_path, out_name)\n",
    "            te_test_path = \"%s/%s/%s/test/%s\" % (proc.path_prefix, proc.current_path, proc.dicts_path, out_name)               \n",
    "            te_train_dfs.append({'col_name': ['fold'] + (c if isinstance(c, list) else [c]), 'dict': proc.spark.read.parquet(te_train_path)})\n",
    "            te_test_dfs.append({'col_name': c, 'dict': proc.spark.read.parquet(te_test_path)})\n",
    "    t2 = timer()\n",
    "    print(\"Generate encoding feature totally took %.3f\" % (t2 - t1))\n",
    "\n",
    "    # merge dicts to original table\n",
    "    op_merge_to_train = ModelMerge(te_train_dfs)\n",
    "    proc.reset_ops([op_merge_to_train])\n",
    "    \n",
    "    # select features\n",
    "    op_select = SelectFeature(target_list + final_feature_list)\n",
    "    proc.append_ops([op_select])\n",
    "\n",
    "    t1 = timer()\n",
    "    if sampleRatio != 1:\n",
    "        df = df.sample(sampleRatio)\n",
    "    df = proc.transform(df, name=output_name)\n",
    "    t2 = timer()\n",
    "    print(\"encodingFeatures took %.3f\" % (t2 - t1))\n",
    "    \n",
    "    return (df, te_train_dfs, te_test_dfs, y_mean_all_df)\n",
    "\n",
    "\n",
    "def splitByDate(df, proc, train_output, test_output, numFolds=5):\n",
    "    # 1.1 get timestamp range\n",
    "    import datetime\n",
    "    min_timestamp = df.select('tweet_timestamp').agg({'tweet_timestamp': 'min'}).collect()[0]['min(tweet_timestamp)']\n",
    "    max_timestamp = df.select('tweet_timestamp').agg({'tweet_timestamp': 'max'}).collect()[0]['max(tweet_timestamp)']\n",
    "    seconds_in_day = 3600 * 24\n",
    "\n",
    "    print(\n",
    "        \"min_timestamp is %s, max_timestamp is %s, 20 days max is %s\" % (\n",
    "            datetime.datetime.fromtimestamp(min_timestamp).strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            datetime.datetime.fromtimestamp(max_timestamp).strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            datetime.datetime.fromtimestamp(min_timestamp + 20 * seconds_in_day).strftime('%Y-%m-%d %H:%M:%S')\n",
    "        ))\n",
    "\n",
    "    time_range_split = {\n",
    "        'train': (min_timestamp, seconds_in_day * 18 + min_timestamp),\n",
    "        'test': (seconds_in_day * 18 + min_timestamp, max_timestamp)\n",
    "    }\n",
    "\n",
    "    print(time_range_split)\n",
    "\n",
    "    # 1.2 save ranged data for train\n",
    "    # filtering out train range data and save\n",
    "    train_start, train_end = time_range_split['train']\n",
    "    test_start, test_end = time_range_split['test']\n",
    "    t1 = timer()\n",
    "    train_df = df.filter(\n",
    "        (f.col('tweet_timestamp') >= f.lit(train_start)) & (f.col('tweet_timestamp') < f.lit(train_end)))\n",
    "    train_df = train_df.withColumn(\"fold\", f.round(f.rand(seed=42)*(numFolds-1)).cast(\"int\"))\n",
    "    train_df.write.format('parquet').mode('overwrite').save(proc.path_prefix + proc.current_path + train_output)\n",
    "    t2 = timer()\n",
    "    print(\"split to train took %.3f\" % (t2 - t1))\n",
    "    \n",
    "    t1 = timer()\n",
    "    test_df = df.filter(\n",
    "        (f.col('tweet_timestamp') >= f.lit(test_start)) & (f.col('tweet_timestamp') < f.lit(test_end)))\n",
    "    test_df.write.format('parquet').mode('overwrite').save(proc.path_prefix + proc.current_path + test_output)\n",
    "    t2 = timer()\n",
    "    print(\"split to test took %.3f\" % (t2 - t1))\n",
    "    \n",
    "    return (proc.spark.read.parquet(proc.path_prefix + proc.current_path + train_output),\n",
    "            proc.spark.read.parquet(proc.path_prefix + proc.current_path + test_output))\n",
    "\n",
    "\n",
    "def mergeFeaturesToTest(df, dict_dfs, te_test_dfs, y_mean_all_df, proc, output_name):\n",
    "    # categorify test data with train generated dictionary\n",
    "    # 1. since we need both mentioned_bucket_id and mentioned_count, add two mention id dict_dfs\n",
    "    for dict_df in dict_dfs:\n",
    "        if dict_df['col_name'] == 'mention':\n",
    "            dict_dfs.append({'col_name': 'mentioned_bucket_id', 'dict':dict_df['dict']})\n",
    "            dict_dfs.append({'col_name': 'mentioned_count', 'dict':dict_df['dict'].drop('dict_col_id').withColumnRenamed('count', 'dict_col_id')})\n",
    "    op_feature_add = FeatureAdd({\"mentioned_bucket_id\": \"f.col('mention')\", \"mentioned_count\": \"f.col('mention')\"}, op='inline')\n",
    "    \n",
    "    # 2. categorify\n",
    "    op_categorify_multiItems = Categorify([{'bucketized_tweet_word': 'tweet'}], dict_dfs=dict_dfs, doSplit=True, sep=' ')\n",
    "    op_categorify_singleItem = Categorify(['mentioned_bucket_id', 'mentioned_count'], dict_dfs=dict_dfs)\n",
    "    proc.reset_ops([op_feature_add, op_categorify_multiItems, op_categorify_singleItem])\n",
    "    \n",
    "    # 3. get most and second used bucketized_tweet_word\n",
    "    op_feature_add_sorted_bucketized_tweet_word = FeatureAdd(\n",
    "        cols={'sorted_bucketized_tweet_word': \"f.expr('sortIntArrayByFrequency(bucketized_tweet_word)')\"}, op='inline')\n",
    "    op_feature_add_convert = FeatureAdd(\n",
    "        cols={'most_used_word_bucket_id': \"f.when(f.size(f.col('sorted_bucketized_tweet_word'))>0, f.col('sorted_bucketized_tweet_word').getItem(0)).otherwise(np.nan)\",\n",
    "             'second_used_word_bucket_id': \"f.when(f.size(f.col('sorted_bucketized_tweet_word'))>1, f.col('sorted_bucketized_tweet_word').getItem(1)).otherwise(np.nan)\"}, op='inline')\n",
    "    proc.append_ops([op_feature_add_sorted_bucketized_tweet_word, op_feature_add_convert])\n",
    "    \n",
    "    # 4. merge dicts to original table\n",
    "    op_merge_to_test = ModelMerge(te_test_dfs)\n",
    "    proc.append_ops([op_merge_to_test])\n",
    "        \n",
    "    # 5. set null in encoding features to y_mean\n",
    "    y_mean_all = y_mean_all_df.collect()[0]\n",
    "    for tgt in target_list:\n",
    "        to_fill_list = []\n",
    "        for feature in final_feature_list:\n",
    "            if 'TE_' in feature and tgt in feature:\n",
    "                to_fill_list.append(feature)\n",
    "        op_fill_na = FillNA(to_fill_list, y_mean_all[tgt])\n",
    "        proc.append_ops([op_fill_na])\n",
    "    \n",
    "    # select features\n",
    "    op_select = SelectFeature(target_list + final_feature_list)\n",
    "    proc.append_ops([op_select])\n",
    "\n",
    "    t1 = timer()\n",
    "    df = proc.transform(df, name=output_name)\n",
    "    t2 = timer()\n",
    "    print(\"mergeFeaturesToTest took %.3f\" % (t2 - t1))\n",
    "\n",
    "\n",
    "def get_encoding_features_dicts(proc):\n",
    "    targets = ['reply_timestamp', 'retweet_timestamp', 'retweet_with_comment_timestamp', 'like_timestamp']\n",
    "    y_mean_all = []\n",
    "    y_mean_all_df = proc.spark.read.parquet(\n",
    "        \"%s/%s/%s/targets_mean\" % (proc.path_prefix, proc.current_path, proc.dicts_path))\n",
    "    features = [\n",
    "            'engaged_with_user_id',\n",
    "            'language',\n",
    "            'dt_dow',\n",
    "            'tweet_type',\n",
    "            'most_used_word_bucket_id',\n",
    "            'second_used_word_bucket_id',\n",
    "            'mentioned_count',\n",
    "            'mentioned_bucket_id',\n",
    "            ['has_mention', 'engaging_user_id'],\n",
    "            ['mentioned_count', 'engaging_user_id'],\n",
    "            ['mentioned_bucket_id', 'engaging_user_id'],\n",
    "            ['language', 'engaged_with_user_id'],\n",
    "            ['language', 'engaging_user_id'],\n",
    "            ['dt_dow', 'engaged_with_user_id'],\n",
    "            ['dt_dow', 'engaging_user_id'],\n",
    "            ['dt_hour', 'engaged_with_user_id'],\n",
    "            ['dt_hour', 'engaging_user_id'],\n",
    "            ['tweet_type', 'engaged_with_user_id'],\n",
    "            ['tweet_type', 'engaging_user_id']\n",
    "    ]\n",
    "    excludes = {'dt_dow': ['reply_timestamp', 'retweet_with_comment_timestamp', 'like_timestamp'],\n",
    "               'tweet_type': ['like_timestamp', 'retweet_with_comment_timestamp']\n",
    "              }\n",
    "\n",
    "    te_train_dfs = []\n",
    "    te_test_dfs = []\n",
    "    for c in features:\n",
    "        target_tmp = targets\n",
    "        out_name = \"\"\n",
    "        if str(c) in excludes:\n",
    "            target_tmp = []\n",
    "            for tgt in targets:\n",
    "                if tgt not in excludes[c]:\n",
    "                    target_tmp.append(tgt)\n",
    "        out_col_list = []\n",
    "        for tgt in target_tmp:\n",
    "            if isinstance(c, list):\n",
    "                out_col_list.append('GTE_'+'_'.join(c)+'_'+tgt)\n",
    "                out_name = 'GTE_'+'_'.join(c)\n",
    "            else:\n",
    "                out_col_list.append(f'TE_{c}_{tgt}')\n",
    "                out_name = f'TE_{c}'\n",
    "        te_train_path = \"%s/%s/%s/train/%s\" % (proc.path_prefix, proc.current_path, proc.dicts_path, out_name)\n",
    "        te_test_path = \"%s/%s/%s/test/%s\" % (proc.path_prefix, proc.current_path, proc.dicts_path, out_name)\n",
    "        te_train_dfs.append({'col_name': ['fold'] + (c if isinstance(c, list) else [c]), 'dict': proc.spark.read.parquet(te_train_path)})\n",
    "        te_test_dfs.append({'col_name': c, 'dict': proc.spark.read.parquet(te_test_path)})\n",
    "        \n",
    "    return (te_train_dfs, te_test_dfs, y_mean_all_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "path_prefix = \"hdfs://\"\n",
    "current_path = \"/recsys2021_0608_example/\"\n",
    "original_folder = \"/recsys2021_0608/\"\n",
    "dicts_folder = \"recsys_dicts/\"\n",
    "recsysSchema = RecsysSchema()\n",
    "\n",
    "##### 1. Start spark and initialize data processor #####\n",
    "scala_udf_jars = \"/mnt/nvme2/chendi/BlueWhale/recdp/ScalaProcessUtils/target/recdp-scala-extensions-0.1.0-jar-with-dependencies.jar\"\n",
    "\n",
    "t0 = timer()\n",
    "spark = SparkSession.builder.master('yarn')\\\n",
    "    .appName(\"Recsys2021_data_process\")\\\n",
    "    .config(\"spark.executor.memory\", \"20g\")\\\n",
    "    .config(\"spark.executor.memoryOverhead\", \"10g\")\\\n",
    "    .config(\"spark.sql.broadcastTimeout\", \"7200\")\\\n",
    "    .config(\"spark.cleaner.periodicGC.interval\", \"10min\")\\\n",
    "    .config(\"spark.executorEnv.HF_DATASETS_OFFLINE\", \"1\")\\\n",
    "    .config(\"spark.executorEnv.TRANSFORMERS_OFFLINE\", \"1\")\\\n",
    "    .config(\"spark.driver.extraClassPath\", f\"{scala_udf_jars}\")\\\n",
    "    .config(\"spark.executor.extraClassPath\", f\"{scala_udf_jars}\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "schema = recsysSchema.toStructType()\n",
    "\n",
    "# 1.1 prepare dataFrames\n",
    "# 1.2 create RecDP DataProcessor\n",
    "proc = DataProcessor(spark, path_prefix,\n",
    "                     current_path=current_path, dicts_path=dicts_folder, shuffle_disk_capacity=\"1200GB\")\n",
    "df = spark.read.parquet(path_prefix + original_folder)\n",
    "df = df.withColumnRenamed('enaging_user_following_count', 'engaging_user_following_count')\n",
    "df = df.withColumnRenamed('enaging_user_is_verified', 'engaging_user_is_verified')\n",
    "\n",
    "# # fast test, comment for full dataset\n",
    "# df.sample(0.01).write.format(\"parquet\").mode(\"overwrite\").save(\"%s/sample_0_0_1\" % current_path)\n",
    "# df = spark.read.parquet(\"%s/sample_0_0_1\" % current_path)\n",
    "\n",
    "# ===============================================\n",
    "# decode tweet_tokens\n",
    "df = decodeBertTokenizerAndExtractFeatures(df, proc, output_name=\"decoded_with_extracted_features\")\n",
    "\n",
    "# ===============================================\n",
    "# splitting and sampling\n",
    "df, test_df = splitByDate(df, proc, train_output=\"train\", test_output=\"test\", numFolds=5)\n",
    "\n",
    "# ===============================================\n",
    "# generate dictionary for categorify indexing\n",
    "df, dict_dfs = categorifyFeatures(df, proc, output_name=\"train_with_categorified_features\", gen_dict=True, sampleRatio=1)\n",
    "\n",
    "# ===============================================\n",
    "# encoding features\n",
    "df, te_train_dfs, te_test_dfs, y_mean_all_df = encodingFeatures(df, proc, output_name=\"train_with_features_sample_0_0_3\", gen_dict=True, sampleRatio=0.03)\n",
    "\n",
    "# ===============================================\n",
    "# adding features to test\n",
    "### Below codes is used to prepare for mergeFeaturesToTest for test separately ###\n",
    "# test_df = spark.read.parquet(\"/recsys2021_0608_example/test\")\n",
    "# dict_names = ['tweet', 'mention']\n",
    "# dict_dfs = [{'col_name': name, 'dict': spark.read.parquet(\n",
    "#     \"%s/%s/%s/%s\" % (proc.path_prefix, proc.current_path, proc.dicts_path, name))} for name in dict_names]\n",
    "# te_train_dfs, te_test_dfs, y_mean_all_df = get_encoding_features_dicts(proc)\n",
    "##################################################################################\n",
    "test_df = mergeFeaturesToTest(test_df, dict_dfs, te_test_dfs, y_mean_all_df, proc, output_name=\"test_with_features\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* per core memory size is 5.000 GB and shuffle_disk maximum capacity is 1200.000 GB\n",
    "* None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
    "* BertTokenizer decode and feature extacting took 2992.285\n",
    "\n",
    "======\n",
    "\n",
    "* min_timestamp is 2021-02-04 08:00:00, max_timestamp is 2021-02-25 07:59:59, 20 days max is 2021-02-24 08:00:00\n",
    "* {'train': (1612396800, 1613952000), 'test': (1613952000, 1614211199)}\n",
    "* split to train took 595.243\n",
    "* split to test took 311.072\n",
    "\n",
    "======\n",
    "\n",
    "* Generate Dictionary took 847.997\n",
    "\n",
    "======\n",
    "\n",
    "* bhj total threshold is 67.109 M rows, one bhj threshold is 30.000 M rows, flush_threshold is 960.000 GB\n",
    "* ('tweet', DataFrame[dict_col: string, count: bigint, dict_col_id: int], 148528557)\n",
    "* etstimated_to_shuffle_size for tweet is 123.897 GB, will do smj\n",
    "* do smj to bucketized_tweet_word\n",
    "* bhj total threshold is 67.109 M rows, one bhj threshold is 30.000 M rows, flush_threshold is 960.000 GB\n",
    "* ('mentioned_bucket_id', DataFrame[dict_col: string, count: bigint, dict_col_id: int], 4891669)\n",
    "* ('mentioned_count', DataFrame[dict_col: string, dict_col_id: bigint], 4891669)\n",
    "* mentioned_bucket_id will do bhj\n",
    "* mentioned_count will do bhj\n",
    "* do bhj to mentioned_bucket_id\n",
    "* do bhj to mentioned_count\n",
    "* categorify and getMostAndSecondUsedWordBucketId took 3731.166\n",
    "\n",
    "======\n",
    "\n",
    "* generating target encoding for engaged_with_user_id upon ['reply_timestamp', 'retweet_timestamp', 'retweet_with_comment_timestamp', 'like_timestamp'] took 92.6 seconds\n",
    "* generating target encoding for language upon ['reply_timestamp', 'retweet_timestamp', 'retweet_with_comment_timestamp', 'like_timestamp'] took 21.1 seconds\n",
    "* generating target encoding for dt_dow upon ['retweet_timestamp'] took 11.7 seconds\n",
    "* generating target encoding for tweet_type upon ['retweet_with_comment_timestamp', 'like_timestamp'] took 14.7 seconds\n",
    "* generating target encoding for most_used_word_bucket_id upon ['reply_timestamp', 'retweet_timestamp', 'retweet_with_comment_timestamp', 'like_timestamp'] took 17.6 seconds\n",
    "* generating target encoding for second_used_word_bucket_id upon ['reply_timestamp', 'retweet_timestamp', 'retweet_with_comment_timestamp', 'like_timestamp'] took 16.5 seconds\n",
    "* generating target encoding for mentioned_count upon ['reply_timestamp', 'retweet_timestamp', 'retweet_with_comment_timestamp', 'like_timestamp'] took 20.6 seconds\n",
    "* generating target encoding for mentioned_bucket_id upon ['reply_timestamp', 'retweet_timestamp', 'retweet_with_comment_timestamp', 'like_timestamp'] took 14.1 seconds\n",
    "* generating target encoding for ['has_mention', 'engaging_user_id'] upon ['reply_timestamp', 'retweet_timestamp', 'retweet_with_comment_timestamp', 'like_timestamp'] took 120.0 seconds\n",
    "* generating target encoding for ['mentioned_count', 'engaging_user_id'] upon ['reply_timestamp', 'retweet_timestamp', 'retweet_with_comment_timestamp', 'like_timestamp'] took 126.9 seconds\n",
    "* generating target encoding for ['mentioned_bucket_id', 'engaging_user_id'] upon ['reply_timestamp', 'retweet_timestamp', 'retweet_with_comment_timestamp', 'like_timestamp'] took 119.3 seconds\n",
    "* generating target encoding for ['language', 'engaged_with_user_id'] upon ['reply_timestamp', 'retweet_timestamp', 'retweet_with_comment_timestamp', 'like_timestamp'] took 126.6 seconds\n",
    "* generating target encoding for ['language', 'engaging_user_id'] upon ['reply_timestamp', 'retweet_timestamp', 'retweet_with_comment_timestamp', 'like_timestamp'] took 144.0 seconds\n",
    "* generating target encoding for ['dt_dow', 'engaged_with_user_id'] upon ['reply_timestamp', 'retweet_timestamp', 'retweet_with_comment_timestamp', 'like_timestamp'] took 136.2 seconds\n",
    "* generating target encoding for ['dt_dow', 'engaging_user_id'] upon ['reply_timestamp', 'retweet_timestamp', 'retweet_with_comment_timestamp', 'like_timestamp'] took 157.3 seconds\n",
    "* generating target encoding for ['dt_hour', 'engaged_with_user_id'] upon ['reply_timestamp', 'retweet_timestamp', 'retweet_with_comment_timestamp', 'like_timestamp'] took 153.2 seconds\n",
    "* generating target encoding for ['dt_hour', 'engaging_user_id'] upon ['reply_timestamp', 'retweet_timestamp', 'retweet_with_comment_timestamp', 'like_timestamp'] took 177.4 seconds\n",
    "* generating target encoding for ['tweet_type', 'engaged_with_user_id'] upon ['reply_timestamp', 'retweet_timestamp', 'retweet_with_comment_timestamp', 'like_timestamp'] took 118.2 seconds\n",
    "* generating target encoding for ['tweet_type', 'engaging_user_id'] upon ['reply_timestamp', 'retweet_timestamp', 'retweet_with_comment_timestamp', 'like_timestamp'] took 141.2 seconds\n",
    "* Generate encoding feature totally took 1738.177\n",
    "\n",
    "======\n",
    "\n",
    "* bhj total threshold is 67.109 M rows, one bhj threshold is 30.000 M rows, flush_threshold is 960.000 GB\n",
    "* (['fold', 'tweet_type'], DataFrame[tweet_type: string, fold: double, TE_tweet_type_retweet_with_comment_timestamp: float, TE_tweet_type_like_timestamp: float], 18)\n",
    "* (['fold', 'dt_dow'], DataFrame[dt_dow: int, fold: double, TE_dt_dow_retweet_timestamp: float], 42)\n",
    "* (['fold', 'most_used_word_bucket_id'], DataFrame[most_used_word_bucket_id: string, fold: double, TE_most_used_word_bucket_id_reply_timestamp: float, TE_most_used_word_bucket_id_retweet_timestamp: float, TE_most_used_word_bucket_id_retweet_with_comment_timestamp: float, TE_most_used_word_bucket_id_like_timestamp: float], 120)\n",
    "* (['fold', 'second_used_word_bucket_id'], DataFrame[second_used_word_bucket_id: string, fold: double, TE_second_used_word_bucket_id_reply_timestamp: float, TE_second_used_word_bucket_id_retweet_timestamp: float, TE_second_used_word_bucket_id_retweet_with_comment_timestamp: float, TE_second_used_word_bucket_id_like_timestamp: float], 126)\n",
    "* (['fold', 'mentioned_bucket_id'], DataFrame[mentioned_bucket_id: int, fold: double, TE_mentioned_bucket_id_reply_timestamp: float, TE_mentioned_bucket_id_retweet_timestamp: float, TE_mentioned_bucket_id_retweet_with_comment_timestamp: float, TE_mentioned_bucket_id_like_timestamp: float], 132)\n",
    "* (['fold', 'language'], DataFrame[language: string, fold: double, TE_language_reply_timestamp: float, TE_language_retweet_timestamp: float, TE_language_retweet_with_comment_timestamp: float, TE_language_like_timestamp: float], 396)\n",
    "* (['fold', 'mentioned_count'], DataFrame[mentioned_count: bigint, fold: double, TE_mentioned_count_reply_timestamp: float, TE_mentioned_count_retweet_timestamp: float, TE_mentioned_count_retweet_with_comment_timestamp: float, TE_mentioned_count_like_timestamp: float], 25968)\n",
    "* (['fold', 'engaged_with_user_id'], DataFrame[engaged_with_user_id: string, fold: double, TE_engaged_with_user_id_reply_timestamp: float, TE_engaged_with_user_id_retweet_timestamp: float, TE_engaged_with_user_id_retweet_with_comment_timestamp: float, TE_engaged_with_user_id_like_timestamp: float], 68843219)\n",
    "* (['fold', 'language', 'engaged_with_user_id'], DataFrame[language: string, engaged_with_user_id: string, fold: double, GTE_language_engaged_with_user_id_reply_timestamp: float, GTE_language_engaged_with_user_id_retweet_timestamp: float, GTE_language_engaged_with_user_id_retweet_with_comment_timestamp: float, GTE_language_engaged_with_user_id_like_timestamp: float], 89264853)\n",
    "* (['fold', 'tweet_type', 'engaged_with_user_id'], DataFrame[tweet_type: string, engaged_with_user_id: string, fold: double, GTE_tweet_type_engaged_with_user_id_reply_timestamp: float, GTE_tweet_type_engaged_with_user_id_retweet_timestamp: float, GTE_tweet_type_engaged_with_user_id_retweet_with_comment_timestamp: float, GTE_tweet_type_engaged_with_user_id_like_timestamp: float], 95344319)\n",
    "* (['fold', 'mentioned_bucket_id', 'engaging_user_id'], DataFrame[mentioned_bucket_id: int, engaging_user_id: string, fold: double, GTE_mentioned_bucket_id_engaging_user_id_reply_timestamp: float, GTE_mentioned_bucket_id_engaging_user_id_retweet_timestamp: float, GTE_mentioned_bucket_id_engaging_user_id_retweet_with_comment_timestamp: float, GTE_mentioned_bucket_id_engaging_user_id_like_timestamp: float], 129179090)\n",
    "* (['fold', 'has_mention', 'engaging_user_id'], DataFrame[has_mention: int, engaging_user_id: string, fold: double, GTE_has_mention_engaging_user_id_reply_timestamp: float, GTE_has_mention_engaging_user_id_retweet_timestamp: float, GTE_has_mention_engaging_user_id_retweet_with_comment_timestamp: float, GTE_has_mention_engaging_user_id_like_timestamp: float], 131959138)\n",
    "* (['fold', 'language', 'engaging_user_id'], DataFrame[language: string, engaging_user_id: string, fold: double, GTE_language_engaging_user_id_reply_timestamp: float, GTE_language_engaging_user_id_retweet_timestamp: float, GTE_language_engaging_user_id_retweet_with_comment_timestamp: float, GTE_language_engaging_user_id_like_timestamp: float], 149779869)\n",
    "* (['fold', 'dt_dow', 'engaged_with_user_id'], DataFrame[dt_dow: int, engaged_with_user_id: string, fold: double, GTE_dt_dow_engaged_with_user_id_reply_timestamp: float, GTE_dt_dow_engaged_with_user_id_retweet_timestamp: float, GTE_dt_dow_engaged_with_user_id_retweet_with_comment_timestamp: float, GTE_dt_dow_engaged_with_user_id_like_timestamp: float], 155971686)\n",
    "* (['fold', 'tweet_type', 'engaging_user_id'], DataFrame[tweet_type: string, engaging_user_id: string, fold: double, GTE_tweet_type_engaging_user_id_reply_timestamp: float, GTE_tweet_type_engaging_user_id_retweet_timestamp: float, GTE_tweet_type_engaging_user_id_retweet_with_comment_timestamp: float, GTE_tweet_type_engaging_user_id_like_timestamp: float], 162333565)\n",
    "* (['fold', 'mentioned_count', 'engaging_user_id'], DataFrame[mentioned_count: bigint, engaging_user_id: string, fold: double, GTE_mentioned_count_engaging_user_id_reply_timestamp: float, GTE_mentioned_count_engaging_user_id_retweet_timestamp: float, GTE_mentioned_count_engaging_user_id_retweet_with_comment_timestamp: float, GTE_mentioned_count_engaging_user_id_like_timestamp: float], 167086112)\n",
    "* (['fold', 'dt_hour', 'engaged_with_user_id'], DataFrame[dt_hour: int, engaged_with_user_id: string, fold: double, GTE_dt_hour_engaged_with_user_id_reply_timestamp: float, GTE_dt_hour_engaged_with_user_id_retweet_timestamp: float, GTE_dt_hour_engaged_with_user_id_retweet_with_comment_timestamp: float, GTE_dt_hour_engaged_with_user_id_like_timestamp: float], 201562399)\n",
    "* (['fold', 'dt_dow', 'engaging_user_id'], DataFrame[dt_dow: int, engaging_user_id: string, fold: double, GTE_dt_dow_engaging_user_id_reply_timestamp: float, GTE_dt_dow_engaging_user_id_retweet_timestamp: float, GTE_dt_dow_engaging_user_id_retweet_with_comment_timestamp: float, GTE_dt_dow_engaging_user_id_like_timestamp: float], 247354193)\n",
    "* (['fold', 'dt_hour', 'engaging_user_id'], DataFrame[dt_hour: int, engaging_user_id: string, fold: double, GTE_dt_hour_engaging_user_id_reply_timestamp: float, GTE_dt_hour_engaging_user_id_retweet_timestamp: float, GTE_dt_hour_engaging_user_id_retweet_with_comment_timestamp: float, GTE_dt_hour_engaging_user_id_like_timestamp: float], 337378319)\n",
    "* ['fold', 'tweet_type'] will do bhj\n",
    "* ['fold', 'dt_dow'] will do bhj\n",
    "* ['fold', 'most_used_word_bucket_id'] will do bhj\n",
    "* ['fold', 'second_used_word_bucket_id'] will do bhj\n",
    "* ['fold', 'mentioned_bucket_id'] will do bhj\n",
    "* ['fold', 'language'] will do bhj\n",
    "* ['fold', 'mentioned_count'] will do bhj\n",
    "* etstimated_to_shuffle_size for ['fold', 'engaged_with_user_id'] is 3.328 GB, will do smj\n",
    "* etstimated_to_shuffle_size for ['fold', 'language', 'engaged_with_user_id'] is 3.238 GB, will do smj\n",
    "* etstimated_to_shuffle_size for ['fold', 'tweet_type', 'engaged_with_user_id'] is 3.148 GB, will do smj\n",
    "* etstimated_to_shuffle_size for ['fold', 'mentioned_bucket_id', 'engaging_user_id'] is 3.058 GB, will do smj\n",
    "* etstimated_to_shuffle_size for ['fold', 'has_mention', 'engaging_user_id'] is 2.968 GB, will do smj\n",
    "* etstimated_to_shuffle_size for ['fold', 'language', 'engaging_user_id'] is 2.878 GB, will do smj\n",
    "* etstimated_to_shuffle_size for ['fold', 'dt_dow', 'engaged_with_user_id'] is 2.788 GB, will do smj\n",
    "* etstimated_to_shuffle_size for ['fold', 'tweet_type', 'engaging_user_id'] is 2.698 GB, will do smj\n",
    "* etstimated_to_shuffle_size for ['fold', 'mentioned_count', 'engaging_user_id'] is 2.608 GB, will do smj\n",
    "* etstimated_to_shuffle_size for ['fold', 'dt_hour', 'engaged_with_user_id'] is 2.518 GB, will do smj\n",
    "* etstimated_to_shuffle_size for ['fold', 'dt_dow', 'engaging_user_id'] is 2.428 GB, will do smj\n",
    "* etstimated_to_shuffle_size for ['fold', 'dt_hour', 'engaging_user_id'] is 2.338 GB, will do smj\n",
    "* do bhj to ['fold', 'tweet_type']\n",
    "* do bhj to ['fold', 'dt_dow']\n",
    "* do bhj to ['fold', 'most_used_word_bucket_id']\n",
    "* do bhj to ['fold', 'second_used_word_bucket_id']\n",
    "* do bhj to ['fold', 'mentioned_bucket_id']\n",
    "* do bhj to ['fold', 'language']\n",
    "* do bhj to ['fold', 'mentioned_count']\n",
    "* do smj to ['fold', 'engaged_with_user_id']\n",
    "* do smj to ['fold', 'language', 'engaged_with_user_id']\n",
    "* do smj to ['fold', 'tweet_type', 'engaged_with_user_id']\n",
    "* do smj to ['fold', 'mentioned_bucket_id', 'engaging_user_id']\n",
    "* do smj to ['fold', 'has_mention', 'engaging_user_id']\n",
    "* do smj to ['fold', 'language', 'engaging_user_id']\n",
    "* do smj to ['fold', 'dt_dow', 'engaged_with_user_id']\n",
    "* do smj to ['fold', 'tweet_type', 'engaging_user_id']\n",
    "* do smj to ['fold', 'mentioned_count', 'engaging_user_id']\n",
    "* do smj to ['fold', 'dt_hour', 'engaged_with_user_id']\n",
    "* do smj to ['fold', 'dt_dow', 'engaging_user_id']\n",
    "* do smj to ['fold', 'dt_hour', 'engaging_user_id']\n",
    "* encodingFeatures took 362.135\n",
    "\n",
    "======\n",
    "\n",
    "* bhj total threshold is 67.109 M rows, one bhj threshold is 30.000 M rows, flush_threshold is 960.000 GB\n",
    "* ('tweet', DataFrame[dict_col: string, count: bigint, dict_col_id: int], 148528557)\n",
    "* etstimated_to_shuffle_size for tweet is 20.076 GB, will do smj\n",
    "* do smj to bucketized_tweet_word\n",
    "* bhj total threshold is 67.109 M rows, one bhj threshold is 30.000 M rows, flush_threshold is 960.000 GB\n",
    "* ('mentioned_bucket_id', DataFrame[dict_col: string, count: bigint, dict_col_id: int], 4891669)\n",
    "* ('mentioned_count', DataFrame[dict_col: string, dict_col_id: bigint], 4891669)\n",
    "* ('mentioned_bucket_id', DataFrame[dict_col: string, count: bigint, dict_col_id: int], 4891669)\n",
    "* ('mentioned_count', DataFrame[dict_col: string, dict_col_id: bigint], 4891669)\n",
    "* mentioned_bucket_id will do bhj\n",
    "* mentioned_count will do bhj\n",
    "* mentioned_bucket_id will do bhj\n",
    "* mentioned_count will do bhj\n",
    "* do bhj to mentioned_bucket_id\n",
    "* do bhj to mentioned_count\n",
    "* do bhj to mentioned_bucket_id\n",
    "* do bhj to mentioned_count\n",
    "* bhj total threshold is 67.109 M rows, one bhj threshold is 30.000 M rows, flush_threshold is 960.000 GB\n",
    "* ('tweet_type', DataFrame[tweet_type: string, TE_tweet_type_retweet_with_comment_timestamp: float, TE_tweet_type_like_timestamp: float], 3)\n",
    "* ('dt_dow', DataFrame[dt_dow: int, TE_dt_dow_retweet_timestamp: float], 7)\n",
    "* ('most_used_word_bucket_id', DataFrame[most_used_word_bucket_id: string, TE_most_used_word_bucket_id_reply_timestamp: float, TE_most_used_word_bucket_id_retweet_timestamp: float, TE_most_used_word_bucket_id_retweet_with_comment_timestamp: float, TE_most_used_word_bucket_id_like_timestamp: float], 20)\n",
    "* ('second_used_word_bucket_id', DataFrame[second_used_word_bucket_id: string, TE_second_used_word_bucket_id_reply_timestamp: float, TE_second_used_word_bucket_id_retweet_timestamp: float, TE_second_used_word_bucket_id_retweet_with_comment_timestamp: float, TE_second_used_word_bucket_id_like_timestamp: float], 21)\n",
    "* ('mentioned_bucket_id', DataFrame[mentioned_bucket_id: int, TE_mentioned_bucket_id_reply_timestamp: float, TE_mentioned_bucket_id_retweet_timestamp: float, TE_mentioned_bucket_id_retweet_with_comment_timestamp: float, TE_mentioned_bucket_id_like_timestamp: float], 22)\n",
    "* ('language', DataFrame[language: string, TE_language_reply_timestamp: float, TE_language_retweet_timestamp: float, TE_language_retweet_with_comment_timestamp: float, TE_language_like_timestamp: float], 66)\n",
    "* ('mentioned_count', DataFrame[mentioned_count: bigint, TE_mentioned_count_reply_timestamp: float, TE_mentioned_count_retweet_timestamp: float, TE_mentioned_count_retweet_with_comment_timestamp: float, TE_mentioned_count_like_timestamp: float], 4328)\n",
    "* ('engaged_with_user_id', DataFrame[engaged_with_user_id: string, TE_engaged_with_user_id_reply_timestamp: float, TE_engaged_with_user_id_retweet_timestamp: float, TE_engaged_with_user_id_retweet_with_comment_timestamp: float, TE_engaged_with_user_id_like_timestamp: float], 22887349)\n",
    "* (['language', 'engaged_with_user_id'], DataFrame[language: string, engaged_with_user_id: string, GTE_language_engaged_with_user_id_reply_timestamp: float, GTE_language_engaged_with_user_id_retweet_timestamp: float, GTE_language_engaged_with_user_id_retweet_with_comment_timestamp: float, GTE_language_engaged_with_user_id_like_timestamp: float], 35483505)\n",
    "* (['tweet_type', 'engaged_with_user_id'], DataFrame[tweet_type: string, engaged_with_user_id: string, GTE_tweet_type_engaged_with_user_id_reply_timestamp: float, GTE_tweet_type_engaged_with_user_id_retweet_timestamp: float, GTE_tweet_type_engaged_with_user_id_retweet_with_comment_timestamp: float, GTE_tweet_type_engaged_with_user_id_like_timestamp: float], 36043484)\n",
    "* (['has_mention', 'engaging_user_id'], DataFrame[has_mention: int, engaging_user_id: string, GTE_has_mention_engaging_user_id_reply_timestamp: float, GTE_has_mention_engaging_user_id_retweet_timestamp: float, GTE_has_mention_engaging_user_id_retweet_with_comment_timestamp: float, GTE_has_mention_engaging_user_id_like_timestamp: float], 47561701)\n",
    "* (['mentioned_bucket_id', 'engaging_user_id'], DataFrame[mentioned_bucket_id: int, engaging_user_id: string, GTE_mentioned_bucket_id_engaging_user_id_reply_timestamp: float, GTE_mentioned_bucket_id_engaging_user_id_retweet_timestamp: float, GTE_mentioned_bucket_id_engaging_user_id_retweet_with_comment_timestamp: float, GTE_mentioned_bucket_id_engaging_user_id_like_timestamp: float], 52816686)\n",
    "* (['language', 'engaging_user_id'], DataFrame[language: string, engaging_user_id: string, GTE_language_engaging_user_id_reply_timestamp: float, GTE_language_engaging_user_id_retweet_timestamp: float, GTE_language_engaging_user_id_retweet_with_comment_timestamp: float, GTE_language_engaging_user_id_like_timestamp: float], 62810211)\n",
    "* (['tweet_type', 'engaging_user_id'], DataFrame[tweet_type: string, engaging_user_id: string, GTE_tweet_type_engaging_user_id_reply_timestamp: float, GTE_tweet_type_engaging_user_id_retweet_timestamp: float, GTE_tweet_type_engaging_user_id_retweet_with_comment_timestamp: float, GTE_tweet_type_engaging_user_id_like_timestamp: float], 63468214)\n",
    "* (['dt_dow', 'engaged_with_user_id'], DataFrame[dt_dow: int, engaged_with_user_id: string, GTE_dt_dow_engaged_with_user_id_reply_timestamp: float, GTE_dt_dow_engaged_with_user_id_retweet_timestamp: float, GTE_dt_dow_engaged_with_user_id_retweet_with_comment_timestamp: float, GTE_dt_dow_engaged_with_user_id_like_timestamp: float], 69049586)\n",
    "* (['mentioned_count', 'engaging_user_id'], DataFrame[mentioned_count: bigint, engaging_user_id: string, GTE_mentioned_count_engaging_user_id_reply_timestamp: float, GTE_mentioned_count_engaging_user_id_retweet_timestamp: float, GTE_mentioned_count_engaging_user_id_retweet_with_comment_timestamp: float, GTE_mentioned_count_engaging_user_id_like_timestamp: float], 92173751)\n",
    "* (['dt_hour', 'engaged_with_user_id'], DataFrame[dt_hour: int, engaged_with_user_id: string, GTE_dt_hour_engaged_with_user_id_reply_timestamp: float, GTE_dt_hour_engaged_with_user_id_retweet_timestamp: float, GTE_dt_hour_engaged_with_user_id_retweet_with_comment_timestamp: float, GTE_dt_hour_engaged_with_user_id_like_timestamp: float], 102369370)\n",
    "* (['dt_dow', 'engaging_user_id'], DataFrame[dt_dow: int, engaging_user_id: string, GTE_dt_dow_engaging_user_id_reply_timestamp: float, GTE_dt_dow_engaging_user_id_retweet_timestamp: float, GTE_dt_dow_engaging_user_id_retweet_with_comment_timestamp: float, GTE_dt_dow_engaging_user_id_like_timestamp: float], 116332109)\n",
    "* (['dt_hour', 'engaging_user_id'], DataFrame[dt_hour: int, engaging_user_id: string, GTE_dt_hour_engaging_user_id_reply_timestamp: float, GTE_dt_hour_engaging_user_id_retweet_timestamp: float, GTE_dt_hour_engaging_user_id_retweet_with_comment_timestamp: float, GTE_dt_hour_engaging_user_id_like_timestamp: float], 192785849)\n",
    "* tweet_type will do bhj\n",
    "* dt_dow will do bhj\n",
    "* most_used_word_bucket_id will do bhj\n",
    "* second_used_word_bucket_id will do bhj\n",
    "* mentioned_bucket_id will do bhj\n",
    "* language will do bhj\n",
    "* mentioned_count will do bhj\n",
    "* engaged_with_user_id will do bhj\n",
    "* etstimated_to_shuffle_size for ['language', 'engaged_with_user_id'] is 17.399 GB, will do smj\n",
    "* etstimated_to_shuffle_size for ['tweet_type', 'engaged_with_user_id'] is 16.897 GB, will do smj\n",
    "* etstimated_to_shuffle_size for ['has_mention', 'engaging_user_id'] is 16.396 GB, will do smj\n",
    "* etstimated_to_shuffle_size for ['mentioned_bucket_id', 'engaging_user_id'] is 15.894 GB, will do smj\n",
    "* etstimated_to_shuffle_size for ['language', 'engaging_user_id'] is 15.392 GB, will do smj\n",
    "* etstimated_to_shuffle_size for ['tweet_type', 'engaging_user_id'] is 14.890 GB, will do smj\n",
    "* etstimated_to_shuffle_size for ['dt_dow', 'engaged_with_user_id'] is 14.388 GB, will do smj\n",
    "* etstimated_to_shuffle_size for ['mentioned_count', 'engaging_user_id'] is 13.886 GB, will do smj\n",
    "* etstimated_to_shuffle_size for ['dt_hour', 'engaged_with_user_id'] is 13.384 GB, will do smj\n",
    "* etstimated_to_shuffle_size for ['dt_dow', 'engaging_user_id'] is 12.882 GB, will do smj\n",
    "* etstimated_to_shuffle_size for ['dt_hour', 'engaging_user_id'] is 12.380 GB, will do smj\n",
    "* do bhj to tweet_type\n",
    "* do bhj to dt_dow\n",
    "* do bhj to most_used_word_bucket_id\n",
    "* do bhj to second_used_word_bucket_id\n",
    "* do bhj to mentioned_bucket_id\n",
    "* do bhj to language\n",
    "* do bhj to mentioned_count\n",
    "* do bhj to engaged_with_user_id\n",
    "* do smj to ['language', 'engaged_with_user_id']\n",
    "* do smj to ['tweet_type', 'engaged_with_user_id']\n",
    "* do smj to ['has_mention', 'engaging_user_id']\n",
    "* do smj to ['mentioned_bucket_id', 'engaging_user_id']\n",
    "* do smj to ['language', 'engaging_user_id']\n",
    "* do smj to ['tweet_type', 'engaging_user_id']\n",
    "* do smj to ['dt_dow', 'engaged_with_user_id']\n",
    "* do smj to ['mentioned_count', 'engaging_user_id']\n",
    "* do smj to ['dt_hour', 'engaged_with_user_id']\n",
    "* do smj to ['dt_dow', 'engaging_user_id']\n",
    "* do smj to ['dt_hour', 'engaging_user_id']\n",
    "* mergeFeaturesToTest took 1259.446\n",
    "\n",
    "======\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
