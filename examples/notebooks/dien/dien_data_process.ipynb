{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/nvme2/chendi/BlueWhale/recdp\n"
     ]
    }
   ],
   "source": [
    "import init\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql import *\n",
    "from pyspark import *\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.types import *\n",
    "from timeit import default_timer as timer\n",
    "import logging\n",
    "from pyrecdp.data_processor import *\n",
    "from pyrecdp.utils import *\n",
    "\n",
    "def load_csv(path):\n",
    "    review_id_field = StructField('reviewer_id', StringType())\n",
    "    asin_field = StructField('asin', StringType())\n",
    "    overall_field = StructField('overall', FloatType())\n",
    "    unix_time_field = StructField('unix_review_time', IntegerType())\n",
    "    reviews_info_schema = StructType(\n",
    "        [review_id_field, asin_field, overall_field, unix_time_field])\n",
    "\n",
    "    category_field = StructField('category', StringType())\n",
    "    item_info_schema = StructType([asin_field, category_field])\n",
    "\n",
    "    reviews_info_df = spark.read.schema(reviews_info_schema).option('sep', '\\t').csv(path + \"/reviews-info\")\n",
    "    item_info_df = spark.read.schema(item_info_schema).option('sep', '\\t').csv(path + \"/item-info\")\n",
    "\n",
    "    return reviews_info_df, item_info_df\n",
    "\n",
    "\n",
    "def collapse_by_hist(df, item_info_df, proc, output_name, min_num_hist = 0):\n",
    "    op_model_merge = ModelMerge([{'col_name': 'asin', 'dict': item_info_df}])\n",
    "    op_collapse_by_hist = CollapseByHist(['asin', 'category'], by = ['reviewer_id'], orderBy = ['unix_review_time', 'asin'], minNumHist = min_num_hist)\n",
    "    proc.reset_ops([op_model_merge, op_collapse_by_hist])\n",
    "    \n",
    "    t1 = timer()\n",
    "    df = proc.transform(df, output_name)\n",
    "    t2 = timer()\n",
    "    print(f\"Merge with category and collapse hist took {t2 - t1} secs\")\n",
    "    return df\n",
    "    \n",
    "\n",
    "def add_negative_sample(df, item_info_df, proc, output_name, gen_dict = True):\n",
    "    dict_dfs = []\n",
    "    if gen_dict:\n",
    "        op_gen_dict = GenerateDictionary(['asin'])\n",
    "        proc.reset_ops([op_gen_dict])\n",
    "        t1 = timer()\n",
    "        dict_dfs = proc.generate_dicts(df)\n",
    "        t2 = timer()\n",
    "        print(\"Generate Dictionary took %.3f\" % (t2 - t1))    \n",
    "    else:\n",
    "        dict_names = ['asin']\n",
    "        dict_dfs = [{'col_name': name, 'dict': proc.spark.read.parquet(\n",
    "            \"%s/%s/%s/%s\" % (proc.path_prefix, proc.current_path, proc.dicts_path, name))} for name in dict_names]\n",
    "    \n",
    "    # add negative_sample as new row\n",
    "    op_negative_sample = NegativeSample(['asin'], dict_dfs)\n",
    "    op_drop_category = DropFeature(['category'])\n",
    "    op_add_category = ModelMerge([{'col_name': 'asin', 'dict': item_info_df}])\n",
    "    op_select = SelectFeature(['pos', 'reviewer_id', 'asin', 'category', 'hist_asin', 'hist_category'])\n",
    "    proc.reset_ops([op_negative_sample, op_drop_category, op_add_category, op_select])\n",
    "    t1 = timer()\n",
    "    df = proc.transform(df, output_name)\n",
    "    t2 = timer()    \n",
    "    print(f\"add_negative_sample took {t2 - t1} secs\")\n",
    "    return df\n",
    "\n",
    "def save_to_voc(df, proc, cols, default_name, default_v, output_name):\n",
    "    import pickle\n",
    "    col_name = ''\n",
    "    dtypes_list = []\n",
    "    if isinstance(cols, list):\n",
    "        col_name = cols[0]\n",
    "        dtypes_list = df.select(*cols).dtypes\n",
    "    elif isinstance(cols, str):\n",
    "        col_name = cols\n",
    "        dtypes_list = df.select(cols).dtypes\n",
    "    else:\n",
    "        raise ValueError(\"save_to_voc expects cols as String or list of String\")\n",
    "\n",
    "    to_select = []\n",
    "    if len(dtypes_list) == 1 and 'array' not in dtypes_list[0][1]:\n",
    "        to_select.append(f.col(dtypes_list[0][0]))\n",
    "        dict_df = df.select(*to_select)\n",
    "    else:\n",
    "        for name, dtype in dtypes_list:\n",
    "            if 'array' in dtype:\n",
    "                to_select.append(f.col(name))\n",
    "            else:\n",
    "                to_select.append(f.array(f.col(name)))\n",
    "\n",
    "        dict_df = df.withColumn(col_name, f.array_union(*to_select))\n",
    "        dict_df = dict_df.select(f.explode(f.col(col_name)).alias(col_name))\n",
    "    dict_df = dict_df.filter(f\"{col_name} is not null\").groupBy(col_name).count().orderBy(f.desc('count')).select(col_name)\n",
    "    collected = [row[col_name] for row in dict_df.collect()]\n",
    "\n",
    "    voc = {}\n",
    "    voc[default_name] = default_v\n",
    "    voc.update(dict((col_id, col_idx) for (col_id, col_idx) in zip(collected, range(1, len(collected) + 1))))\n",
    "    pickle.dump(voc, open(proc.current_path + f'/{output_name}', \"wb\"), protocol=0)\n",
    "\n",
    "\n",
    "def save_to_uid_voc(df, proc):\n",
    "    # saving (using python)\n",
    "    # build uid_dict, mid_dict and cat_dict\n",
    "    t1 = timer()\n",
    "    save_to_voc(df, proc, ['reviewer_id'], 'A1Y6U82N6TYZPI', 0, 'uid_voc.pkl')\n",
    "    t2 = timer()\n",
    "    print(f\"save_to_uid_voc took {t2 - t1} secs\")\n",
    "    \n",
    "def save_to_mid_voc(df, proc):\n",
    "    t1 = timer()\n",
    "    save_to_voc(df, proc, ['hist_asin', 'asin'], 'default_mid', 0, 'mid_voc.pkl')\n",
    "    t2 = timer()\n",
    "    print(f\"save_to_mid_voc took {t2 - t1} secs\")\n",
    "    \n",
    "    \n",
    "def save_to_cat_voc(df, proc):\n",
    "    t1 = timer()\n",
    "    save_to_voc(df, proc, ['hist_category', 'category'], 'default_cat', 0, 'cat_voc.pkl')\n",
    "    t2 = timer()\n",
    "    print(f\"save_to_cat_voc took {t2 - t1} secs\")\n",
    "    \n",
    "    \n",
    "def save_to_local_train_splitByUser(df, proc):\n",
    "    t1 = timer()\n",
    "    dict_df = df.select('pos', 'reviewer_id', 'asin', 'category',\n",
    "                        f.expr(\"concat_ws('\\x02', hist_asin)\"),\n",
    "                        f.expr(\"concat_ws('\\x02', hist_category)\"))\n",
    "    collected = [[c1, c2, c3, c4, c5, c6] for (c1, c2, c3, c4, c5, c6) in dict_df.collect()]\n",
    "    user_map = {}\n",
    "    for items in collected:\n",
    "        if items[1] not in user_map:\n",
    "            user_map[items[1]] = []\n",
    "        user_map[items[1]].append(items)\n",
    "    with open(proc.current_path + \"/local_train_splitByUser\", 'w') as fp:\n",
    "        for user, r in user_map.items():\n",
    "            positive_sorted = sorted(r, key=lambda x: x[0])\n",
    "            for items in positive_sorted:\n",
    "                print('\\t'.join([str(x) for x in items]), file=fp)\n",
    "    t2 = timer()\n",
    "    print(f\"save_to_local_train_splitByUser took {t2 - t1} secs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "path_prefix = \"file://\"\n",
    "current_path = \"/mnt/nvme2/chendi/BlueWhale/recdp/examples/notebooks/dien/output/\"\n",
    "original_folder = \"/mnt/nvme2/chendi/BlueWhale/recdp/examples/notebooks/dien/\"\n",
    "\n",
    "scala_udf_jars = \"/mnt/nvme2/chendi/BlueWhale/recdp/ScalaProcessUtils/target/recdp-scala-extensions-0.1.0-jar-with-dependencies.jar\"\n",
    "\n",
    "##### 1. Start spark and initialize data processor #####\n",
    "t0 = timer()\n",
    "spark = SparkSession.builder.master('yarn')\\\n",
    "    .appName(\"dien_data_process\")\\\n",
    "    .config(\"spark.executor.memory\", \"18g\")\\\n",
    "    .config(\"spark.executor.memoryOverhead\", \"5g\")\\\n",
    "    .config(\"spark.driver.extraClassPath\", f\"{scala_udf_jars}\")\\\n",
    "    .config(\"spark.executor.extraClassPath\", f\"{scala_udf_jars}\")\\\n",
    "    .config(\"spark.executor.cores\", \"4\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "# 1.1 prepare dataFrames\n",
    "# 1.2 create RecDP DataProcessor\n",
    "proc = DataProcessor(spark, path_prefix, current_path=current_path, shuffle_disk_capacity=\"1200GB\")\n",
    "\n",
    "# ===============================================\n",
    "# basic: Do categorify for all columns\n",
    "reviews_info_df, item_info_df = load_csv(path_prefix + original_folder)\n",
    "\n",
    "# 1. join records with its category and then collapse history \n",
    "df = reviews_info_df\n",
    "df = collapse_by_hist(df, item_info_df, proc, \"collapsed\", min_num_hist = 2)\n",
    "\n",
    "# 2. add negative sample to records\n",
    "df = add_negative_sample(df, item_info_df, proc, \"records_with_negative_sample\", gen_dict = True)\n",
    "\n",
    "# df = spark.read.parquet(path_prefix + current_path + \"records_with_negative_sample\")\n",
    "save_to_uid_voc(df, proc)\n",
    "save_to_mid_voc(df, proc)\n",
    "save_to_cat_voc(df, proc)\n",
    "save_to_local_train_splitByUser(df, proc)\n",
    "\n",
    "####################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* recdp-scala-extension is enabled\n",
    "* per core memory size is 4.500 GB and shuffle_disk maximum capacity is 1200.000 GB\n",
    "* \n",
    "* Merge with category and collapse hist took 274.15958739898633 secs\n",
    "* Generate Dictionary took 2.550 secs\n",
    "* add_negative_sample took 9.529635827988386 secs\n",
    "* save_to_uid_voc took 10.749223954975605 secs\n",
    "* save_to_mid_voc took 13.278015322983265 secs\n",
    "* save_to_cat_voc took 1.4347403410356492 secs\n",
    "* save_to_local_train_splitByUser took 29.36967344605364 secs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
