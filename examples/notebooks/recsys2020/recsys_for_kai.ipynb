{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/nvme2/chendi/BlueWhale/frameworks.bigdata.bluewhale/RecDP\n"
     ]
    }
   ],
   "source": [
    "#!/env/bin/python\n",
    "\n",
    "import init\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql import *\n",
    "from pyspark import *\n",
    "import pyspark.sql.functions as f\n",
    "from timeit import default_timer as timer\n",
    "import logging\n",
    "from RecsysSchema import RecsysSchema\n",
    "from pyrecdp.data_processor import *\n",
    "from pyrecdp.utils import *\n",
    "import hashlib\n",
    "\n",
    "def categorifyAllFeatures(df, proc, output_name=\"categorified\", gen_dict=False):\n",
    "    dict_dfs = []\n",
    "    if gen_dict:\n",
    "        # only call below function when target dicts were not pre-prepared\n",
    "        op_multiItems = GenerateDictionary(\n",
    "            ['present_domains', 'present_links', 'hashtags'], doSplit=True)\n",
    "        op_singleItems = GenerateDictionary(['tweet_id', 'language', {'src_cols': [\n",
    "                                            'engaged_with_user_id', 'engaging_user_id'], 'col_name': 'user_id'}])\n",
    "        proc.reset_ops([op_multiItems, op_singleItems])\n",
    "        t1 = timer()\n",
    "        dict_dfs = proc.generate_dicts(df)\n",
    "        t2 = timer()\n",
    "        print(\"Generate Dictionary took %.3f\" % (t2 - t1))\n",
    "    else:\n",
    "        # or we can simply load from pre-gened\n",
    "        dict_names = ['hashtags', 'language', 'present_domains',\n",
    "                      'present_links', 'tweet_id', 'user_id']\n",
    "        dict_dfs = [{'col_name': name, 'dict': proc.spark.read.parquet(\n",
    "            \"%s/%s/%s/%s\" % (proc.path_prefix, proc.current_path, proc.dicts_path, name))} for name in dict_names]\n",
    "\n",
    "    # pre-defined dict\n",
    "    # pre-define\n",
    "    media = {\n",
    "        '': 0,\n",
    "        'GIF': 1,\n",
    "        'GIF_GIF': 2,\n",
    "        'GIF_Photo': 3,\n",
    "        'GIF_Video': 4,\n",
    "        'Photo': 5,\n",
    "        'Photo_GIF': 6,\n",
    "        'Photo_Photo': 7,\n",
    "        'Photo_Video': 8,\n",
    "        'Video': 9,\n",
    "        'Video_GIF': 10,\n",
    "        'Video_Photo': 11,\n",
    "        'Video_Video': 12\n",
    "    }\n",
    "\n",
    "    tweet_type = {'Quote': 0, 'Retweet': 1, 'TopLevel': 2}\n",
    "\n",
    "    media_df = proc.spark.createDataFrame(convert_to_spark_dict(media))\n",
    "    tweet_type_df = proc.spark.createDataFrame(\n",
    "        convert_to_spark_dict(tweet_type))\n",
    "\n",
    "    dict_dfs.append({'col_name': 'present_media', 'dict': media_df})\n",
    "    dict_dfs.append({'col_name': 'tweet_type', 'dict': tweet_type_df})\n",
    "\n",
    "    for i in dict_dfs:\n",
    "        dict_name = i['col_name']\n",
    "        dict_df = i['dict']\n",
    "        print(\"%s has numRows as %d\" % (dict_name, dict_df.count()))\n",
    "\n",
    "    ###### 2. define operations and append them to data processor ######\n",
    "\n",
    "    # 1. define operations\n",
    "    # 1.1 fill na and features\n",
    "    op_fillna_str = FillNA(\n",
    "        ['present_domains', 'present_links', 'hashtags', 'present_media', 'tweet_id'], \"\")\n",
    "\n",
    "    # 1.3 categorify\n",
    "    # since language dict is small, we may use udf to make partition more even\n",
    "    op_categorify_multi = Categorify(\n",
    "        ['present_domains', 'present_links', 'hashtags'], dict_dfs=dict_dfs, doSplit=True, keepMostFrequent=True)\n",
    "    op_fillna_for_categorified = FillNA(['present_domains', 'present_links', 'hashtags'], -1)\n",
    "\n",
    "    # transform\n",
    "    proc.append_ops([op_fillna_str, op_categorify_multi, op_fillna_for_categorified])\n",
    "    t1 = timer()\n",
    "    df = proc.transform(df, name=output_name)\n",
    "    t2 = timer()\n",
    "    print(\"Data Process 1 and udf categorify took %.3f\" % (t2 - t1))\n",
    "\n",
    "    return df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "per core memory size is 7.500 GB and shuffle_disk maximum capacity is 1200.000 GB\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "Path does not exist: hdfs://sr602:9000/recsys2020_0608_categorify_example/recsys_dicts/language",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-22eeb19413e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumnRenamed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'enaging_user_following_count'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'engaging_user_following_count'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumnRenamed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'enaging_user_is_verified'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'engaging_user_is_verified'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcategorifyAllFeatures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1-478d2d05efda>\u001b[0m in \u001b[0;36mcategorifyAllFeatures\u001b[0;34m(df, proc, output_name, gen_dict)\u001b[0m\n\u001b[1;32m     37\u001b[0m                       'present_links', 'tweet_id', 'user_id']\n\u001b[1;32m     38\u001b[0m         dict_dfs = [{'col_name': name, 'dict': proc.spark.read.parquet(\n\u001b[0;32m---> 39\u001b[0;31m             \"%s/%s/%s/%s\" % (proc.path_prefix, proc.current_path, proc.dicts_path, name))} for name in dict_names]\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;31m# pre-defined dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-478d2d05efda>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     37\u001b[0m                       'present_links', 'tweet_id', 'user_id']\n\u001b[1;32m     38\u001b[0m         dict_dfs = [{'col_name': name, 'dict': proc.spark.read.parquet(\n\u001b[0;32m---> 39\u001b[0;31m             \"%s/%s/%s/%s\" % (proc.path_prefix, proc.current_path, proc.dicts_path, name))} for name in dict_names]\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;31m# pre-defined dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/hadoop/spark/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mparquet\u001b[0;34m(self, *paths, **options)\u001b[0m\n\u001b[1;32m    456\u001b[0m                        modifiedAfter=modifiedAfter)\n\u001b[1;32m    457\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpaths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m     def text(self, paths, wholetext=False, lineSep=None, pathGlobFilter=None,\n",
      "\u001b[0;32m/hadoop/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/hadoop/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Path does not exist: hdfs://sr602:9000/recsys2020_0608_categorify_example/recsys_dicts/language"
     ]
    }
   ],
   "source": [
    "path_prefix = \"hdfs://\"\n",
    "current_path = \"/recsys2020_0608_categorify_example/\"\n",
    "original_folder = \"/recsys2021_0608/\"\n",
    "dicts_folder = \"recsys_dicts/\"\n",
    "recsysSchema = RecsysSchema()\n",
    "\n",
    "##### 1. Start spark and initialize data processor #####\n",
    "t0 = timer()\n",
    "spark = SparkSession.builder.master('yarn')\\\n",
    "    .appName(\"Recsys2020_data_process\")\\\n",
    "    .config(\"spark.executor.memory\", \"30g\")\\\n",
    "    .config(\"spark.executor.memoryOverhead\", \"12g\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "schema = recsysSchema.toStructType()\n",
    "\n",
    "# 1.1 prepare dataFrames\n",
    "# 1.2 create RecDP DataProcessor\n",
    "proc = DataProcessor(spark, path_prefix,\n",
    "                     current_path=current_path, dicts_path=dicts_folder, shuffle_disk_capacity=\"1200GB\")\n",
    "\n",
    "# ===============================================\n",
    "# basic: Do categorify for all columns\n",
    "df = spark.read.parquet(path_prefix + original_folder)\n",
    "\n",
    "# rename firstly\n",
    "df = df.withColumnRenamed('enaging_user_following_count', 'engaging_user_following_count')\n",
    "df = df.withColumnRenamed('enaging_user_is_verified', 'engaging_user_is_verified')\n",
    "df = categorifyAllFeatures(df, proc, gen_dict=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* per core memory size is 7.500 GB and shuffle_disk maximum capacity is 1200.000 GB\n",
    "* hashtags has numRows as 6541935\n",
    "* language has numRows as 66\n",
    "* present_domains has numRows as 896783\n",
    "* present_links has numRows as 14804300\n",
    "* tweet_id has numRows as 279597527\n",
    "* user_id has numRows as 42850559\n",
    "* present_media has numRows as 13\n",
    "* tweet_type has numRows as 3\n",
    "* categorify threshold is 50.000 M rows, flush_threshold is 960.000 GB\n",
    "* ('tweet_type', DataFrame[dict_col: string, dict_col_id: bigint], 3)\n",
    "* ('present_media', DataFrame[dict_col: string, dict_col_id: bigint], 13)\n",
    "* ('language', DataFrame[dict_col: string, dict_col_id: int], 66)\n",
    "* ('present_domains', DataFrame[dict_col: string, dict_col_id: int], 896783)\n",
    "* ('hashtags', DataFrame[dict_col: string, dict_col_id: int], 6541935)\n",
    "* ('present_links', DataFrame[dict_col: string, dict_col_id: int], 14804300)\n",
    "* ('user_id', DataFrame[dict_col: string, dict_col_id: int], 42850559)\n",
    "* ('tweet_id', DataFrame[dict_col: string, dict_col_id: int], 279597527)\n",
    "* tweet_type will do udf\n",
    "* present_media will do udf\n",
    "* language will do udf\n",
    "* present_domains will do udf\n",
    "* hashtags will do udf\n",
    "* present_links will do udf\n",
    "* etstimated_to_shuffle_size for user_id is 81.653 GB, will do shj\n",
    "* etstimated_to_shuffle_size for tweet_id is 78.153 GB, will do shj\n",
    "* Data Process 1 and udf categorify took 799.987"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
