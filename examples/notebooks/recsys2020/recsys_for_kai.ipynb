{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/nvme2/chendi/BlueWhale/recdp\n"
     ]
    }
   ],
   "source": [
    "#!/env/bin/python\n",
    "\n",
    "import init\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql import *\n",
    "from pyspark import *\n",
    "import pyspark.sql.functions as f\n",
    "from timeit import default_timer as timer\n",
    "import logging\n",
    "from RecsysSchema import RecsysSchema\n",
    "from pyrecdp.data_processor import *\n",
    "from pyrecdp.utils import *\n",
    "import hashlib\n",
    "\n",
    "def categorifyAllFeatures(df, proc, output_name=\"categorified\", gen_dict=False):\n",
    "    dict_dfs = []\n",
    "    if gen_dict:\n",
    "        # only call below function when target dicts were not pre-prepared\n",
    "        op_multiItems = GenerateDictionary(\n",
    "            ['present_domains', 'present_links', 'hashtags'], doSplit=True)\n",
    "        # op_singleItems = GenerateDictionary(['tweet_id', 'language', {'src_cols': [\n",
    "        #                                    'engaged_with_user_id', 'engaging_user_id'], 'col_name': 'user_id'}])\n",
    "        proc.reset_ops([op_multiItems])\n",
    "        t1 = timer()\n",
    "        dict_dfs = proc.generate_dicts(df)\n",
    "        t2 = timer()\n",
    "        print(\"Generate Dictionary took %.3f\" % (t2 - t1))\n",
    "    else:\n",
    "        # or we can simply load from pre-gened\n",
    "        dict_names = ['hashtags', 'present_domains','present_links']\n",
    "        dict_dfs = [{'col_name': name, 'dict': proc.spark.read.parquet(\n",
    "            \"%s/%s/%s/%s\" % (proc.path_prefix, proc.current_path, proc.dicts_path, name))} for name in dict_names]\n",
    "\n",
    "    # pre-defined dict\n",
    "    # pre-define\n",
    "    media = {\n",
    "        '': 0,\n",
    "        'GIF': 1,\n",
    "        'GIF_GIF': 2,\n",
    "        'GIF_Photo': 3,\n",
    "        'GIF_Video': 4,\n",
    "        'Photo': 5,\n",
    "        'Photo_GIF': 6,\n",
    "        'Photo_Photo': 7,\n",
    "        'Photo_Video': 8,\n",
    "        'Video': 9,\n",
    "        'Video_GIF': 10,\n",
    "        'Video_Photo': 11,\n",
    "        'Video_Video': 12\n",
    "    }\n",
    "\n",
    "    tweet_type = {'Quote': 0, 'Retweet': 1, 'TopLevel': 2}\n",
    "\n",
    "    media_df = proc.spark.createDataFrame(convert_to_spark_dict(media))\n",
    "    tweet_type_df = proc.spark.createDataFrame(\n",
    "        convert_to_spark_dict(tweet_type))\n",
    "\n",
    "    dict_dfs.append({'col_name': 'present_media', 'dict': media_df})\n",
    "    dict_dfs.append({'col_name': 'tweet_type', 'dict': tweet_type_df})\n",
    "\n",
    "    for i in dict_dfs:\n",
    "        dict_name = i['col_name']\n",
    "        dict_df = i['dict']\n",
    "        print(\"%s has numRows as %d\" % (dict_name, dict_df.count()))\n",
    "\n",
    "    ###### 2. define operations and append them to data processor ######\n",
    "\n",
    "    # 1. define operations\n",
    "    # 1.1 fill na and features\n",
    "    op_fillna_str = FillNA(\n",
    "        ['present_domains', 'present_links', 'hashtags', 'present_media', 'tweet_id'], \"\")\n",
    "\n",
    "    # 1.3 categorify\n",
    "    # since language dict is small, we may use udf to make partition more even\n",
    "    #'present_domains', 'present_links', \n",
    "    op_categorify_multi = Categorify(\n",
    "        ['present_domains', 'present_links', 'hashtags'], dict_dfs=dict_dfs, doSplit=True, keepMostFrequent=True)\n",
    "    op_fillna_for_categorified = FillNA(['present_domains', 'present_links', 'hashtags'], -1)\n",
    "\n",
    "    # transform\n",
    "    proc.append_ops([op_fillna_str, op_categorify_multi, op_fillna_for_categorified])\n",
    "    t1 = timer()\n",
    "    df = proc.transform(df, name=output_name)\n",
    "    t2 = timer()\n",
    "    print(\"Data Process 1 and udf categorify took %.3f\" % (t2 - t1))\n",
    "\n",
    "    return df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "path_prefix = \"hdfs://\"\n",
    "current_path = \"/recsys2020_0608_categorify_example/\"\n",
    "original_folder = \"/recsys2021_0608/\"\n",
    "dicts_folder = \"recsys_dicts/\"\n",
    "recsysSchema = RecsysSchema()\n",
    "\n",
    "scala_udf_jars = \"/mnt/nvme2/chendi/BlueWhale/recdp/ScalaProcessUtils/target/recdp-scala-extensions-0.1.0-jar-with-dependencies.jar\"\n",
    "\n",
    "##### 1. Start spark and initialize data processor #####\n",
    "t0 = timer()\n",
    "spark = SparkSession.builder.master('yarn')\\\n",
    "    .appName(\"Recsys2020_data_process\")\\\n",
    "    .config(\"spark.executor.memory\", \"18g\")\\\n",
    "    .config(\"spark.executor.memoryOverhead\", \"5g\")\\\n",
    "    .config(\"spark.driver.extraClassPath\", f\"{scala_udf_jars}\")\\\n",
    "    .config(\"spark.executor.extraClassPath\", f\"{scala_udf_jars}\")\\\n",
    "    .config(\"spark.executor.cores\", \"4\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "schema = recsysSchema.toStructType()\n",
    "\n",
    "# 1.1 prepare dataFrames\n",
    "# 1.2 create RecDP DataProcessor\n",
    "proc = DataProcessor(spark, path_prefix,\n",
    "                     current_path=current_path, dicts_path=dicts_folder, shuffle_disk_capacity=\"1200GB\")\n",
    "\n",
    "# ===============================================\n",
    "# basic: Do categorify for all columns\n",
    "df = spark.read.parquet(path_prefix + original_folder)\n",
    "\n",
    "# rename firstly\n",
    "df = df.withColumnRenamed('enaging_user_following_count', 'engaging_user_following_count')\n",
    "df = df.withColumnRenamed('enaging_user_is_verified', 'engaging_user_is_verified')\n",
    "df = categorifyAllFeatures(df, proc, gen_dict=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* per core memory size is 7.500 GB and shuffle_disk maximum capacity is 1200.000 GB\n",
    "* hashtags has numRows as 6541935\n",
    "* language has numRows as 66\n",
    "* present_domains has numRows as 896783\n",
    "* present_links has numRows as 14804300\n",
    "* tweet_id has numRows as 279597527\n",
    "* user_id has numRows as 42850559\n",
    "* present_media has numRows as 13\n",
    "* tweet_type has numRows as 3\n",
    "* categorify threshold is 50.000 M rows, flush_threshold is 960.000 GB\n",
    "* ('tweet_type', DataFrame[dict_col: string, dict_col_id: bigint], 3)\n",
    "* ('present_media', DataFrame[dict_col: string, dict_col_id: bigint], 13)\n",
    "* ('language', DataFrame[dict_col: string, dict_col_id: int], 66)\n",
    "* ('present_domains', DataFrame[dict_col: string, dict_col_id: int], 896783)\n",
    "* ('hashtags', DataFrame[dict_col: string, dict_col_id: int], 6541935)\n",
    "* ('present_links', DataFrame[dict_col: string, dict_col_id: int], 14804300)\n",
    "* ('user_id', DataFrame[dict_col: string, dict_col_id: int], 42850559)\n",
    "* ('tweet_id', DataFrame[dict_col: string, dict_col_id: int], 279597527)\n",
    "* tweet_type will do udf\n",
    "* present_media will do udf\n",
    "* language will do udf\n",
    "* present_domains will do udf\n",
    "* hashtags will do udf\n",
    "* present_links will do udf\n",
    "* etstimated_to_shuffle_size for user_id is 81.653 GB, will do shj\n",
    "* etstimated_to_shuffle_size for tweet_id is 78.153 GB, will do shj\n",
    "* Data Process 1 and udf categorify took 799.987\n",
    "\n",
    "### After switching to use scala udf, process time will be longer while we don't need to assign bigger memoryOverHead to spark\n",
    "\n",
    "* recdp-scala-extension is enabled\n",
    "* per core memory size is 4.500 GB and shuffle_disk maximum capacity is 1200.000 GB\n",
    "* hashtags has numRows as 6541935\n",
    "* present_domains has numRows as 896783\n",
    "* present_links has numRows as 14804300\n",
    "* present_media has numRows as 13\n",
    "* tweet_type has numRows as 3\n",
    "* bhj total threshold is 60.398 M rows, one bhj threshold is 30.000 M rows, flush_threshold is 960.000 GB\n",
    "* ('present_domains', DataFrame[dict_col: string, dict_col_id: int, count: bigint], 896783)\n",
    "* ('hashtags', DataFrame[dict_col: string, dict_col_id: int, count: bigint], 6541935)\n",
    "* ('present_links', DataFrame[dict_col: string, dict_col_id: int, count: bigint], 14804300)\n",
    "* present_domains will do udf\n",
    "* hashtags will do udf\n",
    "* present_links will do udf\n",
    "* Data Process 1 and udf categorify took 1045.646"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
