{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/env/bin/python\n",
    "\n",
    "import init\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql import *\n",
    "from pyspark import *\n",
    "import pyspark.sql.functions as f\n",
    "from timeit import default_timer as timer\n",
    "import logging\n",
    "from RecsysSchema import RecsysSchema\n",
    "from pyrecdp.data_processor import *\n",
    "from pyrecdp.utils import *\n",
    "import hashlib\n",
    "\n",
    "def categorifyAllFeatures(df, proc, output_name=\"categorified\", gen_dicts=False):\n",
    "    # 1. define operations\n",
    "    # 1.1 fill na and features\n",
    "    op_fillna_str = FillNA(\n",
    "        ['present_domains', 'present_links', 'hashtags'], \"\")\n",
    "\n",
    "    # 1.3 categorify\n",
    "    # since language dict is small, we may use udf to make partition more even\n",
    "    #'present_domains', 'present_links', \n",
    "    op_categorify_multi = Categorify(\n",
    "        ['present_domains', 'present_links', 'hashtags'], gen_dicts=gen_dicts, doSplit=True, keepMostFrequent=True)\n",
    "    op_fillna_for_categorified = FillNA(['present_domains', 'present_links', 'hashtags'], -1)\n",
    "\n",
    "    # transform\n",
    "    proc.append_ops([op_fillna_str, op_categorify_multi, op_fillna_for_categorified])\n",
    "    t1 = timer()\n",
    "    df = proc.transform(df, name=output_name)\n",
    "    t2 = timer()\n",
    "    print(\"Data Process 1 and udf categorify took %.3f\" % (t2 - t1))\n",
    "\n",
    "    return df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_prefix = \"hdfs://\"\n",
    "current_path = \"/recsys2020_0608_categorify_example_1/\"\n",
    "original_folder = \"/recsys2021_0608/\"\n",
    "dicts_folder = \"recsys_dicts/\"\n",
    "recsysSchema = RecsysSchema()\n",
    "\n",
    "scala_udf_jars = \"/mnt/nvme2/chendi/BlueWhale/recdp/ScalaProcessUtils/target/recdp-scala-extensions-0.1.0-jar-with-dependencies.jar\"\n",
    "\n",
    "##### 1. Start spark and initialize data processor #####\n",
    "t0 = timer()\n",
    "spark = SparkSession.builder.master('yarn')\\\n",
    "    .appName(\"Recsys2020_data_process\")\\\n",
    "    .config(\"spark.executor.memory\", \"30g\")\\\n",
    "    .config(\"spark.executor.memoryOverhead\", \"5g\")\\\n",
    "    .config(\"spark.driver.extraClassPath\", f\"{scala_udf_jars}\")\\\n",
    "    .config(\"spark.executor.extraClassPath\", f\"{scala_udf_jars}\")\\\n",
    "    .config(\"spark.executor.cores\", \"4\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "schema = recsysSchema.toStructType()\n",
    "\n",
    "# 1.1 prepare dataFrames\n",
    "# 1.2 create RecDP DataProcessor\n",
    "proc = DataProcessor(spark, path_prefix,\n",
    "                     current_path=current_path, dicts_path=dicts_folder, shuffle_disk_capacity=\"1200GB\")\n",
    "\n",
    "# ===============================================\n",
    "# basic: Do categorify for all columns\n",
    "df = spark.read.parquet(path_prefix + original_folder)\n",
    "\n",
    "# rename firstly\n",
    "df = df.withColumnRenamed('enaging_user_following_count', 'engaging_user_following_count')\n",
    "df = df.withColumnRenamed('enaging_user_is_verified', 'engaging_user_is_verified')\n",
    "df = categorifyAllFeatures(df, proc, gen_dicts=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* per core memory size is 7.500 GB and shuffle_disk maximum capacity is 1200.000 GB\n",
    "* hashtags has numRows as 6541935\n",
    "* language has numRows as 66\n",
    "* present_domains has numRows as 896783\n",
    "* present_links has numRows as 14804300\n",
    "* tweet_id has numRows as 279597527\n",
    "* user_id has numRows as 42850559\n",
    "* present_media has numRows as 13\n",
    "* tweet_type has numRows as 3\n",
    "* categorify threshold is 50.000 M rows, flush_threshold is 960.000 GB\n",
    "* ('tweet_type', DataFrame[dict_col: string, dict_col_id: bigint], 3)\n",
    "* ('present_media', DataFrame[dict_col: string, dict_col_id: bigint], 13)\n",
    "* ('language', DataFrame[dict_col: string, dict_col_id: int], 66)\n",
    "* ('present_domains', DataFrame[dict_col: string, dict_col_id: int], 896783)\n",
    "* ('hashtags', DataFrame[dict_col: string, dict_col_id: int], 6541935)\n",
    "* ('present_links', DataFrame[dict_col: string, dict_col_id: int], 14804300)\n",
    "* ('user_id', DataFrame[dict_col: string, dict_col_id: int], 42850559)\n",
    "* ('tweet_id', DataFrame[dict_col: string, dict_col_id: int], 279597527)\n",
    "* tweet_type will do udf\n",
    "* present_media will do udf\n",
    "* language will do udf\n",
    "* present_domains will do udf\n",
    "* hashtags will do udf\n",
    "* present_links will do udf\n",
    "* etstimated_to_shuffle_size for user_id is 81.653 GB, will do shj\n",
    "* etstimated_to_shuffle_size for tweet_id is 78.153 GB, will do shj\n",
    "* Data Process 1 and udf categorify took 799.987\n",
    "\n",
    "### After switching to use scala udf, process time will be longer while we don't need to assign bigger memoryOverHead to spark\n",
    "\n",
    "* recdp-scala-extension is enabled\n",
    "* per core memory size is 4.500 GB and shuffle_disk maximum capacity is 1200.000 GB\n",
    "* hashtags has numRows as 6541935\n",
    "* present_domains has numRows as 896783\n",
    "* present_links has numRows as 14804300\n",
    "* present_media has numRows as 13\n",
    "* tweet_type has numRows as 3\n",
    "* bhj total threshold is 60.398 M rows, one bhj threshold is 30.000 M rows, flush_threshold is 960.000 GB\n",
    "* ('present_domains', DataFrame[dict_col: string, dict_col_id: int, count: bigint], 896783)\n",
    "* ('hashtags', DataFrame[dict_col: string, dict_col_id: int, count: bigint], 6541935)\n",
    "* ('present_links', DataFrame[dict_col: string, dict_col_id: int, count: bigint], 14804300)\n",
    "* present_domains will do udf\n",
    "* hashtags will do udf\n",
    "* present_links will do udf\n",
    "* Data Process 1 and udf categorify took 1045.646"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
