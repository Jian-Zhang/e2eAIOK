{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/nvme2/chendi/BlueWhale/frameworks.bigdata.bluewhale/RecDP\n"
     ]
    }
   ],
   "source": [
    "#!/env/bin/python\n",
    "\n",
    "import init\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql import *\n",
    "from pyspark import *\n",
    "import pyspark.sql.functions as f\n",
    "from timeit import default_timer as timer\n",
    "import logging\n",
    "from pyrecdp.data_processor import *\n",
    "from pyrecdp.utils import *\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "\n",
    "\n",
    "def decodeBertTokenizer(df, proc, output_name):\n",
    "    #from transformers import BertTokenizer\n",
    "    #tokenizer = BertTokenizer.from_pretrained(\n",
    "    #    'bert-base-multilingual-cased', do_lower_case=False)\n",
    "\n",
    "    # define UDF\n",
    "    def decode_and_clean_tweet_text(x):\n",
    "        # x = tokenizer.decode([int(n) for n in x.split('\\t')])\n",
    "        # x = x.replace('https : / / t. co / ', 'https://t.co/').replace('@ ', '@')\n",
    "        x = \"_\".join([n for n in x.split('\\t')])\n",
    "        return x\n",
    "        \n",
    "    @pandas_udf('string')\n",
    "    def tweet_tokens_decode_and_format(v):\n",
    "        v1s = []\n",
    "        for index, token in v.items():\n",
    "            v1s.append(decode_and_clean_tweet_text(token))\n",
    "        return pd.Series(v1s, dtype=str)\n",
    "    \n",
    "    # # define UDF\n",
    "    tokenizer_decode = f.udf(lambda x: \"_\".join([n for n in x.split('\\t')]))\n",
    "    \n",
    "    # define decode udf operations\n",
    "    op_feature_modification_tokenizer_decode = FeatureAdd(\n",
    "        cols={'tweet': 'text_tokens'}, udfImpl=tokenizer_decode)\n",
    "    \n",
    "    # define decode udf operations\n",
    "    #op_feature_modification_tokenizer_decode = FeatureAdd(\n",
    "    #    cols={'tweet': 'text_tokens'}, udfImpl=tweet_tokens_decode_and_format)\n",
    "\n",
    "    # execute\n",
    "    proc.reset_ops([op_feature_modification_tokenizer_decode])\n",
    "    t1 = timer()\n",
    "    df = proc.transform(df, name=output_name, df_cnt=626242930)\n",
    "    t2 = timer()\n",
    "    print(\"BertTokenizer decode took %.3f\" % (t2 - t1))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "per core memory size is 2.500 GB and shuffle_disk maximum capacity is 1200.000 GB\n",
      "BertTokenizer decode took 236.789\n"
     ]
    }
   ],
   "source": [
    "path_prefix = \"hdfs://\"\n",
    "current_path = \"/recsys2021_0608_udf_test/\"\n",
    "original_folder = \"/recsys2021_0608/\"\n",
    "\n",
    "native_sql_path = \"/mnt/nvme2/chendi/intel-bigdata/OAP/native-sql-engine/native-sql-engine/core/target/spark-columnar-core-1.2.0-snapshot-jar-with-dependencies.jar\"\n",
    "native_arrow_datasource_path = \"/mnt/nvme2/chendi/intel-bigdata/OAP/native-sql-engine/arrow-data-source/standard/target/spark-arrow-datasource-standard-1.2.0-snapshot-jar-with-dependencies.jar\"\n",
    "\n",
    "##### 1. Start spark and initialize data processor #####\n",
    "t0 = timer()\n",
    "spark = SparkSession.builder.master('yarn')\\\n",
    "    .appName(\"udf_column\")\\\n",
    "    .config(\"spark.sql.broadcastTimeout\", \"7200\")\\\n",
    "    .config(\"spark.cleaner.periodicGC.interval\", \"10min\")\\\n",
    "    .config(\"spark.executorEnv.HF_DATASETS_OFFLINE\", \"1\")\\\n",
    "    .config(\"spark.executorEnv.TRANSFORMERS_OFFLINE\", \"1\")\\\n",
    "    .config(\"spark.executorEnv.LD_LIBRARY_PATH\", \"/usr/local/lib64/\")\\\n",
    "    .config(\"spark.driver.extraClassPath\", \n",
    "            f\"{native_sql_path}:{native_arrow_datasource_path}\")\\\n",
    "    .config(\"spark.executor.extraClassPath\",\n",
    "            f\"{native_sql_path}:{native_arrow_datasource_path}\")\\\n",
    "    .config(\"spark.sql.extensions\", \"com.intel.oap.ColumnarPlugin\")\\\n",
    "    .config(\"spark.shuffle.manager\", \"org.apache.spark.shuffle.sort.ColumnarShuffleManager\")\\\n",
    "    .config(\"spark.executor.memory\", \"10g\")\\\n",
    "    .config(\"spark.executor.memoryOverhead\", \"16g\")\\\n",
    "    .config(\"spark.memory.offHeap.use\", \"true\")\\\n",
    "    .config(\"spark.memory.offHeap.size\", \"12G\")\\\n",
    "    .config(\"spark.executor.extraJavaOptions\", \"-XX:MaxDirectMemorySize=25G\")\\\n",
    "    .config(\"spark.oap.sql.columnar.arrowudf\", \"false\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "# 1.1 prepare dataFrames\n",
    "# 1.2 create RecDP DataProcessor\n",
    "proc = DataProcessor(spark, path_prefix,\n",
    "                     current_path=current_path, shuffle_disk_capacity=\"1200GB\")\n",
    "df = spark.read.format(\"arrow\").load(path_prefix + original_folder)\n",
    "df = df.select(\"text_tokens\")\n",
    "\n",
    "# ===============================================\n",
    "# decode tweet_tokens\n",
    "df = decodeBertTokenizer(df, proc, output_name=\"decoded_with_extracted_features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "per core memory size is 2.500 GB and shuffle_disk maximum capacity is 1200.000 GB\n",
      "BertTokenizer decode took 276.205\n"
     ]
    }
   ],
   "source": [
    "path_prefix = \"hdfs://\"\n",
    "current_path = \"/recsys2021_0608_udf_test/\"\n",
    "original_folder = \"/recsys2021_0608/\"\n",
    "\n",
    "native_sql_path = \"/mnt/nvme2/chendi/intel-bigdata/OAP/native-sql-engine/native-sql-engine/core/target/spark-columnar-core-1.2.0-snapshot-jar-with-dependencies.jar\"\n",
    "native_arrow_datasource_path = \"/mnt/nvme2/chendi/intel-bigdata/OAP/native-sql-engine/arrow-data-source/standard/target/spark-arrow-datasource-standard-1.2.0-snapshot-jar-with-dependencies.jar\"\n",
    "\n",
    "##### 1. Start spark and initialize data processor #####\n",
    "t0 = timer()\n",
    "spark = SparkSession.builder.master('yarn')\\\n",
    "    .appName(\"udf_row\")\\\n",
    "    .config(\"spark.sql.broadcastTimeout\", \"7200\")\\\n",
    "    .config(\"spark.cleaner.periodicGC.interval\", \"10min\")\\\n",
    "    .config(\"spark.executorEnv.HF_DATASETS_OFFLINE\", \"1\")\\\n",
    "    .config(\"spark.executorEnv.TRANSFORMERS_OFFLINE\", \"1\")\\\n",
    "    .config(\"spark.executor.memory\", \"10g\")\\\n",
    "    .config(\"spark.executor.memoryOverhead\", \"16g\")\\\n",
    "    .config(\"spark.memory.offHeap.use\", \"true\")\\\n",
    "    .config(\"spark.memory.offHeap.size\", \"12G\")\\\n",
    "    .config(\"spark.executor.extraJavaOptions\", \"-XX:MaxDirectMemorySize=25G\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "# 1.1 prepare dataFrames\n",
    "# 1.2 create RecDP DataProcessor\n",
    "proc = DataProcessor(spark, path_prefix,\n",
    "                     current_path=current_path, shuffle_disk_capacity=\"1200GB\")\n",
    "df = spark.read.parquet(path_prefix + original_folder)\n",
    "df = df.select(\"text_tokens\")\n",
    "\n",
    "# ===============================================\n",
    "# decode tweet_tokens\n",
    "df = decodeBertTokenizer(df, proc, output_name=\"decoded_with_extracted_features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "======\n",
    "#### pandas udf\n",
    "* per core memory size is 2.500 GB and shuffle_disk maximum capacity is 1200.000 GB\n",
    "* BertTokenizer decode took 237.324\n",
    "\n",
    "======\n",
    "#### python udf\n",
    "* per core memory size is 2.500 GB and shuffle_disk maximum capacity is 1200.000 GB\n",
    "* BertTokenizer decode took 276.205"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|         text_tokens|               tweet|\n",
      "+--------------------+--------------------+\n",
      "|101\t56898\t137\t144...|101_56898_137_144...|\n",
      "|101\t108\t139\t11403...|101_108_139_11403...|\n",
      "|101\t17160\t55112\t1...|101_17160_55112_1...|\n",
      "|101\t17116\t15045\t1...|101_17116_15045_1...|\n",
      "|101\t56898\t137\t653...|101_56898_137_653...|\n",
      "|101\t56898\t137\t552...|101_56898_137_552...|\n",
      "|101\t164\t108\t9519\t...|101_164_108_9519_...|\n",
      "|101\t30407\t10113\t1...|101_30407_10113_1...|\n",
      "|101\t29869\t86598\t1...|101_29869_86598_1...|\n",
      "|101\t787\t33691\t201...|101_787_33691_201...|\n",
      "|101\t16437\t29846\t1...|101_16437_29846_1...|\n",
      "|101\t14516\t59148\t1...|101_14516_59148_1...|\n",
      "|101\t142\t183\t10804...|101_142_183_10804...|\n",
      "|101\t1097\t20119\t15...|101_1097_20119_15...|\n",
      "|101\t73293\t10918\t7...|101_73293_10918_7...|\n",
      "|101\t56898\t137\t300...|101_56898_137_300...|\n",
      "|101\t56898\t137\t337...|101_56898_137_337...|\n",
      "|101\t10092\t1963\t60...|101_10092_1963_60...|\n",
      "|101\t56898\t137\t101...|101_56898_137_101...|\n",
      "|101\t43527\t15211\t1...|101_43527_15211_1...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
