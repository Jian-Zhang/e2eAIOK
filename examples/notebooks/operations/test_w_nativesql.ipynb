{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| \t                         | plus_one\t        | replace_string\t| stringIndex(66)\t |stringIndex(6,322,546)|\n",
    "| :------------------------- | ---------------- | ----------------- | ------------------ | -------------------- |\n",
    "|python_udf\t                 | 59.168\t        | 232.173\t        |67.364\t             |120 (47.460 + 72.965) |\n",
    "|pandas_udf\t                 | 25\t            | 190.982\t        |31.426\t             |87 (40.811 + 47.776)  |\n",
    "|columnar_pandas_udf         | 24.8\t            | 139.316\t        |22.892\t             |97 (48.254 + 49.678)  |\n",
    "|scala_udf\t                 | 7.514\t        | 187.557\t        | \t                 |                      |\n",
    "|scala_dataframe\t         | 6.951\t        | 191.276\t        |11.748\t             |139.288               |\n",
    "|spark_inline\t             | 6.397\t        | 273.398\t \t    |                    |                      |\n",
    "|bhj\t \t \t             | 10.926\t        | 25.935            |                    |                      |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| \t                   | plus_one\t        | replace_string\t| stringIndex(66)\t |stringIndex(6,322,546)|\n",
    "| :------------------- | ------------------ | ----------------- | ------------------ | -------------------- |\n",
    "|python_udf\t           |[0152_0](http://sr602:18080/history/application_1626660109287_0152/SQL/execution/?id=0)\t            |[0152_10](http://sr602:18080/history/application_1626660109287_0152/SQL/execution/?id=10)\t        |[0152_24](http://sr602:18080/history/application_1626660109287_0152/SQL/execution/?id=24)\t         |[0152_36](http://sr602:18080/history/application_1626660109287_0152/SQL/execution/?id=36)               |\n",
    "|pandas_udf\t           |[0152_2](http://sr602:18080/history/application_1626660109287_0152/SQL/execution/?id=2)\t            |[0152_12](http://sr602:18080/history/application_1626660109287_0152/SQL/execution/?id=12)\t        |[0152_27](http://sr602:18080/history/application_1626660109287_0152/SQL/execution/?id=27)\t         |[0152_39](http://sr602:18080/history/application_1626660109287_0152/SQL/execution/?id=39)               |\n",
    "|columnar_pandas_udf   |[0155_0](http://sr602:18080/history/application_1626660109287_0155/SQL/execution/?id=0)\t            |[0155_2](http://sr602:18080/history/application_1626660109287_0155/SQL/execution/?id=2)\t            |[0155_5](http://sr602:18080/history/application_1626660109287_0155/SQL/execution/?id=5)\t             |[0155_8](http://sr602:18080/history/application_1626660109287_0155/SQL/execution/?id=8)                |\n",
    "|scala_udf\t           |[0152_4](http://sr602:18080/history/application_1626660109287_0152/SQL/execution/?id=4)\t            |[0152_14](http://sr602:18080/history/application_1626660109287_0152/SQL/execution/?id=14)\t        | \t                 |                      |\n",
    "|scala_dataframe\t   |[0152_6](http://sr602:18080/history/application_1626660109287_0152/SQL/execution/?id=6)\t            |[0152_16](http://sr602:18080/history/application_1626660109287_0152/SQL/execution/?id=16)\t        |[0152_30](http://sr602:18080/history/application_1626660109287_0152/SQL/execution/?id=30)\t         |[0152_42](http://sr602:18080/history/application_1626660109287_0152/SQL/execution/?id=42)               |\n",
    "|spark_inline\t       |[0152_8](http://sr602:18080/history/application_1626660109287_0152/SQL/execution/?id=8)\t            |[0152_18](http://sr602:18080/history/application_1626660109287_0152/SQL/execution/?id=18)\t        | \t                 |                      |\n",
    "|bhj\t \t \t         |  [0152_21](http://sr602:18080/history/application_1626660109287_0152/SQL/execution/?id=21)\t            |[0152_33](http://sr602:18080/history/application_1626660109287_0152/SQL/execution/?id=33)            |                    |                      |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/nvme2/chendi/BlueWhale/recdp\n"
     ]
    }
   ],
   "source": [
    "#!/env/bin/python\n",
    "\n",
    "import init\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql import *\n",
    "from pyspark import *\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.types import *\n",
    "from timeit import default_timer as timer\n",
    "import logging\n",
    "from pyrecdp.data_processor import *\n",
    "from pyrecdp.utils import *\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "\n",
    "\n",
    "def intUDFTest(df, proc, output_name, method=\"pandas_udf\"):\n",
    "    ops = []\n",
    "    def python_plus_one(x):\n",
    "        return x + 1\n",
    "    \n",
    "    if method == 'python_udf':\n",
    "        ops.append(FeatureAdd(\n",
    "            cols={'tweet': 'a'}, udfImpl=f.udf(python_plus_one)))\n",
    "        \n",
    "    elif method == 'pandas_udf':\n",
    "        @pandas_udf('int')\n",
    "        def pd_plus_one(v):\n",
    "            v1s = []\n",
    "            for index, token in v.items():\n",
    "                v1s.append(python_plus_one(token))\n",
    "            return pd.Series(v1s, dtype=int)\n",
    "        ops.append(FeatureAdd(\n",
    "            cols={'tweet': 'a'}, udfImpl=pd_plus_one))\n",
    "        \n",
    "    elif method == 'scala_udf':\n",
    "        proc.spark.udf.registerJavaFunction(\"scala_plus_one\",\"com.intel.recdp.ScalaUDFTestInt\")\n",
    "        ops.append(FeatureAdd(\n",
    "            cols={'tweet': \"f.expr('scala_plus_one(a)')\"}, op='inline'))\n",
    "        \n",
    "    elif method == 'scala_df':\n",
    "        ops.append(FeatureAdd(\n",
    "            cols={'tweet': \"f.col('a')\"}, op='inline'))\n",
    "        ops.append(ScalaDFTest(cols=['tweet'], method='int'))       \n",
    "        \n",
    "    elif method == 'spark_inline':\n",
    "        ops.append(FeatureAdd(\n",
    "            cols={'tweet': \"f.col('a') + f.lit(1)\"}, op='inline'))\n",
    "    \n",
    "    ops.append(DropFeature(['a']))\n",
    "\n",
    "    # execute\n",
    "    proc.reset_ops(ops)\n",
    "    t1 = timer()\n",
    "    df = proc.transform(df, name=output_name, df_cnt=626242930)\n",
    "    t2 = timer()\n",
    "    print(f\"intUDFTest with [{method}] took %.3f secs\" % (t2 - t1))\n",
    "\n",
    "    return df\n",
    "\n",
    "def stringUDFTest(df, proc, output_name, method=\"pandas_udf\"):\n",
    "    ops = []\n",
    "    def python_replace_str(x):\n",
    "        return \"_\".join([n for n in x.split('\\t')])\n",
    "    \n",
    "    if method == 'python_udf':\n",
    "        ops.append(FeatureAdd(\n",
    "            cols={'tweet': 'a'}, udfImpl=f.udf(python_replace_str)))\n",
    "        \n",
    "    elif method == 'pandas_udf':\n",
    "        @pandas_udf('string')\n",
    "        def pd_replace_str(v):\n",
    "            v1s = []\n",
    "            for index, token in v.items():\n",
    "                v1s.append(python_replace_str(token))\n",
    "            return pd.Series(v1s, dtype=str)\n",
    "        ops.append(FeatureAdd(\n",
    "            cols={'tweet': 'a'}, udfImpl=pd_replace_str))\n",
    "        \n",
    "    elif method == 'scala_udf':\n",
    "        proc.spark.udf.registerJavaFunction(\"scala_replace_str\",\"com.intel.recdp.ScalaUDFTestStr\")\n",
    "        ops.append(FeatureAdd(\n",
    "            cols={'tweet': \"f.expr('scala_replace_str(a)')\"}, op='inline'))\n",
    "        \n",
    "    elif method == 'scala_df':\n",
    "        ops.append(FeatureAdd(\n",
    "            cols={'tweet': \"f.col('a')\"}, op='inline'))\n",
    "        ops.append(ScalaDFTest(cols=['tweet'], method='str'))       \n",
    "        \n",
    "    elif method == 'spark_inline':\n",
    "        ops.append(FeatureAdd(\n",
    "            cols={'tweet': \"f.concat_ws('_', f.split(f.col('a'), '\\t'))\"}, op='inline'))\n",
    "    \n",
    "    ops.append(DropFeature(['a']))\n",
    "\n",
    "    # execute\n",
    "    proc.reset_ops(ops)\n",
    "    t1 = timer()\n",
    "    df = proc.transform(df, name=output_name, df_cnt=626242930)\n",
    "    t2 = timer()\n",
    "    print(f\"stringUDFTest with [{method}] took %.3f secs\" % (t2 - t1))\n",
    "\n",
    "    return df\n",
    "\n",
    "def broadcastUDFTest(df, proc, output_name, dict_df, method=\"pandas_udf\"):\n",
    "    ops = []\n",
    "    if method == 'broadcast':\n",
    "        ops.append(FeatureAdd(cols={'tweet': \"f.col('a')\"}, op='inline'))\n",
    "        ops.append(Categorify(['tweet'], dict_dfs = [{'col_name': 'tweet', 'dict': dict_df}]))\n",
    "\n",
    "    elif method == 'python_udf':\n",
    "        t1 = timer()\n",
    "        broadcast_handler = proc.spark.sparkContext.broadcast(dict((row['dict_col'], row['dict_col_id']) for row in dict_df.collect()))\n",
    "        t2 = timer()\n",
    "        print(f\"broadcast for [{method}] took %.3f secs\" % (t2 - t1))\n",
    "        def python_replace_str(x):\n",
    "            m = broadcast_handler.value\n",
    "            if x in m:\n",
    "                return m[x]\n",
    "            else:\n",
    "                return -1\n",
    "        ops.append(FeatureAdd(\n",
    "            cols={'tweet': 'a'}, udfImpl=f.udf(python_replace_str)))\n",
    "        \n",
    "    elif method == 'pandas_udf':\n",
    "        t1 = timer()\n",
    "        broadcast_handler = proc.spark.sparkContext.broadcast(dict((row['dict_col'], row['dict_col_id']) for row in dict_df.collect()))\n",
    "        t2 = timer()\n",
    "        print(f\"broadcast for [{method}] took %.3f secs\" % (t2 - t1))\n",
    "        def python_replace_str(x):\n",
    "            m = broadcast_handler.value\n",
    "            if x in m:\n",
    "                return m[x]\n",
    "            else:\n",
    "                return -1\n",
    "        @pandas_udf('int')\n",
    "        def pd_replace_str(v):\n",
    "            v1s = []\n",
    "            for index, token in v.items():\n",
    "                v1s.append(python_replace_str(token))\n",
    "            return pd.Series(v1s, dtype=int)\n",
    "        ops.append(FeatureAdd(\n",
    "            cols={'tweet': 'a'}, udfImpl=pd_replace_str))\n",
    "        \n",
    "    elif method == 'scala_df':\n",
    "        ops.append(FeatureAdd(\n",
    "            cols={'tweet': \"f.col('a')\"}, op='inline'))\n",
    "        ops.append(ScalaDFTest(cols=['tweet'], dicts=[{'col_name': 'a', 'dict': dict_df}], method='broadcast'))       \n",
    "    \n",
    "    ops.append(DropFeature(['a']))\n",
    "\n",
    "    # execute\n",
    "    proc.reset_ops(ops)\n",
    "    t1 = timer()\n",
    "    df = proc.transform(df, name=output_name, df_cnt=626242930)\n",
    "    t2 = timer()\n",
    "    print(f\"broadcastUDFTest with [{method}] took %.3f secs\" % (t2 - t1))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recdp-scala-extension is enabled\n",
      "per core memory size is 2.500 GB and shuffle_disk maximum capacity is 1200.000 GB\n",
      "intUDFTest with [python_udf] took 59.168 secs\n",
      "+-----+\n",
      "|tweet|\n",
      "+-----+\n",
      "|  986|\n",
      "+-----+\n",
      "only showing top 1 row\n",
      "\n",
      "intUDFTest with [pandas_udf] took 25.000 secs\n",
      "+-----+\n",
      "|tweet|\n",
      "+-----+\n",
      "|32437|\n",
      "+-----+\n",
      "only showing top 1 row\n",
      "\n",
      "intUDFTest with [scala_udf] took 7.514 secs\n",
      "+-----+\n",
      "|tweet|\n",
      "+-----+\n",
      "|32437|\n",
      "+-----+\n",
      "only showing top 1 row\n",
      "\n",
      "intUDFTest with [scala_df] took 6.951 secs\n",
      "+-----+\n",
      "|tweet|\n",
      "+-----+\n",
      "|32437|\n",
      "+-----+\n",
      "only showing top 1 row\n",
      "\n",
      "intUDFTest with [spark_inline] took 6.397 secs\n",
      "+-----+\n",
      "|tweet|\n",
      "+-----+\n",
      "|32437|\n",
      "+-----+\n",
      "only showing top 1 row\n",
      "\n",
      "stringUDFTest with [python_udf] took 232.173 secs\n",
      "+--------------------+\n",
      "|               tweet|\n",
      "+--------------------+\n",
      "|101_56898_137_144...|\n",
      "+--------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "stringUDFTest with [pandas_udf] took 190.982 secs\n",
      "+--------------------+\n",
      "|               tweet|\n",
      "+--------------------+\n",
      "|101_56898_137_144...|\n",
      "+--------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "stringUDFTest with [scala_udf] took 187.557 secs\n",
      "+--------------------+\n",
      "|               tweet|\n",
      "+--------------------+\n",
      "|101_56898_137_144...|\n",
      "+--------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "stringUDFTest with [scala_df] took 191.276 secs\n",
      "+--------------------+\n",
      "|               tweet|\n",
      "+--------------------+\n",
      "|101_56898_137_144...|\n",
      "+--------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "stringUDFTest with [spark_inline] took 273.398 secs\n",
      "+--------------------+\n",
      "|               tweet|\n",
      "+--------------------+\n",
      "|101_56898_137_144...|\n",
      "+--------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "do bhj to tweet\n",
      "Adding a CodegenSeparator to pure BHJ WSCG case\n",
      "broadcastUDFTest with [broadcast] took 10.926 secs\n",
      "+-----+\n",
      "|tweet|\n",
      "+-----+\n",
      "|    4|\n",
      "+-----+\n",
      "only showing top 1 row\n",
      "\n",
      "broadcast for [python_udf] took 0.244 secs\n",
      "broadcastUDFTest with [python_udf] took 67.364 secs\n",
      "+-----+\n",
      "|tweet|\n",
      "+-----+\n",
      "|    8|\n",
      "+-----+\n",
      "only showing top 1 row\n",
      "\n",
      "broadcast for [pandas_udf] took 0.104 secs\n",
      "broadcastUDFTest with [pandas_udf] took 31.426 secs\n",
      "+-----+\n",
      "|tweet|\n",
      "+-----+\n",
      "|    4|\n",
      "+-----+\n",
      "only showing top 1 row\n",
      "\n",
      "broadcastUDFTest with [scala_df] took 11.748 secs\n",
      "+-----+\n",
      "|tweet|\n",
      "+-----+\n",
      "|    4|\n",
      "+-----+\n",
      "only showing top 1 row\n",
      "\n",
      "do bhj to tweet\n",
      "Adding a CodegenSeparator to pure BHJ WSCG case\n",
      "broadcastUDFTest with [broadcast] took 25.935 secs\n",
      "+-----+\n",
      "|tweet|\n",
      "+-----+\n",
      "| 1783|\n",
      "+-----+\n",
      "only showing top 1 row\n",
      "\n",
      "broadcast for [python_udf] took 47.460 secs\n",
      "broadcastUDFTest with [python_udf] took 72.965 secs\n",
      "+-----+\n",
      "|tweet|\n",
      "+-----+\n",
      "| 8930|\n",
      "+-----+\n",
      "only showing top 1 row\n",
      "\n",
      "broadcast for [pandas_udf] took 40.811 secs\n",
      "broadcastUDFTest with [pandas_udf] took 47.776 secs\n",
      "+-----+\n",
      "|tweet|\n",
      "+-----+\n",
      "| 8930|\n",
      "+-----+\n",
      "only showing top 1 row\n",
      "\n",
      "broadcastUDFTest with [scala_df] took 139.288 secs\n",
      "+-----+\n",
      "|tweet|\n",
      "+-----+\n",
      "| 1783|\n",
      "+-----+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "path_prefix = \"hdfs://\"\n",
    "current_path = \"/recsys2021_0608_udf_test/\"\n",
    "original_folder = \"/recsys2021_0608/\"\n",
    "\n",
    "native_sql_path = \"/mnt/nvme2/chendi/intel-bigdata/OAP/native-sql-engine/native-sql-engine/core/target/spark-columnar-core-1.2.0-snapshot-jar-with-dependencies.jar\"\n",
    "native_arrow_datasource_path = \"/mnt/nvme2/chendi/intel-bigdata/OAP/native-sql-engine/arrow-data-source/standard/target/spark-arrow-datasource-standard-1.2.0-snapshot-jar-with-dependencies.jar\"\n",
    "scala_udf_jars = \"/mnt/nvme2/chendi/BlueWhale/recdp/ScalaProcessUtils/target/recdp-scala-extensions-0.1.0-jar-with-dependencies.jar\"\n",
    "\n",
    "##### 1. Start spark and initialize data processor #####\n",
    "t0 = timer()\n",
    "\n",
    "#### vanilla spark ####\n",
    "spark = SparkSession.builder.master('yarn')\\\n",
    "    .appName(\"udf_vanilla\")\\\n",
    "    .config(\"spark.sql.broadcastTimeout\", \"7200\")\\\n",
    "    .config(\"spark.cleaner.periodicGC.interval\", \"10min\")\\\n",
    "    .config(\"spark.driver.extraClassPath\", \n",
    "            f\"{scala_udf_jars}\")\\\n",
    "    .config(\"spark.executor.extraClassPath\",\n",
    "            f\"{scala_udf_jars}\")\\\n",
    "    .config(\"spark.executor.memory\", \"10g\")\\\n",
    "    .config(\"spark.executor.memoryOverhead\", \"16g\")\\\n",
    "    .getOrCreate()\n",
    "proc = DataProcessor(spark, path_prefix,\n",
    "                     current_path=current_path, shuffle_disk_capacity=\"1200GB\", enable_gazelle=False)\n",
    "\n",
    "##################################################################\n",
    "#['python_udf', 'pandas_udf', 'scala_udf', 'scala_df', 'spark_inline']\n",
    "for method in ['python_udf', 'pandas_udf', 'scala_udf', 'scala_df', 'spark_inline']:\n",
    "    df = spark.read.format(\"parquet\").load(path_prefix + original_folder)\n",
    "    df = df.selectExpr(\"engaged_with_user_follower_count as a\").withColumn(\"a\", f.col(\"a\").cast(IntegerType()))\n",
    "    df = intUDFTest(df, proc, output_name=\"decoded_with_extracted_features\", method=method)\n",
    "    df.filter(\"tweet is not null\").show(1)\n",
    "for method in ['python_udf', 'pandas_udf', 'scala_udf', 'scala_df', 'spark_inline']:\n",
    "    df = spark.read.format(\"parquet\").load(path_prefix + original_folder)\n",
    "    df = df.selectExpr(\"text_tokens as a\")\n",
    "    df = stringUDFTest(df, proc, output_name=\"decoded_with_extracted_features\", method=method)\n",
    "    df.filter(\"tweet is not null\").show(1)\n",
    "for method in ['broadcast', 'python_udf', 'pandas_udf', 'scala_df']:\n",
    "    df = spark.read.format(\"parquet\").load(path_prefix + original_folder)\n",
    "    dict_df = spark.read.format(\"parquet\").load(\"/recsys2021_0608_processed/recsys_dicts/language\")\n",
    "    df = df.selectExpr(\"language as a\")\n",
    "    df = broadcastUDFTest(df, proc, output_name=\"decoded_with_extracted_features\", dict_df = dict_df, method=method)\n",
    "    df.filter(\"tweet is not null\").filter(\"tweet != -1\").show(1)\n",
    "for method in ['broadcast', 'python_udf', 'pandas_udf', 'scala_df']:\n",
    "    df = spark.read.format(\"parquet\").load(path_prefix + original_folder)\n",
    "    dict_df = spark.read.format(\"parquet\").load(\"/recsys2021_0608_processed/recsys_dicts/hashtags\")\n",
    "    df = df.withColumn(\"a\", f.split(f.col('hashtags'), '\\t').getItem(0)).select(\"a\")\n",
    "    df = df.repartition(200)\n",
    "    df = broadcastUDFTest(df, proc, output_name=\"decoded_with_extracted_features\", dict_df = dict_df, method=method)\n",
    "    df.filter(\"tweet is not null\").filter(\"tweet != -1\").show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recdp-scala-extension is enabled\n",
      "per core memory size is 2.500 GB and shuffle_disk maximum capacity is 1200.000 GB\n",
      "intUDFTest with [pandas_udf] took 24.842 secs\n",
      "+-----+\n",
      "|tweet|\n",
      "+-----+\n",
      "|32437|\n",
      "+-----+\n",
      "only showing top 1 row\n",
      "\n",
      "stringUDFTest with [pandas_udf] took 139.316 secs\n",
      "+--------------------+\n",
      "|               tweet|\n",
      "+--------------------+\n",
      "|101_56898_137_144...|\n",
      "+--------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "broadcast for [pandas_udf] took 0.527 secs\n",
      "broadcastUDFTest with [pandas_udf] took 22.892 secs\n",
      "+-----+\n",
      "|tweet|\n",
      "+-----+\n",
      "|   18|\n",
      "+-----+\n",
      "only showing top 1 row\n",
      "\n",
      "broadcast for [pandas_udf] took 48.254 secs\n",
      "broadcastUDFTest with [pandas_udf] took 49.678 secs\n",
      "+-----+\n",
      "|tweet|\n",
      "+-----+\n",
      "| 1341|\n",
      "+-----+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "path_prefix = \"hdfs://\"\n",
    "current_path = \"/recsys2021_0608_udf_test/\"\n",
    "original_folder = \"/recsys2021_0608/\"\n",
    "\n",
    "native_sql_path = \"/mnt/nvme2/chendi/intel-bigdata/OAP/native-sql-engine/native-sql-engine/core/target/spark-columnar-core-1.2.0-snapshot-jar-with-dependencies.jar\"\n",
    "native_arrow_datasource_path = \"/mnt/nvme2/chendi/intel-bigdata/OAP/native-sql-engine/arrow-data-source/standard/target/spark-arrow-datasource-standard-1.2.0-snapshot-jar-with-dependencies.jar\"\n",
    "scala_udf_jars = \"/mnt/nvme2/chendi/BlueWhale/recdp/ScalaProcessUtils/target/recdp-scala-extensions-0.1.0-jar-with-dependencies.jar\"\n",
    "\n",
    "##### 1. Start spark and initialize data processor #####\n",
    "t0 = timer()\n",
    "#### nativesql ####\n",
    "spark = SparkSession.builder.master('yarn')\\\n",
    "    .appName(\"udf_native-sql\")\\\n",
    "    .config(\"spark.sql.broadcastTimeout\", \"7200\")\\\n",
    "    .config(\"spark.cleaner.periodicGC.interval\", \"10min\")\\\n",
    "    .config(\"spark.executorEnv.LD_LIBRARY_PATH\", \"/usr/local/lib64/\")\\\n",
    "    .config(\"spark.driver.extraClassPath\", \n",
    "            f\"{native_sql_path}:{native_arrow_datasource_path}:{scala_udf_jars}\")\\\n",
    "    .config(\"spark.executor.extraClassPath\",\n",
    "            f\"{native_sql_path}:{native_arrow_datasource_path}:{scala_udf_jars}\")\\\n",
    "    .config(\"spark.sql.extensions\", \"com.intel.oap.ColumnarPlugin, com.intel.oap.spark.sql.ArrowWriteExtension\")\\\n",
    "    .config(\"spark.shuffle.manager\", \"org.apache.spark.shuffle.sort.ColumnarShuffleManager\")\\\n",
    "    .config(\"spark.executor.memory\", \"10g\")\\\n",
    "    .config(\"spark.executor.memoryOverhead\", \"16g\")\\\n",
    "    .config(\"spark.memory.offHeap.use\", \"true\")\\\n",
    "    .config(\"spark.memory.offHeap.size\", \"12G\")\\\n",
    "    .config(\"spark.executor.extraJavaOptions\", \"-XX:MaxDirectMemorySize=25G\")\\\n",
    "    .config(\"spark.oap.sql.columnar.arrowudf\", \"true\")\\\n",
    "    .getOrCreate()\n",
    "proc = DataProcessor(spark, path_prefix,\n",
    "                    current_path=current_path, shuffle_disk_capacity=\"1200GB\", enable_gazelle=True)\n",
    "\n",
    "##################################################################\n",
    "#['python_udf', 'pandas_udf', 'scala_udf', 'scala_df', 'spark_inline']\n",
    "for method in ['pandas_udf']:\n",
    "    df = spark.read.format(\"arrow\").load(path_prefix + original_folder)\n",
    "    df = df.selectExpr(\"engaged_with_user_follower_count as a\").withColumn(\"a\", f.col(\"a\").cast(IntegerType()))\n",
    "    df = intUDFTest(df, proc, output_name=\"decoded_with_extracted_features\", method=method)\n",
    "    df.filter(\"tweet is not null\").show(1)\n",
    "for method in ['pandas_udf']:\n",
    "    df = spark.read.format(\"arrow\").load(path_prefix + original_folder)\n",
    "    df = df.selectExpr(\"text_tokens as a\")\n",
    "    df = stringUDFTest(df, proc, output_name=\"decoded_with_extracted_features\", method=method)\n",
    "    df.filter(\"tweet is not null\").show(1)\n",
    "for method in ['pandas_udf']:\n",
    "    df = spark.read.format(\"arrow\").load(path_prefix + original_folder)\n",
    "    dict_df = spark.read.format(\"arrow\").load(\"/recsys2021_0608_processed/recsys_dicts/language\")\n",
    "    df = df.selectExpr(\"language as a\")\n",
    "    df = broadcastUDFTest(df, proc, output_name=\"decoded_with_extracted_features\", dict_df = dict_df, method=method)\n",
    "    df.filter(\"tweet is not null\").filter(\"tweet != -1\").show(1)\n",
    "for method in ['pandas_udf']:\n",
    "    df = spark.read.format(\"arrow\").load(path_prefix + original_folder)\n",
    "    dict_df = spark.read.format(\"arrow\").load(\"/recsys2021_0608_processed/recsys_dicts/hashtags\")\n",
    "    df = df.withColumn(\"a\", f.split(f.col('hashtags'), '\\t').getItem(0)).select(\"a\")\n",
    "    df = df.repartition(200)\n",
    "    df = broadcastUDFTest(df, proc, output_name=\"decoded_with_extracted_features\", dict_df = dict_df, method=method)\n",
    "    df.filter(\"tweet is not null\").filter(\"tweet != -1\").show(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
