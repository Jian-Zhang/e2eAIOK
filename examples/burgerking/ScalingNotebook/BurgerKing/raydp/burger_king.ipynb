{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import databricks.koalas as ks\n",
    "\n",
    "import ray\n",
    "from ray.util.sgd.torch.torch_trainer import TorchTrainer\n",
    "\n",
    "import raydp.spark.context as context\n",
    "from raydp.spark.torch_sgd import TorchEstimator\n",
    "from raydp.spark.utils import random_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from typing import Dict\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add spark home into the env\n",
    "os.environ[\"SPARK_HOME\"] = \"/home/xianyang/sw/spark-3.0.0-preview2-bin-hadoop2.7\"\n",
    "\n",
    "GB = 1024 * 1024 * 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-05-11 16:57:38,286\tWARNING import_thread.py:136 -- The actor 'SparkWorkerService' has been exported 100 times. It's possible that this warning is accidental, but this may indicate that the same remote function is being defined repeatedly from within many tasks and exported to all of the workers. This can be a performance issue and can be resolved by defining the remote function on the driver instead. See https://github.com/ray-project/ray/issues/6240 for more discussion.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=142408)\u001b[0m starting org.apache.spark.deploy.master.Master, logging to /home/xianyang/sw/spark-3.0.0-preview2-bin-hadoop2.7/logs/spark-xianyang-org.apache.spark.deploy.master.Master-1-sr233.out\n",
      "\u001b[2m\u001b[36m(pid=144105)\u001b[0m starting org.apache.spark.deploy.worker.Worker, logging to /home/xianyang/sw/spark-3.0.0-preview2-bin-hadoop2.7/logs/spark-xianyang-org.apache.spark.deploy.worker.Worker-1-sr233.out\n",
      "\u001b[2m\u001b[36m(pid=99181, ip=10.0.0.134)\u001b[0m starting org.apache.spark.deploy.worker.Worker, logging to /home/xianyang/sw/spark-3.0.0-preview2-bin-hadoop2.7/logs/spark-xianyang-org.apache.spark.deploy.worker.Worker-1-sr234.out\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://sr233:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.0-preview2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://10.1.0.133:8080</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Burger King</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fbd8dfb12d0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# connect to ray cluster\n",
    "ray.init(address=\"sr233:6379\", node_ip_address=\"sr233\", redis_password=\"123\")\n",
    "\n",
    "# init spark context\n",
    "context.init_spark(app_name=\"Burger King\",\n",
    "                   num_executors=2,\n",
    "                   executor_cores=10,\n",
    "                   executor_memory=int(40 * GB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data processing with koalas\n",
    "data_path = \"hdfs://sr233:9000/data_10000\"\n",
    "\n",
    "df: ks.DataFrame = ks.read_json(data_path)\n",
    "train_df, test_df = random_split(df, [0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_plus = 522\n",
    "n_time = 167\n",
    "n_bkids = 126\n",
    "n_weather = 35\n",
    "n_feels = 20\n",
    "\n",
    "# Bidirectional recurrent neural network (many-to-one)\n",
    "class BiRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, fcn_input_size, fcn_output_size):\n",
    "        super(BiRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.embeds_pluids = nn.Embedding(n_plus, 50)\n",
    "        self.embeds_bkidx = nn.Embedding(n_bkids, 100)\n",
    "        self.embeds_timeidx = nn.Embedding(n_time, 100)\n",
    "        self.embeds_feelsBucket = nn.Embedding(n_feels, 100)\n",
    "        self.embeds_weather = nn.Embedding(n_weather, 100)\n",
    "        \n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
    "        \n",
    "        self.hidden1 = nn.Linear(100, 100)\n",
    "        self.hidden2 = nn.Linear(100, 1)\n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        self.drop_layer = nn.Dropout(p=0.3)\n",
    "        self.fc = nn.Linear(fcn_input_size, fcn_output_size)\n",
    "        \n",
    "    def forward(self, pluids, timeidx, bkidx, weatheridx, feelsBucket):\n",
    "        plu_embed = self.embeds_pluids(pluids)\n",
    "        bkidx_embed = self.embeds_bkidx(bkidx)\n",
    "        time_embed = self.embeds_timeidx(timeidx)\n",
    "        weather_embed = self.embeds_weather(weatheridx)\n",
    "        feels_embed = self.embeds_feelsBucket(feelsBucket)\n",
    "\n",
    "        x = plu_embed\n",
    "\n",
    "        h0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size) # 2 for bidirection \n",
    "        # Forward propagate gru\n",
    "        gru_out, _ = self.gru(x, h0)\n",
    "        ut = torch.tanh(self.hidden1(gru_out))\n",
    "        # et shape: [batch_size, seq_len, att_hops]\n",
    "        et = self.hidden2(ut)\n",
    "\n",
    "        # att shape: [batch_size,  att_hops, seq_len]\n",
    "        att = F.softmax(torch.transpose(et, 2, 1))\n",
    "        \n",
    "        # output shape [batch_size, att_hops, embedding_width]\n",
    "        output = torch.matmul(att, gru_out)\n",
    "        \n",
    "        # flatten the output\n",
    "        attention_output = self.flatten(output)\n",
    "        context_features = torch.mul(attention_output,(1 + bkidx_embed + time_embed + weather_embed + feels_embed))\n",
    "        ac1 = F.relu(context_features)\n",
    "        \n",
    "        dropout = self.drop_layer(ac1)\n",
    "        output = self.fc(dropout)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xianyang/sw/miniconda3/envs/test/lib/python3.7/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=99179, ip=10.0.0.134)\u001b[0m /opt/conda/conda-bld/pytorch_1587428190859/work/torch/csrc/utils/tensor_numpy.cpp:141: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n",
      "\u001b[2m\u001b[36m(pid=99179, ip=10.0.0.134)\u001b[0m /home/xianyang/sw/miniconda3/envs/test/lib/python3.7/site-packages/ray/workers/default_worker.py:46: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "\u001b[2m\u001b[36m(pid=99179, ip=10.0.0.134)\u001b[0m   parser.add_argument(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/conda-bld/pytorch_1587428190859/work/torch/csrc/utils/tensor_numpy.cpp:141: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n",
      "/home/xianyang/sw/miniconda3/envs/test/lib/python3.7/site-packages/ipykernel_launcher.py:46: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch-0: {'num_samples': 140068, 'epoch': 1, 'batch_count': 701, 'train_loss': 0.6943321002558821, 'last_train_loss': 0.6958527565002441}\n",
      "Epoch-1: {'num_samples': 140068, 'epoch': 2, 'batch_count': 701, 'train_loss': 0.6943508767096453, 'last_train_loss': 0.6848205924034119}\n",
      "Epoch-2: {'num_samples': 140068, 'epoch': 3, 'batch_count': 701, 'train_loss': 0.6931239345679874, 'last_train_loss': 0.6537535786628723}\n",
      "Epoch-3: {'num_samples': 140068, 'epoch': 4, 'batch_count': 701, 'train_loss': 0.6907080163565417, 'last_train_loss': 0.6253771781921387}\n",
      "Epoch-4: {'num_samples': 140068, 'epoch': 5, 'batch_count': 701, 'train_loss': 0.6880078527841406, 'last_train_loss': 0.5378180146217346}\n",
      "Epoch-5: {'num_samples': 140068, 'epoch': 6, 'batch_count': 701, 'train_loss': 0.6852638408370186, 'last_train_loss': 0.6295316219329834}\n",
      "Epoch-6: {'num_samples': 140068, 'epoch': 7, 'batch_count': 701, 'train_loss': 0.6817466152246421, 'last_train_loss': 0.50005704164505}\n",
      "Epoch-7: {'num_samples': 140068, 'epoch': 8, 'batch_count': 701, 'train_loss': 0.678258717654144, 'last_train_loss': 0.5078309774398804}\n",
      "Epoch-8: {'num_samples': 140068, 'epoch': 9, 'batch_count': 701, 'train_loss': 0.6751007195020091, 'last_train_loss': 0.3865250051021576}\n",
      "Epoch-9: {'num_samples': 140068, 'epoch': 10, 'batch_count': 701, 'train_loss': 0.6713362052825196, 'last_train_loss': 0.2845967710018158}\n"
     ]
    }
   ],
   "source": [
    "# train with SGD\n",
    "model = BiRNN(50, 50, 5, 100, 2)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "\n",
    "estimator = TorchEstimator(num_workers=2,\n",
    "                           model=model,\n",
    "                           optimizer=optimizer,\n",
    "                           loss=loss,\n",
    "                           feature_columns=[\"pluids\", \"timeidx\", \"bkidx\", \"weatheridx\", \"feelsBucket\"],\n",
    "                           feature_shapes=[5, 0, 0, 0, 0],\n",
    "                           feature_types=[torch.long, torch.long, torch.long, torch.long, torch.long],\n",
    "                           label_column=\"label\",\n",
    "                           label_type=torch.long,\n",
    "                           batch_size=100,\n",
    "                           num_epochs=10)\n",
    "\n",
    "estimator.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.evaluate(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(estimator.get_model())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.shutdown()\n",
    "context.stop_spark()\n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
