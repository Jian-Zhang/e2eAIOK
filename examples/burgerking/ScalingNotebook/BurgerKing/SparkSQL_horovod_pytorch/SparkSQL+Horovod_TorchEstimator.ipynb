{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark + Horovod for Burgerking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configargparse\n",
    "import argparse\n",
    "import os\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"4\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import logging\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import pyspark\n",
    "import pyspark.sql.types as T\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "import horovod.spark.torch as hvd\n",
    "from horovod.spark.common.store import HDFSStore\n",
    "from horovod.spark.common.backend import SparkBackend "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = configargparse.ArgParser(default_config_files=['../conf/burgerking.conf'])\n",
    "p.add_argument(\"--spark-master-address\", type=str)\n",
    "p.add_argument(\"--spark-cores-max\", type=int)\n",
    "p.add_argument(\"--spark-executor-cores\", type=int)\n",
    "p.add_argument(\"--spark-executor-memory\", type=str)\n",
    "p.add_argument(\"--spark-default-parallelism\", type=int)\n",
    "p.add_argument(\"--spark-data-dir\", type=str)\n",
    "p.add_argument(\"--hdfs-store\", type=str)\n",
    "p.add_argument(\"--nics\", type=str)\n",
    "options, _ = p.parse_known_args()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = pyspark.sql.SparkSession.builder.master(options.spark_master_address)\\\n",
    "                                    .config(\"spark.cores.max\", options.spark_cores_max) \\\n",
    "                                    .config(\"spark.executor.cores\", options.spark_executor_cores) \\\n",
    "                                    .config(\"spark.executor.memory\", options.spark_executor_memory) \\\n",
    "                                    .config(\"spark.default.parallelism\", options.spark_default_parallelism) \\\n",
    "                                    .config(\"spark.sql.execution.arrow.enabled\", \"true\") \\\n",
    "                                    .config(\"spark.executorEnv.PATH\", os.environ['PATH']) \\\n",
    "                                    .getOrCreate()   \n",
    "                                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# train data: 99891\n",
      "# test data:  109\n",
      "time:         5.80s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "df = spark.read.json(options.spark_data_dir)\n",
    "train_df, test_df = df.randomSplit([0.999, 0.001], seed=100)\n",
    "end = time.time()\n",
    "\n",
    "prepare_time = end - start\n",
    "print(f\"# train data: {train_df.count()}\")\n",
    "print(f\"# test data:  {test_df.count()}\")\n",
    "print(f\"time:         {prepare_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_check_url: hdfs://\n"
     ]
    }
   ],
   "source": [
    "store = HDFSStore(options.hdfs_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_plus = 522\n",
    "n_time = 167\n",
    "n_bkids = 126\n",
    "n_weather = 35\n",
    "n_feels = 20\n",
    "\n",
    "# Bidirectional recurrent neural network (many-to-one)\n",
    "class BiRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, fcn_input_size, fcn_output_size):\n",
    "        super(BiRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embeds_pluids = nn.Embedding(n_plus, 50, sparse=True)\n",
    "        self.embeds_bkidx = nn.Embedding(n_bkids, 100, sparse=True)\n",
    "        self.embeds_timeidx = nn.Embedding(n_time, 100, sparse=True)\n",
    "        self.embeds_feelsBucket = nn.Embedding(n_feels, 100, sparse=True)\n",
    "        self.embeds_weather = nn.Embedding(n_weather, 100, sparse=True)\n",
    "\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
    "\n",
    "        self.hidden1 = nn.Linear(100, 100)\n",
    "        self.hidden2 = nn.Linear(100, 1)\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        self.drop_layer = nn.Dropout(p=0.3)\n",
    "        self.fc = nn.Linear(fcn_input_size, fcn_output_size)\n",
    "\n",
    "\n",
    "    def forward(self, pluids, timeidx, bkidx, weatheridx, feelsBucket):\n",
    "\n",
    "        pluids = pluids.long()\n",
    "        timeidx = timeidx.long()\n",
    "        bkidx = bkidx.long()\n",
    "        weatheridx = weatheridx.long()\n",
    "        feelsBucket = feelsBucket.long()\n",
    "        plu_embed = self.embeds_pluids(pluids)\n",
    "        bkidx_embed = self.embeds_bkidx(bkidx)\n",
    "        time_embed = self.embeds_timeidx(timeidx)\n",
    "        weather_embed = self.embeds_weather(weatheridx)\n",
    "        feels_embed = self.embeds_feelsBucket(feelsBucket)\n",
    "\n",
    "        x = plu_embed\n",
    "\n",
    "        h0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size) # 2 for bidirection \n",
    "        \n",
    "        # Forward propagate gru\n",
    "        gru_out, _ = self.gru(x, h0)\n",
    "        ut = torch.tanh(self.hidden1(gru_out))\n",
    "        # et shape: [batch_size, seq_len, att_hops]\n",
    "        et = self.hidden2(ut)\n",
    "\n",
    "        # att shape: [batch_size,  att_hops, seq_len]\n",
    "        att = F.softmax(torch.transpose(et, 2, 1))\n",
    "\n",
    "        # output shape [batch_size, att_hops, embedding_width]\n",
    "        output = torch.matmul(att, gru_out)\n",
    "\n",
    "        # flatten the output\n",
    "        attention_output = self.flatten(output)\n",
    "        context_features = torch.mul(attention_output,(1 + bkidx_embed + time_embed + weather_embed + feels_embed))\n",
    "        ac1 = F.relu(context_features)\n",
    "\n",
    "        dropout = self.drop_layer(ac1)\n",
    "        output = self.fc(dropout)\n",
    "\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16000\n",
    "num_epoch = 5\n",
    "loss = nn.CrossEntropyLoss()\n",
    "num_proc=options.spark_cores_max // options.spark_executor_cores\n",
    "feature_cols = [\"pluids\", \"timeidx\", \"bkidx\", \"weatheridx\", \"feelsBucket\"]\n",
    "\n",
    "model = BiRNN(50, 50, 1, 100, 522)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "def f(data):\n",
    "    #avoid omp resource competition\n",
    "    os.environ[\"OMP_NUM_THREADS\"] = \"4\"\n",
    "    iter([os.environ[\"OMP_NUM_THREADS\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_partitions=20\n",
      "writing dataframes\n",
      "train_data_path=hdfs://sr257:9000/tmp/intermediate_train_data.0\n",
      "val_data_path=hdfs://sr257:9000/tmp/intermediate_val_data.0\n",
      "train_partitions=20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bluewhale/Bluewhale/tools/venv/envs/bluewhale/lib/python3.7/site-packages/horovod/spark/common/util.py:508: FutureWarning: The 'field_by_name' method is deprecated, use 'field' instead\n",
      "  metadata, avg_row_size = make_metadata_dictionary(train_data_schema)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_rows=99891\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[1,0]<stderr>:/home/bluewhale/Bluewhale/tools/venv/envs/bluewhale/lib/python3.7/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "[1,0]<stderr>:  return torch._C._cuda_getDeviceCount() > 0\n",
      "[1,1]<stderr>:/home/bluewhale/Bluewhale/tools/venv/envs/bluewhale/lib/python3.7/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "[1,1]<stderr>:  return torch._C._cuda_getDeviceCount() > 0\n",
      "[1,0]<stderr>:2020-12-18 09:50:13.947496: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/bluewhale/Bluewhale/tools/mpi/lib:/usr/lib64/:/opt/rh/devtoolset-7/root/usr/lib64:/opt/rh/devtoolset-7/root/usr/lib:/opt/rh/devtoolset-7/root/usr/lib64/dyninst:/opt/rh/devtoolset-7/root/usr/lib/dyninst:/opt/rh/devtoolset-7/root/usr/lib64:/opt/rh/devtoolset-7/root/usr/lib:/usr/lib64/:\n",
      "[1,0]<stderr>:2020-12-18 09:50:13.947525: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "[1,1]<stderr>:SLF4J: Class path contains multiple SLF4J bindings.\n",
      "[1,1]<stderr>:SLF4J: Found binding in [jar:file:/home/bluewhale/envs/hadoop-2.7.7/share/hadoop/kms/tomcat/webapps/kms/WEB-INF/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "[1,1]<stderr>:SLF4J: Found binding in [jar:file:/home/bluewhale/envs/hadoop-2.7.7/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "[1,1]<stderr>:SLF4J: Found binding in [jar:file:/home/bluewhale/envs/hadoop-2.7.7/share/hadoop/httpfs/tomcat/webapps/webhdfs/WEB-INF/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "[1,1]<stderr>:SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "[1,1]<stderr>:SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]\n",
      "[1,1]<stderr>:20/12/18 09:50:14 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "[1,0]<stderr>:SLF4J: Class path contains multiple SLF4J bindings.\n",
      "[1,0]<stderr>:SLF4J: Found binding in [jar:file:/home/bluewhale/envs/hadoop-2.7.7/share/hadoop/kms/tomcat/webapps/kms/WEB-INF/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "[1,0]<stderr>:SLF4J: Found binding in [jar:file:/home/bluewhale/envs/hadoop-2.7.7/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class][1,0]<stderr>:\n",
      "[1,0]<stderr>:SLF4J: Found binding in [jar:file:/home/bluewhale/envs/hadoop-2.7.7/share/hadoop/httpfs/tomcat/webapps/webhdfs/WEB-INF/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "[1,0]<stderr>:SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "[1,0]<stderr>:SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]\n",
      "[1,1]<stderr>:/home/bluewhale/Bluewhale/tools/venv/envs/bluewhale/lib/python3.7/runpy.py:125: RuntimeWarning: 'petastorm.workers_pool.exec_in_new_process' found in sys.modules after import of package 'petastorm.workers_pool', but prior to execution of 'petastorm.workers_pool.exec_in_new_process'; this may result in unpredictable behaviour\n",
      "[1,1]<stderr>:  warn(RuntimeWarning(msg))\n",
      "[1,1]<stderr>:/home/bluewhale/Bluewhale/tools/venv/envs/bluewhale/lib/python3.7/runpy.py:125: RuntimeWarning: 'petastorm.workers_pool.exec_in_new_process' found in sys.modules after import of package 'petastorm.workers_pool', but prior to execution of 'petastorm.workers_pool.exec_in_new_process'; this may result in unpredictable behaviour\n",
      "[1,1]<stderr>:  warn(RuntimeWarning(msg))\n",
      "[1,0]<stderr>:20/12/18 09:50:15 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "[1,1]<stderr>:SLF4J: Class path contains multiple SLF4J bindings.\n",
      "[1,1]<stderr>:SLF4J: Found binding in [jar:file:/home/bluewhale/envs/hadoop-2.7.7/share/hadoop/kms/tomcat/webapps/kms/WEB-INF/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "[1,1]<stderr>:SLF4J: Found binding in [jar:file:/home/bluewhale/envs/hadoop-2.7.7/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "[1,1]<stderr>:SLF4J: Found binding in [jar:file:/home/bluewhale/envs/hadoop-2.7.7/share/hadoop/httpfs/tomcat/webapps/webhdfs/WEB-INF/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "[1,1]<stderr>:SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "[1,1]<stderr>:SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]\n",
      "[1,1]<stderr>:SLF4J: Class path contains multiple SLF4J bindings.\n",
      "[1,1]<stderr>:SLF4J: Found binding in [jar:file:/home/bluewhale/envs/hadoop-2.7.7/share/hadoop/kms/tomcat/webapps/kms/WEB-INF/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "[1,1]<stderr>:SLF4J: Found binding in [jar:file:/home/bluewhale/envs/hadoop-2.7.7/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "[1,1]<stderr>:SLF4J: Found binding in [jar:file:/home/bluewhale/envs/hadoop-2.7.7/share/hadoop/httpfs/tomcat/webapps/webhdfs/WEB-INF/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "[1,1]<stderr>:SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "[1,1]<stderr>:SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]\n",
      "[1,1]<stderr>:20/12/18 09:50:16 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "[1,1]<stderr>:20/12/18 09:50:16 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "[1,0]<stderr>:/home/bluewhale/Bluewhale/tools/venv/envs/bluewhale/lib/python3.7/runpy.py:125: RuntimeWarning: 'petastorm.workers_pool.exec_in_new_process' found in sys.modules after import of package 'petastorm.workers_pool', but prior to execution of 'petastorm.workers_pool.exec_in_new_process'; this may result in unpredictable behaviour\n",
      "[1,0]<stderr>:  warn(RuntimeWarning(msg))\n",
      "[1,0]<stderr>:/home/bluewhale/Bluewhale/tools/venv/envs/bluewhale/lib/python3.7/runpy.py:125: RuntimeWarning: 'petastorm.workers_pool.exec_in_new_process' found in sys.modules after import of package 'petastorm.workers_pool', but prior to execution of 'petastorm.workers_pool.exec_in_new_process'; this may result in unpredictable behaviour\n",
      "[1,0]<stderr>:  warn(RuntimeWarning(msg))\n",
      "[1,1]<stderr>:/home/bluewhale/Bluewhale/tools/venv/envs/bluewhale/lib/python3.7/site-packages/petastorm/arrow_reader_worker.py:53: FutureWarning: Calling .data on ChunkedArray is provided for compatibility after Column was removed, simply drop this attribute\n",
      "[1,1]<stderr>:  column_as_pandas = column.data.chunks[0].to_pandas()\n",
      "[1,1]<stderr>:/home/bluewhale/Bluewhale/tools/venv/envs/bluewhale/lib/python3.7/site-packages/petastorm/pytorch.py:336: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "[1,1]<stderr>:  row_as_dict[k] = self.transform_fn(v)\n",
      "[1,0]<stderr>:SLF4J: Class path contains multiple SLF4J bindings.\n",
      "[1,0]<stderr>:SLF4J: Found binding in [jar:file:/home/bluewhale/envs/hadoop-2.7.7/share/hadoop/kms/tomcat/webapps/kms/WEB-INF/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[1,0]<stderr>:SLF4J: Found binding in [jar:file:/home/bluewhale/envs/hadoop-2.7.7/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "[1,0]<stderr>:SLF4J: Found binding in [jar:file:/home/bluewhale/envs/hadoop-2.7.7/share/hadoop/httpfs/tomcat/webapps/webhdfs/WEB-INF/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "[1,0]<stderr>:SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "[1,0]<stderr>:SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]\n",
      "[1,0]<stderr>:SLF4J: Class path contains multiple SLF4J bindings.\n",
      "[1,0]<stderr>:SLF4J: Found binding in [jar:file:/home/bluewhale/envs/hadoop-2.7.7/share/hadoop/kms/tomcat/webapps/kms/WEB-INF/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "[1,0]<stderr>:SLF4J: Found binding in [jar:file:/home/bluewhale/envs/hadoop-2.7.7/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "[1,0]<stderr>:SLF4J: Found binding in [jar:file:/home/bluewhale/envs/hadoop-2.7.7/share/hadoop/httpfs/tomcat/webapps/webhdfs/WEB-INF/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "[1,0]<stderr>:SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "[1,0]<stderr>:SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]\n",
      "[1,1]<stderr>:/home/bluewhale/Bluewhale/tools/venv/envs/bluewhale/lib/python3.7/site-packages/horovod/spark/task/mpirun_exec_fn.py:54: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "[1,0]<stderr>:20/12/18 09:50:17 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "[1,0]<stderr>:20/12/18 09:50:17 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "[1,0]<stderr>:/home/bluewhale/Bluewhale/tools/venv/envs/bluewhale/lib/python3.7/site-packages/petastorm/arrow_reader_worker.py:53: FutureWarning: Calling .data on ChunkedArray is provided for compatibility after Column was removed, simply drop this attribute\n",
      "[1,0]<stderr>:  column_as_pandas = column.data.chunks[0].to_pandas()\n",
      "[1,0]<stderr>:/home/bluewhale/Bluewhale/tools/venv/envs/bluewhale/lib/python3.7/site-packages/petastorm/pytorch.py:336: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "[1,0]<stderr>:  row_as_dict[k] = self.transform_fn(v)\n",
      "[1,0]<stderr>:/home/bluewhale/Bluewhale/tools/venv/envs/bluewhale/lib/python3.7/site-packages/horovod/spark/task/mpirun_exec_fn.py:54: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,0]<stdout>:epoch:\t0\tstep\t0:\t{'loss': 6.2590155601501465, 'all_metrics': []}[1,0]<stdout>:\n",
      "[1,1]<stdout>:{'epoch': 0, 'train': {'loss': 6.258663654327393, 'all_metrics': []}}\n",
      "[1,0]<stdout>:{'epoch': 0, 'train': {'loss': 6.258663654327393, 'all_metrics': []}}\n",
      "[1,0]<stdout>:epoch:\t1\tstep\t0:\t{'loss': 6.257025718688965, 'all_metrics': []}\n",
      "[1,1]<stdout>:{'epoch': 1, 'train': {'loss': 6.256636142730713, 'all_metrics': []}}\n",
      "[1,0]<stdout>:{'epoch': 1, 'train': {'loss': 6.256636142730713, 'all_metrics': []}}\n",
      "[1,0]<stdout>:epoch:\t2\tstep\t0:\t{'loss': 6.255742073059082, 'all_metrics': []}[1,0]<stdout>:\n",
      "[1,1]<stdout>:{'epoch': 2, 'train': {'loss': 6.255521297454834, 'all_metrics': []}}\n",
      "[1,0]<stdout>:{'epoch': 2, 'train': {'loss': 6.255521297454834, 'all_metrics': []}}\n",
      "[1,0]<stdout>:epoch:\t3\tstep\t0:\t{'loss': 6.255092620849609, 'all_metrics': []}[1,0]<stdout>:\n",
      "[1,0]<stdout>:{'epoch': 3, 'train': {'loss': 6.2541399002075195, 'all_metrics': []}}\n",
      "[1,1]<stdout>:{'epoch': 3, 'train': {'loss': 6.2541399002075195, 'all_metrics': []}}\n",
      "[1,0]<stdout>:epoch:\t4\tstep\t0:\t{'loss': 6.252333641052246, 'all_metrics': []}\n",
      "[1,0]<stdout>:{'epoch': 4, 'train': {'loss': 6.252382755279541, 'all_metrics': []}}\n",
      "[1,1]<stdout>:{'epoch': 4, 'train': {'loss': 6.252382755279541, 'all_metrics': []}}\n",
      "train time: 64.6401207447052\n"
     ]
    }
   ],
   "source": [
    "backend = SparkBackend(num_proc, nics=[options.nics], use_mpi=True)\n",
    "torch_estimator = hvd.TorchEstimator(backend=backend,\n",
    "                                     store=store,\n",
    "                                     model=model,\n",
    "                                     optimizer=optimizer,\n",
    "                                     loss=lambda input, target: loss(input, target.long()),\n",
    "                                     feature_cols=feature_cols,\n",
    "                                     input_shapes=[[-1, 5], [-1], [-1], [-1], [-1]],\n",
    "                                     label_cols=['label'],\n",
    "                                     batch_size=batch_size,\n",
    "                                     epochs=num_epoch,\n",
    "                                     verbose=2)\n",
    "start = time.time()\n",
    "torch_model = torch_estimator.fit(train_df).setOutputCols(['label_prob'])\n",
    "end = time.time()\n",
    "train_time = end - start\n",
    "print(f\"train time: {train_time}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the held-out test DataFrame\n",
    "pred_df = torch_model.transform(test_df)\n",
    "argmax = udf(lambda v: float(np.argmax(v)), returnType=T.DoubleType())\n",
    "pred_df = pred_df.withColumn('label_pred', argmax(pred_df.label_prob))\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol='label_pred', labelCol='label', metricName='accuracy')\n",
    "test_acc = evaluator.evaluate(pred_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Preparation: 5.80s / epoch\n",
      "Train Total: 64.64s / epoch\n",
      "Train Avg: 12.93s / epoch\n",
      "Test Acc: 0.00\n"
     ]
    }
   ],
   "source": [
    "print(f\"Data Preparation: {prepare_time:.2f}s\\\") \n",
    "print(f\"Train Total: {train_time:.2f}s / epoch\") \n",
    "print(f\"Train Avg: {train_time/num_epoch:.2f}s / epoch\") \n",
    "print(f'Test Acc: {test_acc:.2f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
