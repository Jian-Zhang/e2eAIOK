{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"16\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import logging\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import pyspark\n",
    "import pyspark.sql.types as T\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "import horovod.spark.torch as hvd\n",
    "from horovod.spark.common.store import HDFSStore\n",
    "from horovod.spark.common.backend import SparkBackend "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    spark = pyspark.sql.SparkSession.builder.master(\"spark://sr231:7077\")\\\n",
    "                                        .config(\"spark.cores.max\", 96) \\\n",
    "                                        .config(\"spark.executor.cores\", 16) \\\n",
    "                                        .config(\"spark.executor.memory\", \"120g\") \\\n",
    "                                        .config(\"spark.default.parallelism\", 96) \\\n",
    "                                        .config(\"spark.sql.execution.arrow.enabled\", \"true\") \\\n",
    "                                        .config(\"spark.task.cpus\", 1) \\\n",
    "                                        .getOrCreate()\n",
    "    time1=time.time()\n",
    "    df = spark.read.json(\"hdfs://sr231:9000/data\")\n",
    "  \n",
    "    train_df, test_df = df.randomSplit([0.999, 0.001],seed=100)\n",
    "    print(\"traindf count:\",train_df.count())\n",
    "    print(\"testdf count:\",test_df.count())\n",
    "\n",
    "    store = HDFSStore('hdfs://sr231:9000/tmp')\n",
    "    time2=time.time()\n",
    "    data_prepare_time=time2-time1\n",
    "    print(\"data_prepare_time:\",data_prepare_time)\n",
    "\n",
    "    n_plus=522\n",
    "    n_time=167\n",
    "    n_bkids=126\n",
    "    n_weather=35\n",
    "    n_feels=20\n",
    "\n",
    "    # Bidirectional recurrent neural network (many-to-one)\n",
    "    class BiRNN(nn.Module):\n",
    "        def __init__(self, input_size, hidden_size, num_layers, fcn_input_size, fcn_output_size):\n",
    "            super(BiRNN, self).__init__()\n",
    "            self.hidden_size = hidden_size\n",
    "            self.num_layers = num_layers\n",
    "        \n",
    "            self.embeds_pluids = nn.Embedding(n_plus, 50, sparse=True)\n",
    "            self.embeds_bkidx = nn.Embedding(n_bkids, 100, sparse=True)\n",
    "            self.embeds_timeidx = nn.Embedding(n_time, 100, sparse=True)\n",
    "            self.embeds_feelsBucket = nn.Embedding(n_feels, 100, sparse=True)\n",
    "            self.embeds_weather = nn.Embedding(n_weather, 100, sparse=True)\n",
    "        \n",
    "            self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
    "        \n",
    "            self.hidden1 = nn.Linear(100, 100)\n",
    "            self.hidden2 = nn.Linear(100, 1)\n",
    "            self.flatten = nn.Flatten()\n",
    "        \n",
    "            self.drop_layer = nn.Dropout(p=0.3)\n",
    "            self.fc = nn.Linear(fcn_input_size, fcn_output_size)\n",
    "        \n",
    "        \n",
    "        def forward(self, pluids, timeidx, bkidx, weatheridx, feelsBucket):\n",
    "            \n",
    "            pluids = pluids.long()\n",
    "            timeidx = timeidx.long()\n",
    "            bkidx = bkidx.long()\n",
    "            weatheridx = weatheridx.long()\n",
    "            feelsBucket = feelsBucket.long()\n",
    "            plu_embed = self.embeds_pluids(pluids)\n",
    "            bkidx_embed = self.embeds_bkidx(bkidx)\n",
    "            time_embed = self.embeds_timeidx(timeidx)\n",
    "            weather_embed = self.embeds_weather(weatheridx)\n",
    "            feels_embed = self.embeds_feelsBucket(feelsBucket)\n",
    "\n",
    "            x = plu_embed\n",
    "\n",
    "            h0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size) # 2 for bidirection \n",
    "            # Forward propagate gru\n",
    "            gru_out, _ = self.gru(x, h0)\n",
    "            ut = torch.tanh(self.hidden1(gru_out))\n",
    "            # et shape: [batch_size, seq_len, att_hops]\n",
    "            et = self.hidden2(ut)\n",
    "\n",
    "            # att shape: [batch_size,  att_hops, seq_len]\n",
    "            att = F.softmax(torch.transpose(et, 2, 1))\n",
    "        \n",
    "            # output shape [batch_size, att_hops, embedding_width]\n",
    "            output = torch.matmul(att, gru_out)\n",
    "        \n",
    "            # flatten the output\n",
    "            attention_output = self.flatten(output)\n",
    "            context_features = torch.mul(attention_output,(1 + bkidx_embed + time_embed + weather_embed + feels_embed))\n",
    "            ac1 = F.relu(context_features)\n",
    "        \n",
    "            dropout = self.drop_layer(ac1)\n",
    "            output = self.fc(dropout)\n",
    "\n",
    "            \n",
    "            return output\n",
    "\n",
    "    # train with SGD\n",
    "    model = BiRNN(50, 50, 1, 100, 522)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    num_proc=6\n",
    "\n",
    "    feature_cols = [\"pluids\", \"timeidx\", \"bkidx\", \"weatheridx\", \"feelsBucket\"]\n",
    "    batch_size = 16000\n",
    "    num_epoch = 5\n",
    "\n",
    "    def f(data):\n",
    "            # this is used to avoid omp resource competition\n",
    "            os.environ[\"OMP_NUM_THREADS\"] = \"2\"\n",
    "            iter([os.environ[\"OMP_NUM_THREADS\"]])\n",
    "        \n",
    "    #backend = SparkBackend(num_proc, nic=\"eth2\")\n",
    "    backend = SparkBackend(num_proc, nics=[\"eth2\"], use_gloo=True)\n",
    "    torch_estimator = hvd.TorchEstimator(backend=backend,\n",
    "                                         store=store,\n",
    "                                         model=model,\n",
    "                                         optimizer=optimizer,\n",
    "                                         loss=lambda input, target: loss(input, target.long()),\n",
    "                                         feature_cols=feature_cols,\n",
    "                                         input_shapes=[[-1, 5], [-1], [-1], [-1], [-1]],\n",
    "                                         label_cols=['label'],\n",
    "                                         batch_size=batch_size,\n",
    "                                         epochs=num_epoch,\n",
    "                                         verbose=2)\n",
    "\n",
    "\n",
    "    torch_model = torch_estimator.fit(train_df).setOutputCols(['label_prob'])\n",
    "    time3 = time.time()\n",
    "    print(f\"Duration(train): {time3 - time2}\") \n",
    "\n",
    "    \n",
    "    #spark.range(0, 96, 96).rdd.mapPartitions(f).count()\n",
    "\n",
    "    # Evaluate the model on the held-out test DataFrame\n",
    "    start = time.time()\n",
    "    pred_df = torch_model.transform(test_df)\n",
    "    argmax = udf(lambda v: float(np.argmax(v)), returnType=T.DoubleType())\n",
    "    pred_df = pred_df.withColumn('label_pred', argmax(pred_df.label_prob))\n",
    "    evaluator = MulticlassClassificationEvaluator(predictionCol='label_pred', labelCol='label', metricName='accuracy')\n",
    "\n",
    "    train_count=train_df.count()\n",
    "    test_count=test_df.count()\n",
    "    print(\"traindf count:\",train_count)\n",
    "    print(\"testdf count:\",test_count)\n",
    "\n",
    "    print(f\"Duration(train): {time3 - time2}\")\n",
    "    print(f\"Average epoch duration(train): {(time3 - time2)/5}\") \n",
    "    print(\"data_prepare_time:\",data_prepare_time)\n",
    "\n",
    "    print('Test accuracy:', evaluator.evaluate(pred_df))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hvd",
   "language": "python",
   "name": "hvd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
