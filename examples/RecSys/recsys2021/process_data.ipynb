{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DataProcessUtils.init_spark import *\n",
    "from DataProcessUtils.utils import *\n",
    "from DataProcessUtils.data_processor import *\n",
    "from RecsysSchema import RecsysSchema\n",
    "\n",
    "import logging\n",
    "from timeit import default_timer as timer\n",
    "import os\n",
    "from pyspark import *\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "path_prefix = \"hdfs://\"\n",
    "folder = \"/recsys2021/decompress/\"\n",
    "file = \"/recsys2021/decompress/part-00036\"\n",
    "files = ['part-00036', 'part-00037', 'part-00038', 'part-00039', 'part-00040', 'part-00041', 'part-00042', 'part-00043', 'part-00044', 'part-00045'] \n",
    "#path = [os.path.join(path_prefix, folder, file) for file in files]\n",
    "#path = os.path.join(path_prefix, folder)\n",
    "path = os.path.join(path_prefix, file)\n",
    "recsysSchema = RecsysSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataFrame[\n",
    "*  text_tokens: string, \n",
    "*  hashtags: string, \n",
    "*  tweet_id: string, \n",
    "*  present_media: string, \n",
    "*  present_links: string, \n",
    "*  present_domains: string, \n",
    "*  tweet_type: string, \n",
    "*  language: string, \n",
    "*  tweet_timestamp: int, \n",
    "*  engaged_with_user_id: string, \n",
    "*  engaged_with_user_follower_count: int, \n",
    "*  engaged_with_user_following_count: int, \n",
    "*  engaged_with_user_is_verified: boolean, \n",
    "*  engaged_with_user_account_creation: int, \n",
    "*  enaging_user_id: string, \n",
    "*  enaging_user_follower_count: int, \n",
    "*  enaging_user_following_count: int, \n",
    "*  enaging_user_is_verified: boolean, \n",
    "*  enaging_user_account_creation: int, \n",
    "*  engagee_follows_engager: boolean, \n",
    "*  reply_timestamp: float,    \n",
    "*  retweet_timestamp: float, \n",
    "*  retweet_with_comment_timestamp: float, \n",
    "*  like_timestamp: float\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### 1. Start spark and initialize data processor #####\n",
    "t0 = timer()\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .master('yarn')\\\n",
    "    .appName(\"Recsys2021_DATA_PROCESS\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Enable Arrow-based columnar data transfers\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "\n",
    "schema = recsysSchema.toStructType()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.schema(schema).option('sep', '\\x01').csv(path)\n",
    "proc = DataProcessor(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "###### 2. define operations and append them to data processor ######\n",
    "# pre-define\n",
    "# 0.1 define udfs\n",
    "replace = udf(lambda x:  '_'.join(x.split('\\t')[:2]) if x else \"\", StringType())\n",
    "count = udf(lambda x: str(x).count('\\t')+1 if x else 0, LongType())\n",
    "# 0.2 define dictionary\n",
    "media = {\n",
    "    '': 0,\n",
    "    'GIF': 1,\n",
    "    'GIF_GIF': 2,\n",
    "    'GIF_Photo': 3,\n",
    "    'GIF_Video': 4,\n",
    "    'Photo': 5,\n",
    "    'Photo_GIF': 6,\n",
    "    'Photo_Photo': 7,\n",
    "    'Photo_Video': 8,\n",
    "    'Video': 9,\n",
    "    'Video_GIF': 10,\n",
    "    'Video_Photo': 11,\n",
    "    'Video_Video': 12\n",
    "}\n",
    "\n",
    "tweet_type = {'Quote': 0, 'Retweet': 1, 'TopLevel': 2}\n",
    "\n",
    "# 1. define operations\n",
    "\n",
    "# 1.1 fill na\n",
    "op_fillna_num = FillNA(['reply_timestamp', 'retweet_timestamp',\n",
    "                        'retweet_with_comment_timestamp', 'like_timestamp'], 0)\n",
    "op_fillna_str = FillNA(['present_domains', 'present_links', 'hashtags', 'present_media'], \"\")\n",
    "\n",
    "\n",
    "# 1.2 feature modify and add\n",
    "op_feature_modification_type_convert = FeatureModification(cols=['tweet_timestamp',\n",
    "                                                                 'engaged_with_user_follower_count',\n",
    "                                                                 'engaged_with_user_following_count',\n",
    "                                                                 'engaged_with_user_account_creation',\n",
    "                                                                 'enaging_user_follower_count',\n",
    "                                                                 'enaging_user_following_count',\n",
    "                                                                 'enaging_user_account_creation', 'reply_timestamp',\n",
    "                                                                 'retweet_timestamp',\n",
    "                                                                 'retweet_with_comment_timestamp',\n",
    "                                                                 'like_timestamp'], op='toInt')\n",
    "op_feature_modification_present_media_replace = FeatureModification(\n",
    "    cols = {'present_media': \"concat_ws('_', split(col('present_media'),'\\t'))\"}, op = 'inline')\n",
    "#op_feature_modification_present_media_replace = FeatureModification(\n",
    "#    cols=['present_media'], udfImpl=replace)\n",
    "\n",
    "#op_feature_add_len = FeatureAdd(\n",
    "#    cols={'len_hashtags': 'hashtags', 'len_domains': 'present_domains', 'len_links': 'present_links'}, udfImpl=count)\n",
    "op_feature_add_len_hashtags = FeatureAdd(\n",
    "    cols={'len_hashtags': \"when(col('hashtags') == '', lit(0)).otherwise(size(split(col('hashtags'), '\\t')))\"}, op = 'inline')\n",
    "op_feature_add_len_domains = FeatureAdd(\n",
    "    cols={'len_domains': \"when(col('present_domains') == '', lit(0)).otherwise(size(split(col('present_domains'), '\\t')))\"}, op = 'inline')\n",
    "op_feature_add_len_links = FeatureAdd(\n",
    "    cols={'len_links': \"when(col('present_links') == '', lit(0)).otherwise(size(split(col('present_links'), '\\t')))\"}, op = 'inline')\n",
    "op_feature_add_engage_time = FeatureAdd(\n",
    "    cols={'engage_time': \"least(col('reply_timestamp'), col('retweet_timestamp'), col('retweet_with_comment_timestamp'), col('like_timestamp'))\"}, op='inline')\n",
    "op_new_feature_dt_dow = FeatureAdd(cols={\n",
    "    \"dt_dow\": \"dayofweek(from_unixtime(col('tweet_timestamp'))).cast(IntegerType())\",\n",
    "    \"dt_hour\": \"hour(from_unixtime(col('tweet_timestamp'))).cast(IntegerType())\",\n",
    "    \"dt_minute\": \"minute(from_unixtime(col('tweet_timestamp'))).cast(IntegerType())\",\n",
    "    \"dt_second\": \"second(from_unixtime(col('tweet_timestamp'))).cast(IntegerType())\"}, op='inline')\n",
    "\n",
    "op_feature_change = FeatureModification(cols={\n",
    "    \"reply_timestamp\": \"when(col('reply_timestamp') > 0, 1).otherwise(0)\",\n",
    "    \"retweet_timestamp\": \"when(col('retweet_timestamp') > 0, 1).otherwise(0)\",\n",
    "    \"retweet_with_comment_timestamp\": \"when(col('retweet_with_comment_timestamp') > 0, 1).otherwise(0)\",\n",
    "    \"like_timestamp\": \"when(col('like_timestamp') > 0, 1).otherwise(0)\"}, op='inline')\n",
    "\n",
    "ops = [op_fillna_num, op_fillna_str, \n",
    "       op_feature_modification_type_convert, op_feature_modification_present_media_replace,\n",
    "       op_feature_add_len_hashtags, op_feature_add_len_domains, op_feature_add_len_links, \n",
    "       op_feature_add_engage_time, op_new_feature_dt_dow, op_feature_change]\n",
    "proc.reset_ops(ops)\n",
    "\n",
    "# 1.3 categorify    \n",
    "# udf took lots of memory, process in advance\n",
    "op_categorifyMultiItems = CategorifyMultiItems(\n",
    "    ['present_domains', 'present_links', 'hashtags'])\n",
    "op_categorify_present_media = CategorifyWithDictionary(\n",
    "    ['present_media'], media)\n",
    "op_categorify_tweet_type = CategorifyWithDictionary(\n",
    "    ['tweet_type'], tweet_type)\n",
    "# since language dict is small, we may use udf to make partition more even\n",
    "op_categorify_language = Categorify(['language'], hint = 'udf')\n",
    "\n",
    "ops_1 = [op_categorifyMultiItems, op_categorify_present_media, op_categorify_tweet_type, op_categorify_language]\n",
    "proc.append_ops(ops_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Process and udf categorify took 12.441\n"
     ]
    }
   ],
   "source": [
    "##### 3. do data transform(data frame materialize) #####\n",
    "t1 = timer()\n",
    "df = proc.transform(df)\n",
    "t2 = timer()\n",
    "print(\"Data Process and udf categorify took %.3f\" % (t2 - t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorify w/join took 5.543 7.025\n"
     ]
    }
   ],
   "source": [
    "# below are using join\n",
    "\n",
    "# since we observed extremely high mem footage to \n",
    "# do below joins, split each run to save memory\n",
    "\n",
    "op_categorify_tweet_id = Categorify(['tweet_id'])\n",
    "proc.reset_ops([op_categorify_tweet_id])\n",
    "t1 = timer()\n",
    "df = proc.transform(df)\n",
    "t2 = timer()\n",
    "\n",
    "op_categorify_user_id = Categorify(['engaged_with_user_id', 'enaging_user_id'], src_cols=[\n",
    "                                   'engaged_with_user_id', 'enaging_user_id'])\n",
    "proc.reset_ops([op_categorify_user_id])\n",
    "t5 = timer()\n",
    "df = proc.transform(df)\n",
    "t6 = timer()\n",
    "\n",
    "print(\"Categorify w/join took %.3f %.3f\" % ((t2 - t1), (t6 - t5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.format('parquet').mode(\n",
    "                'overwrite').save(\"/recsys2021/10files/processed_phase_1/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = spark.read.parquet(\"/recsys2021/10files/processed_phase_1/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertTokenizer decode and format took 22.334\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased', do_lower_case=False)\n",
    "\n",
    "# define UDF\n",
    "tokenizer_decode = udf(lambda x: tokenizer.decode( [ int(n) for n in x.split('\\t') ] ))\n",
    "format_url = udf(lambda x: x.replace('https : / / t. co / ', 'https://t.co/').replace('@ ', '@'))\n",
    "\n",
    "# define operations\n",
    "op_feature_modification_tokenizer_decode = FeatureAdd(\n",
    "    cols={'tweet':'text_tokens'}, udfImpl=tokenizer_decode)\n",
    "op_feature_modification_format_url = FeatureModification(\n",
    "    cols=['tweet'], udfImpl=format_url)\n",
    "\n",
    "# execute\n",
    "proc.reset_ops([op_feature_modification_tokenizer_decode, op_feature_modification_format_url])\n",
    "t1 = timer()\n",
    "df = proc.transform(df)\n",
    "t2 = timer()\n",
    "print(\"BertTokenizer decode and format took %.3f\" % (t2 - t1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "\n",
    "def extract_hash(text, split_text='@', no=0):\n",
    "    text = text.lower()\n",
    "    uhash = ''\n",
    "    text_split = text.split('@')\n",
    "    if len(text_split)>(no+1):\n",
    "        text_split = text_split[no+1].split(' ')\n",
    "        cl_loop = True\n",
    "        uhash += clean_text(text_split[0])\n",
    "        while cl_loop:\n",
    "            if len(text_split)>1:\n",
    "                if text_split[1] in ['_']:\n",
    "                    uhash += clean_text(text_split[1]) + clean_text(text_split[2])\n",
    "                    text_split = text_split[2:]\n",
    "                else:\n",
    "                    cl_loop = False\n",
    "            else:\n",
    "                cl_loop = False\n",
    "    hash_object = hashlib.md5(uhash.encode('utf-8'))\n",
    "    return hash_object.hexdigest()\n",
    "\n",
    "def clean_text(text):\n",
    "    if len(text)>1:\n",
    "        if text[-1] in ['!', '?', ':', ';', '.', ',']:\n",
    "            return(text[:-1])\n",
    "    return(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding Feature upon tweet and tweet_nortsign column took 7.115\n"
     ]
    }
   ],
   "source": [
    "# features upon tweet\n",
    "to_notsign = udf(lambda x: x.replace('\\[CLS\\] RT @', ''))\n",
    "count_space = udf(lambda x: x.count(' '))\n",
    "count_text_length = udf(lambda x: len(x))\n",
    "user_defined_hash = udf(lambda x: extract_hash(x, split_text='RT @', no=0))\n",
    "# features upon tweet_nortsign\n",
    "count_at = udf(lambda x: x.count('@'))\n",
    "user_define_hash_1 = udf(lambda x: extract_hash(x))\n",
    "user_define_hash_2 = udf(lambda x: extract_hash(x, no=1))\n",
    "\n",
    "# features upon tweet\n",
    "op_feature_add_tweet_nortsign = FeatureAdd(cols={'tweet_nortsign': 'tweet'}, udfImpl=to_notsign)\n",
    "op_feature_add_count_words = FeatureAdd(cols={'count_words': 'tweet'}, udfImpl=count_space)\n",
    "op_feature_add_count_char = FeatureAdd(cols={'count_char': 'tweet'}, udfImpl=count_text_length)\n",
    "op_feature_add_tw_uhash = FeatureAdd(cols={'tw_uhash': 'tweet'}, udfImpl=user_defined_hash)\n",
    "op_feature_add_tw_hash = FeatureAdd(cols={'tw_hash': \"hash(col('tweet'))%1000000000\"}, op='inline')\n",
    "# features upon tweet_nortsign\n",
    "op_feature_add_count_at = FeatureAdd(cols={'count_ats': 'tweet_nortsign'}, udfImpl=count_at)\n",
    "op_feature_add_tw_uhash0 = FeatureAdd(cols={'tw_hash0': 'tweet_nortsign'}, udfImpl=user_define_hash_1)\n",
    "op_feature_add_tw_uhash1 = FeatureAdd(cols={'tw_hash1': 'tweet_nortsign'}, udfImpl=user_define_hash_2)\n",
    "    \n",
    "# execute\n",
    "proc.reset_ops([op_feature_add_tweet_nortsign, op_feature_add_count_words, op_feature_add_count_char, \n",
    "                op_feature_add_tw_uhash, op_feature_add_tw_hash,\n",
    "                op_feature_add_count_at, op_feature_add_tw_uhash0, op_feature_add_tw_uhash1])\n",
    "t1 = timer()\n",
    "df = proc.transform(df)\n",
    "t2 = timer()\n",
    "print(\"Adding Feature upon tweet and tweet_nortsign column took %.3f\" % (t2 - t1))\n",
    "# expect to spend about 1000secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.format('parquet').mode(\n",
    "                'overwrite').save(\"/recsys2021/1file/processed_phase_2/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = spark.read.parquet(\"/recsys2021/1file/processed_phase_2/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequency encode tweet column took 71.011\n"
     ]
    }
   ],
   "source": [
    "proc = DataProcessor(spark)\n",
    "op_fillna_for_tweet = FillNA(['tweet'], \"\")\n",
    "op_categorify_multiple_tweet = CategorifyMultiItems(['tweet'], strategy=1, sep=' ', skipList=['', '[', ']', '.', '!', '@', '_', '#'])\n",
    "proc.reset_ops([op_fillna_for_tweet, op_categorify_multiple_tweet])\n",
    "t1 = timer()\n",
    "df = proc.transform(df)\n",
    "t2 = timer()\n",
    "print(\"Frequency encode tweet column took %.3f\" % (t2 - t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature engineering upon Frequency encoded tweet column took 5.338\n"
     ]
    }
   ],
   "source": [
    "op_feature_add_tw_freq_hash = FeatureAdd({'tw_freq_hash': \"col('tw_hash')\"}, op='inline')\n",
    "op_feature_add_tw_first_word = FeatureAdd({'tw_first_word': \"col('tweet').getItem(0)\"}, op='inline')\n",
    "op_feature_add_tw_second_word = FeatureAdd({'tw_second_word': \"col('tweet').getItem(1)\"}, op='inline')\n",
    "op_feature_add_tw_last_word = FeatureAdd({'tw_last_word': \"col('tweet').getItem(size(col('tweet')) - 1)\"}, op='inline')\n",
    "op_feature_add_tw_second_last_word = FeatureAdd({'tw_llast_word': \"col('tweet').getItem(size(col('tweet')) - 1)\"}, op='inline')\n",
    "op_feature_add_tw_word_len = FeatureAdd({'tw_len': \"size(col('tweet'))\"}, op='inline')\n",
    "op_feature_modification_fillna = FillNA(['tw_freq_hash', 'tw_first_word', 'tw_second_word', 'tw_last_word', 'tw_llast_word', 'tw_len'], -1)\n",
    "\n",
    "proc.reset_ops([op_feature_add_tw_freq_hash, op_feature_add_tw_first_word, op_feature_add_tw_second_word,\n",
    "                op_feature_add_tw_last_word, op_feature_add_tw_second_last_word, op_feature_add_tw_word_len,\n",
    "                op_feature_modification_fillna])\n",
    "t1 = timer()\n",
    "df = proc.transform(df)\n",
    "t2 = timer()\n",
    "print(\"feature engineering upon Frequency encoded tweet column took %.3f\" % (t2 - t1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------+--------------+------------+-------------+------+\n",
      "|tw_freq_hash|tw_first_word|tw_second_word|tw_last_word|tw_llast_word|tw_len|\n",
      "+------------+-------------+--------------+------------+-------------+------+\n",
      "|   499420595|       399154|        261800|          70|           70|    19|\n",
      "|  -525989518|       716069|         71335|          69|           69|    15|\n",
      "|   560765826|       235425|         45200|         809|          809|     9|\n",
      "|  -603094632|        23209|            -1|       23209|        23209|     1|\n",
      "|    81094762|        26657|         14059|        5595|         5595|     3|\n",
      "|  -682976397|       286150|         80530|          75|           75|    18|\n",
      "|   173727706|       825481|         62636|        2853|         2853|     3|\n",
      "|  -141265501|       772505|        546603|         119|          119|    23|\n",
      "|   463336980|        30457|         18542|          55|           55|    16|\n",
      "|  -468553637|       410254|         74633|          55|           55|    17|\n",
      "|  -329269770|       645534|        644866|       22131|        22131|     6|\n",
      "|   125089140|       380249|        273244|          48|           48|    48|\n",
      "|   162707974|       590601|        407493|          83|           83|    16|\n",
      "|   119914991|       487029|        470432|          71|           71|     5|\n",
      "|     2587298|       575414|        349579|        1866|         1866|     5|\n",
      "|   241831221|         2166|           270|         270|          270|     2|\n",
      "|   441732698|        83926|          6292|          62|           62|    74|\n",
      "|  -554297119|       581054|         95775|         231|          231|     7|\n",
      "|   461154764|           -1|            -1|          -1|           -1|     0|\n",
      "|   856908401|       301348|        301348|          50|           50|    48|\n",
      "+------------+-------------+--------------+------------+-------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('tw_freq_hash', 'tw_first_word', 'tw_second_word', 'tw_last_word', 'tw_llast_word', 'tw_len').show(truncate=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.format('parquet').mode(\n",
    "                'overwrite').save(\"/recsys2021/1file/processed_phase_3/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
