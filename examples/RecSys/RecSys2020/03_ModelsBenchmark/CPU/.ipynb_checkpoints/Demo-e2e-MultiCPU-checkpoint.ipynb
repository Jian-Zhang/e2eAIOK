{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) 2020, NVIDIA CORPORATION.\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "    http://www.apache.org/licenses/LICENSE-2.0\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time\n",
    "start = time.time()\n",
    "very_start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_rows', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "from dask.distributed import Client, wait, LocalCluster\n",
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://10.1.0.131:42286</li>\n",
       "  <li><b>Dashboard: </b><a href='http://10.1.0.131:8787/status' target='_blank'>http://10.1.0.131:8787/status</a></li>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>8</li>\n",
       "  <li><b>Cores: </b>8</li>\n",
       "  <li><b>Memory: </b>1.60 TB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://10.1.0.131:42286' processes=8 threads=8, memory=1.60 TB>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "client = Client(n_workers=8, \n",
    "                       threads_per_worker=1,\n",
    "                       memory_limit='200GB',ip='10.1.0.131')\n",
    "#client = Client(ip='10.2.48.253',memory_limit='100GB')\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 35.3 ms, sys: 4.9 ms, total: 40.2 ms\n",
      "Wall time: 38.3 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "path = '/mnt/DP_disk3/Recsys/nv'\n",
    "train = dd.read_parquet(f'{path}/train-preproc-fold-*.parquet')#,dtypes=dtypes)\n",
    "#train = dd.read_parquet('/mnt/DP_disk3/Recsys/train-1.parquet')#,dtypes=dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['tweet_id', 'media', 'links', 'domains', 'tweet_type', 'language',\n",
       "       'timestamp', 'a_user_id', 'a_follower_count', 'a_following_count',\n",
       "       'a_is_verified', 'a_account_creation', 'b_user_id', 'b_follower_count',\n",
       "       'b_following_count', 'b_is_verified', 'b_account_creation',\n",
       "       'b_follows_a', 'reply', 'retweet', 'retweet_comment', 'like',\n",
       "       'engage_time', 'len_domains', 'len_hashtags', 'len_links', 'hashtags0',\n",
       "       'hashtags1', 'fold'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tweet_id                      uint32\n",
       "media                          uint8\n",
       "links                         uint32\n",
       "domains                       uint32\n",
       "tweet_type                     uint8\n",
       "language                       uint8\n",
       "timestamp             datetime64[ns]\n",
       "a_user_id                     uint32\n",
       "a_follower_count              uint32\n",
       "a_following_count             uint32\n",
       "a_is_verified                   bool\n",
       "a_account_creation    datetime64[ns]\n",
       "b_user_id                     uint32\n",
       "b_follower_count              uint32\n",
       "b_following_count             uint32\n",
       "b_is_verified                   bool\n",
       "b_account_creation    datetime64[ns]\n",
       "b_follows_a                     bool\n",
       "reply                          uint8\n",
       "retweet                        uint8\n",
       "retweet_comment                uint8\n",
       "like                           uint8\n",
       "engage_time           datetime64[ns]\n",
       "len_domains                    uint8\n",
       "len_hashtags                   uint8\n",
       "len_links                      uint8\n",
       "hashtags0                     uint32\n",
       "hashtags1                     uint32\n",
       "fold                          uint32\n",
       "dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11 ms, sys: 4.26 ms, total: 15.3 ms\n",
      "Wall time: 12.2 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# DROP UNUSED COLUMNS\n",
    "#cols_drop = ['links','hashtags0', 'hashtags1', 'fold']\n",
    "cols_drop = ['links']\n",
    "\n",
    "train = train.drop(cols_drop,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dask.dataframe.core.DataFrame'> (Delayed('int-eb0041e1-7f3d-41cb-9efa-f24991454ea2'), 28)\n",
      "CPU times: user 52.6 ms, sys: 14.2 ms, total: 66.8 ms\n",
      "Wall time: 64.7 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train, = dask.persist(train)\n",
    "print(type(train), train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dask.dataframe.core.DataFrame'> (Delayed('int-5e50928b-a547-42e1-bceb-6cd3ac89d07a'), 28)\n",
      "CPU times: user 18.8 ms, sys: 1.17 ms, total: 20 ms\n",
      "Wall time: 17.6 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train = train.repartition(npartitions=8)\n",
    "train, = dask.persist(train)\n",
    "print(type(train), train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i,p in enumerate(train.partitions):\n",
    "#    print(i,len(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names = ['reply', 'retweet', 'retweet_comment', 'like']\n",
    "for col in train.columns:\n",
    "    if col in label_names:\n",
    "        train[col] = train[col].astype('float32')\n",
    "    elif train[col].dtype=='int64':\n",
    "        train[col] = train[col].astype('int32')\n",
    "    elif train[col].dtype=='int16':\n",
    "        train[col] = train[col].astype('int8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.66 ms, sys: 2.16 ms, total: 7.82 ms\n",
      "Wall time: 6.26 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train = train.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Delayed('int-74ca8b69-3c5b-4a93-800b-5f4b211e1527'), 28)\n",
      "CPU times: user 2.76 ms, sys: 3.74 ms, total: 6.5 ms\n",
      "Wall time: 5.85 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train, = dask.persist(train)\n",
    "print(train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()['timestamp']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "# TIME FEATURES\n",
    "# RAPIDS does this 5x faster than Pandas CPU\n",
    "# If we didn't need to copy CPU to GPU to CPU, then 1300x faster!\n",
    "def split_time(df):\n",
    "    #gf = cudf.from_pandas(df[['timestamp']])\n",
    "    df['dt_dow']  = df['timestamp'].dt.weekday#.to_array() \n",
    "    df['dt_hour'] = df['timestamp'].dt.hour#.to_array()\n",
    "    df['dt_minute'] = df['timestamp'].dt.minute#.to_array()\n",
    "    df['dt_second'] = df['timestamp'].dt.second#.to_array()\n",
    "    return df\n",
    "\n",
    "train = split_time(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()[['engage_time','timestamp']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.timestamp.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# ELAPSED TIME\n",
    "for col in ['engage_time','timestamp']:\n",
    "    train[col] = train[col].astype('int64')/1e9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train, = dask.persist(train)\n",
    "print(type(train), train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()[['engage_time','timestamp']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_nan(ds):\n",
    "    mask = ds == 0\n",
    "    ds.loc[mask] = np.nan\n",
    "    return ds\n",
    "train['engage_time'] = train['engage_time'].map_partitions(set_nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['elapsed_time'] = train['engage_time'] - train['timestamp']\n",
    "train['elapsed_time'] = train.elapsed_time.astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train['elapsed_time'].min().compute(),train['elapsed_time'].max().compute())\n",
    "print(train['elapsed_time'].mean().compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# TRAIN FIRST 5 DAYS. VALIDATE LAST 2 DAYS\n",
    "VALID_DOW = [1, 2]# order is [3, 4, 5, 6, 0, 1, 2]\n",
    "valid = train[train['dt_dow'].isin(VALID_DOW)].reset_index(drop=True)\n",
    "train = train[~train['dt_dow'].isin(VALID_DOW)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train,valid = dask.persist(train,valid)\n",
    "print(type(train), train.shape, valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train = train.set_index('timestamp')\n",
    "valid = valid.set_index('timestamp')\n",
    "train,valid = dask.persist(train,valid)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train = train.reset_index()\n",
    "valid = valid.reset_index()\n",
    "train,valid = dask.persist(train,valid)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i,p in enumerate(train.partitions):\n",
    "#    print(i,len(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i,p in enumerate(valid.partitions):\n",
    "#    print(i,len(p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target Encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MTE_one_shot:\n",
    "    \n",
    "    def __init__(self, folds, smooth, seed=42):\n",
    "        self.folds = folds\n",
    "        self.seed = seed\n",
    "        self.smooth = smooth\n",
    "        \n",
    "    def fit_transform(self, train, x_col, y_col, y_mean=None, out_col = None, out_dtype=None):\n",
    "        \n",
    "        self.y_col = y_col\n",
    "        np.random.seed(self.seed)\n",
    "        \n",
    "        if 'fold' not in train.columns:\n",
    "            fsize = len(train)//self.folds\n",
    "            train['fold'] = 1\n",
    "            train['fold'] = train['fold'].cumsum()\n",
    "            train['fold'] = train['fold']//fsize\n",
    "            train['fold'] = train['fold']%self.folds\n",
    "        \n",
    "        if out_col is None:\n",
    "            tag = x_col if isinstance(x_col,str) else '_'.join(x_col)\n",
    "            out_col = f'TE_{tag}_{self.y_col}'\n",
    "        \n",
    "        if y_mean is None:\n",
    "            y_mean = train[y_col].mean()#.compute().astype('float32')\n",
    "        self.mean = y_mean\n",
    "        \n",
    "        cols = ['fold',x_col] if isinstance(x_col,str) else ['fold']+x_col\n",
    "        \n",
    "        agg_each_fold = train.groupby(cols).agg({y_col:['count','sum']}).reset_index()\n",
    "        agg_each_fold.columns = cols + ['count_y','sum_y']\n",
    "        \n",
    "        agg_all = agg_each_fold.groupby(x_col).agg({'count_y':'sum','sum_y':'sum'}).reset_index()\n",
    "        cols = [x_col] if isinstance(x_col,str) else x_col\n",
    "        agg_all.columns = cols + ['count_y_all','sum_y_all']\n",
    "        \n",
    "        agg_each_fold = agg_each_fold.merge(agg_all,on=x_col,how='left')\n",
    "        agg_each_fold['count_y_all'] = agg_each_fold['count_y_all'] - agg_each_fold['count_y']\n",
    "        agg_each_fold['sum_y_all'] = agg_each_fold['sum_y_all'] - agg_each_fold['sum_y']\n",
    "        agg_each_fold[out_col] = (agg_each_fold['sum_y_all']+self.smooth*self.mean)/(agg_each_fold['count_y_all']+self.smooth)\n",
    "        agg_each_fold = agg_each_fold.drop(['count_y_all','count_y','sum_y_all','sum_y'],axis=1)\n",
    "        \n",
    "        agg_all[out_col] = (agg_all['sum_y_all']+self.smooth*self.mean)/(agg_all['count_y_all']+self.smooth)\n",
    "        agg_all = agg_all.drop(['count_y_all','sum_y_all'],axis=1)\n",
    "        self.agg_all = agg_all\n",
    "        \n",
    "        train.columns\n",
    "        cols = ['fold',x_col] if isinstance(x_col,str) else ['fold']+x_col\n",
    "        train = train.merge(agg_each_fold,on=cols,how='left')\n",
    "        del agg_each_fold\n",
    "        #self.agg_each_fold = agg_each_fold\n",
    "        #train[out_col] = train.map_partitions(lambda cudf_df: cudf_df[out_col].nans_to_nulls())\n",
    "        train[out_col] = train[out_col].fillna(self.mean)\n",
    "        \n",
    "        if out_dtype is not None:\n",
    "            train[out_col] = train[out_col].astype(out_dtype)\n",
    "        return train\n",
    "    \n",
    "    def transform(self, test, x_col, out_col = None, out_dtype=None):\n",
    "        if out_col is None:\n",
    "            tag = x_col if isinstance(x_col,str) else '_'.join(x_col)\n",
    "            out_col = f'TE_{tag}_{self.y_col}'\n",
    "        test = test.merge(self.agg_all,on=x_col,how='left')\n",
    "        test[out_col] = test[out_col].fillna(self.mean)\n",
    "        if out_dtype is not None:\n",
    "            test[out_col] = test[out_col].astype(out_dtype)\n",
    "        return test\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TE_media_reply 17.8 seconds<br>\n",
    "TE_tweet_type_reply 27.1 seconds<br>\n",
    "TE_language_reply 52.5 seconds<br>\n",
    "TE_a_user_id_reply 180.0 seconds<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# cuDF TE ENCODING IS SUPER FAST!!\n",
    "idx = 0; cols = []\n",
    "start = time.time()\n",
    "for t in ['reply', 'retweet', 'retweet_comment', 'like']:\n",
    "    start = time.time()\n",
    "    for c in ['media', 'tweet_type', 'language', 'a_user_id', 'b_user_id']:\n",
    "        out_col = f'TE_{c}_{t}'\n",
    "        encoder = MTE_one_shot(folds=5,smooth=20)\n",
    "        train = encoder.fit_transform(train, c, t, out_col=out_col, out_dtype='float32')\n",
    "        valid = encoder.transform(valid, c, out_col=out_col, out_dtype='float32')\n",
    "        cols.append(out_col)\n",
    "        train,valid = dask.persist(train,valid)\n",
    "        del encoder\n",
    "        #train.head()\n",
    "        wait(train)\n",
    "        wait(valid)\n",
    "        print(out_col,\"%.1f seconds\"%(time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['fold'].value_counts().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Column Target Encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# cuDF TE ENCODING IS SUPER FAST!!\n",
    "idx = 0; cols=[]\n",
    "c = ['domains','language','b_follows_a','tweet_type','media','a_is_verified']\n",
    "for t in ['reply', 'retweet', 'retweet_comment', 'like']:\n",
    "    out_col = f'TE_multi_{t}'\n",
    "    encoder = MTE_one_shot(folds=5,smooth=20)\n",
    "    train = encoder.fit_transform(train, c, t, out_col=out_col, out_dtype='float32')\n",
    "    valid = encoder.transform(valid, c, out_col=out_col, out_dtype='float32')\n",
    "    cols.append(out_col)\n",
    "    del encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train,valid = dask.persist(train,valid)\n",
    "wait(train)\n",
    "wait(valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elapsed Time Target Encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# cuDF TE ENCODING IS SUPER FAST!!\n",
    "start = time.time()\n",
    "idx = 0; cols = []\n",
    "for c in ['media', 'tweet_type', 'language']:#, 'a_user_id', 'b_user_id']:\n",
    "    for t in ['elapsed_time']:\n",
    "        out_col = f'TE_{c}_{t}'\n",
    "        encoder = MTE_one_shot(folds=5,smooth=20)\n",
    "        train = encoder.fit_transform(train, c, t, out_col=out_col)\n",
    "        out_dtype='float32' #if 'user_id' in c else None\n",
    "        valid = encoder.transform(valid, c, out_col=out_col, out_dtype=out_dtype)\n",
    "        cols.append(out_col)\n",
    "        print(out_col,\"%.1f seconds\"%(time.time()-start))\n",
    "        #del encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train,valid = dask.persist(train,valid)\n",
    "wait(train)\n",
    "wait(valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrequencyEncoder:\n",
    "    \n",
    "    def __init__(self, seed=42):\n",
    "        self.seed = seed\n",
    "        \n",
    "    def fit_transform(self, train, x_col, c_col=None, out_col = None):\n",
    "        np.random.seed(self.seed)\n",
    "        if c_col is None or c_col not in train.columns:\n",
    "            c_col = 'dummy'\n",
    "            train[c_col] = 1\n",
    "            drop = True\n",
    "        else:\n",
    "            drop = False\n",
    "            \n",
    "        if out_col is None:\n",
    "            tag = x_col if isinstance(x_col,str) else '_'.join(x_col)\n",
    "            out_col = f'CE_{tag}_norm'\n",
    "            \n",
    "        cols = [x_col] if isinstance(x_col,str) else x_col\n",
    "        agg_all = train.groupby(cols).agg({c_col:'count'}).reset_index()\n",
    "        if drop:\n",
    "            train = train.drop(c_col,axis=1)\n",
    "        agg_all.columns = cols + [out_col]\n",
    "        agg_all[out_col] = agg_all[out_col].astype('int32')\n",
    "        agg_all[out_col] = agg_all[out_col]*1.0/len(train)\n",
    "        agg_all[out_col] = agg_all[out_col].astype('float32')\n",
    "    \n",
    "        train = train.merge(agg_all,on=cols,how='left')\n",
    "        del agg_all\n",
    "        #print(train.columns)\n",
    "        #train[out_col] = train.map_partitions(lambda cudf_df: cudf_df[out_col].nans_to_nulls())\n",
    "        return train\n",
    "    \n",
    "    def transform(self, test, x_col, c_col=None, out_col = None):\n",
    "        return self.fit_transform(test, x_col, c_col, out_col)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CountEncoder:\n",
    "    \n",
    "    def __init__(self, seed=42):\n",
    "        self.seed = seed\n",
    "        \n",
    "    def fit_transform(self, train, test, x_col, out_col = None):\n",
    "        np.random.seed(self.seed)\n",
    "        \n",
    "        common_cols = [i for i in train.columns if i in test.columns and i!=x_col]\n",
    "\n",
    "        if len(common_cols):\n",
    "            c_col = common_cols[0]\n",
    "            drop = False\n",
    "        else:\n",
    "            c_col = 'dummy'\n",
    "            train[c_col] = 1\n",
    "            test[c_col]=1\n",
    "            drop = True\n",
    "            \n",
    "        if out_col is None:\n",
    "            tag = x_col if isinstance(x_col,str) else '_'.join(x_col)\n",
    "            out_col = f'CE_{tag}_norm'\n",
    "            \n",
    "        cols = [x_col] if isinstance(x_col,str) else x_col\n",
    "        agg_all = train.groupby(cols).agg({c_col:'count'}).reset_index()\n",
    "        agg_all.columns = cols + [out_col]\n",
    "        \n",
    "        agg_test = test.groupby(cols).agg({c_col:'count'}).reset_index()\n",
    "        agg_test.columns = cols + [out_col+'_test']\n",
    "        agg_all = agg_all.merge(agg_test,on=cols,how='left')\n",
    "        agg_all[out_col+'_test'] = agg_all[out_col+'_test'].fillna(0)\n",
    "        agg_all[out_col] = agg_all[out_col] + agg_all[out_col+'_test']\n",
    "        agg_all = agg_all.drop(out_col+'_test', axis=1)\n",
    "        del agg_test\n",
    "            \n",
    "        if drop:\n",
    "            train = train.drop(c_col,axis=1)\n",
    "            test = test.drop(c_col,axis=1)\n",
    "        train = train.merge(agg_all,on=cols,how='left')\n",
    "        test = test.merge(agg_all,on=cols,how='left')\n",
    "        del agg_all\n",
    "        return train,test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# cuDF CE ENCODING IS SUPER FAST!!\n",
    "start = time.time()\n",
    "idx = 0; cols = []\n",
    "for c in ['media', 'tweet_type', 'language', 'a_user_id', 'b_user_id']:\n",
    "    encoder = CountEncoder()\n",
    "    out_col = f'CE_{c}'\n",
    "    train,valid = encoder.fit_transform(train, valid, c, out_col=out_col)\n",
    "    print\n",
    "    del encoder\n",
    "    train,valid = dask.persist(train,valid)\n",
    "    wait(train)\n",
    "    wait(valid)\n",
    "    print(out_col,\"%.1f seconds\"%(time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# cuDF CE ENCODING IS SUPER FAST!!\n",
    "idx = 0; cols = []\n",
    "start = time.time()\n",
    "for c in ['media', 'tweet_type', 'language', 'a_user_id', 'b_user_id']:\n",
    "    encoder = FrequencyEncoder()\n",
    "    out_col = f'CE_{c}_norm'\n",
    "    train = encoder.fit_transform(train, c, c_col='tweet_id', out_col=out_col)\n",
    "    valid = encoder.transform(valid, c, c_col='tweet_id', out_col=out_col)\n",
    "    cols.append(out_col)\n",
    "    del encoder\n",
    "    train,valid = dask.persist(train,valid)\n",
    "    wait(train)\n",
    "    wait(valid)\n",
    "    print(out_col,\"%.1f seconds\"%(time.time()-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Difference Encode (Lag Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diff_encode_cudf_v1(train,col,tar,sft=1):\n",
    "    train[col+'_sft'] = train[col].shift(sft)\n",
    "    train[tar+'_sft'] = train[tar].shift(sft)\n",
    "    out_col = f'DE_{col}_{tar}_{sft}'\n",
    "    train[out_col] = train[tar]-train[tar+'_sft']\n",
    "    mask = '__MASK__'\n",
    "    train[mask] = train[col] == train[col+'_sft']\n",
    "    train = train.drop([col+'_sft',tar+'_sft'],axis=1)\n",
    "    train[out_col] = train[out_col]*train[mask]\n",
    "    train = train.drop(mask,axis=1)\n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "start = time.time()\n",
    "# cuDF DE ENCODING IS FAST!!\n",
    "idx = 0; cols = []; sc = 'timestamp'\n",
    "for c in ['b_user_id']:\n",
    "    for t in ['b_follower_count','b_following_count','language']:\n",
    "        for s in [1,-1]:\n",
    "            start = time.time()\n",
    "            train = diff_encode_cudf_v1(train, col=c, tar=t, sft=s)\n",
    "            valid = diff_encode_cudf_v1(valid, col=c, tar=t, sft=s)\n",
    "            train,valid = dask.persist(train,valid)\n",
    "            wait(train)\n",
    "            wait(valid)\n",
    "            end = time.time(); idx += 1\n",
    "            print('DE',c,t,s,'%.1f seconds'%(end-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diff Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lang = train[['a_user_id', 'language', 'tweet_id']].drop_duplicates()\n",
    "valid_lang = valid[['a_user_id', 'language', 'tweet_id']].drop_duplicates()\n",
    "train_lang_count = train_lang.groupby(['a_user_id', 'language']).agg({'tweet_id':'count'}).reset_index()\n",
    "valid_lang_count = valid_lang.groupby(['a_user_id', 'language']).agg({'tweet_id':'count'}).reset_index()\n",
    "train_lang_count,valid_lang_count = dask.persist(train_lang_count,valid_lang_count)\n",
    "train_lang_count.head()\n",
    "del train_lang,valid_lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train_lang_count = train_lang_count.merge(valid_lang_count,on=['a_user_id', 'language'],how='left')\n",
    "train_lang_count['tweet_id_y'] = train_lang_count['tweet_id_y'].fillna(0)\n",
    "train_lang_count['tweet_id_x'] = train_lang_count['tweet_id_x'] + train_lang_count['tweet_id_y']\n",
    "train_lang_count = train_lang_count.drop('tweet_id_y',axis=1)\n",
    "train_lang_count.columns = ['a_user_id', 'top_language', 'language_count']\n",
    "train_lang_count, = dask.persist(train_lang_count)\n",
    "train_lang_count.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "train_lang_count = train_lang_count.sort_values(['a_user_id', 'language_count'])\n",
    "train_lang_count['a_user_shifted'] = train_lang_count['a_user_id'].shift(1)\n",
    "train_lang_count = train_lang_count[train_lang_count['a_user_id']!=train_lang_count['a_user_shifted']]\n",
    "train_lang_count = train_lang_count.drop(['a_user_shifted','language_count'],axis=1)\n",
    "train_lang_count.columns = ['a_user_id','top_language']\n",
    "train_lang_count, = dask.persist(train_lang_count)\n",
    "train_lang_count.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diff_language(df,df_lang_count):\n",
    "    df = df.merge(df_lang_count,how='left', left_on='b_user_id', right_on='a_user_id')\n",
    "    df['nan_language'] = df['top_language'].isnull()\n",
    "    df['same_language'] = df['language'] == df['top_language']\n",
    "    df['diff_language'] = df['language'] != df['top_language']\n",
    "    df['same_language'] = df['same_language']*(1-df['nan_language'])\n",
    "    df['diff_language'] = df['diff_language']*(1-df['nan_language'])\n",
    "    df = df.drop('top_language',axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "#train = diff_language(train,train_lang_count)\n",
    "#valid = diff_language(valid,train_lang_count)\n",
    "#train,valid = dask.persist(train,valid)\n",
    "#train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rate feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# follow rate feature\n",
    "train['a_ff_rate'] = (train['a_following_count'] / train['a_follower_count']).astype('float32')\n",
    "train['b_ff_rate'] = (train['b_follower_count']  / train['b_following_count']).astype('float32')\n",
    "valid['a_ff_rate']  = (valid['a_following_count'] / valid['a_follower_count']).astype('float32')\n",
    "valid['b_ff_rate']  = (valid['b_follower_count']  / valid['b_following_count']).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train,valid = dask.persist(train,valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wait(train)\n",
    "wait(valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarize Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "label_names = ['reply', 'retweet', 'retweet_comment', 'like']\n",
    "DONT_USE = ['tweet_id','timestamp','a_account_creation','b_account_creation','engage_time',\n",
    "            'fold','b_user_id','a_user_id', 'dt_dow',\n",
    "            'a_account_creation', 'b_account_creation', 'elapsed_time',\n",
    "             'links','domains','hashtags0','hashtags1']\n",
    "DONT_USE += label_names\n",
    "features = [c for c in train.columns if c not in DONT_USE]\n",
    "\n",
    "RMV = [c for c in DONT_USE if c in train.columns and c not in label_names]\n",
    "RMV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "for col in RMV:\n",
    "    #print(col, col in train.columns)\n",
    "    if col in train.columns:\n",
    "        train = train.drop(col,axis=1)\n",
    "        train, = dask.persist(train)\n",
    "        train.head()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.dtypes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "for col in RMV:\n",
    "    #print(col, col in valid.columns)\n",
    "    if col in valid.columns:\n",
    "        valid = valid.drop(col,axis=1)\n",
    "        valid, = dask.persist(valid,)\n",
    "        valid.head()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model Validate\n",
    "We will train on random 10% of first 5 days and validation on last 2 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "SAMPLE_RATIO = 0.1\n",
    "SEED = 1\n",
    "\n",
    "if SAMPLE_RATIO < 1.0:\n",
    "    print(len(train))\n",
    "    train = train.sample(frac=SAMPLE_RATIO,random_state=42)\n",
    "    train, = dask.persist(train)\n",
    "    train.head()\n",
    "    print(len(train))\n",
    "\n",
    "train = train.compute()\n",
    "Y_train = train[label_names]\n",
    "train = train.drop(label_names,axis=1)\n",
    "\n",
    "features = [c for c in train.columns if c not in DONT_USE]\n",
    "print('Using %i features:'%(len(features)),train.shape[1])\n",
    "np.asarray(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_RATIO = 0.35 # VAL SET NOW SIZE OF TEST SET\n",
    "SEED = 1\n",
    "if SAMPLE_RATIO < 1.0:\n",
    "    print(len(valid))\n",
    "    valid = valid.sample(frac=SAMPLE_RATIO,random_state=42)\n",
    "    valid, = dask.persist(valid)\n",
    "    valid.head()\n",
    "    print(len(valid))\n",
    "    \n",
    "valid = valid.compute()\n",
    "Y_valid = valid[label_names]\n",
    "valid = valid.drop(label_names,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "print('XGB Version',xgb.__version__)\n",
    "\n",
    "xgb_parms = { \n",
    "    'max_depth':8, \n",
    "    'learning_rate':0.1, \n",
    "    'subsample':0.8,\n",
    "    'colsample_bytree':0.3, \n",
    "    'eval_metric':'logloss',\n",
    "    'objective':'binary:logistic',\n",
    "    'nthread':40,\n",
    "    'tree_method':'hist',\n",
    "    #'predictor' : 'gpu_predictor'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train.columns.duplicated().sum()>0:\n",
    "    raise Exception(f'duplicated!: { train.columns[train.columns.duplicated()] }')\n",
    "print('no dup :) ')\n",
    "print(f'X_train.shape {train.shape}')\n",
    "print(f'X_valid.shape {valid.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.dtypes"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "for col in train.columns:\n",
    "    if train[col].dtype=='bool':\n",
    "        train[col] = train[col].astype('int8')\n",
    "        valid[col] = valid[col].astype('int8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# TRAIN AND VALIDATE\n",
    "\n",
    "NROUND = 300\n",
    "VERBOSE_EVAL = 50\n",
    "#ESR = 50\n",
    "    \n",
    "oof = np.zeros((len(valid),len(label_names)))\n",
    "preds = []\n",
    "for i in range(4):\n",
    "\n",
    "    name = label_names[i]\n",
    "    print('#'*25);print('###',name);print('#'*25)\n",
    "       \n",
    "    start = time.time(); print('Creating DMatrix...')\n",
    "        \n",
    "    dtrain = xgb.DMatrix(data=train,label=Y_train.iloc[:, i])\n",
    "    dvalid = xgb.DMatrix(data=valid,label=Y_valid.iloc[:, i])\n",
    "    print('Took %.1f seconds'%(time.time()-start))\n",
    "             \n",
    "    start = time.time(); print('Training...')\n",
    "    model = xgb.train(xgb_parms, \n",
    "                           dtrain=dtrain,\n",
    "                           #evals=[(dtrain,'train'),(dvalid,'valid')],\n",
    "                           num_boost_round=NROUND,\n",
    "                           #early_stopping_rounds=ESR,\n",
    "                           verbose_eval=VERBOSE_EVAL) \n",
    "    print('Took %.1f seconds'%(time.time()-start))\n",
    "        \n",
    "    start = time.time(); print('Predicting...')\n",
    "    #Y_valid[f'pred_{name}'] = xgb.dask.predict(client,model,valid)\n",
    "    oof[:, i] += model.predict(dvalid)\n",
    "    #preds.append(xgb.dask.predict(client,model,valid))\n",
    "    print('Took %.1f seconds'%(time.time()-start))\n",
    "        \n",
    "    del model, dtrain, dvalid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yvalid = Y_valid[label_names].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Validation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve, auc, log_loss\n",
    "\n",
    "def compute_prauc(pred, gt):\n",
    "  prec, recall, thresh = precision_recall_curve(gt, pred)\n",
    "  prauc = auc(recall, prec)\n",
    "  return prauc\n",
    "\n",
    "def calculate_ctr(gt):\n",
    "  positive = len([x for x in gt if x == 1])\n",
    "  ctr = positive/float(len(gt))\n",
    "  return ctr\n",
    "\n",
    "def compute_rce(pred, gt):\n",
    "    cross_entropy = log_loss(gt, pred)\n",
    "    data_ctr = calculate_ctr(gt)\n",
    "    strawman_cross_entropy = log_loss(gt, [data_ctr for _ in range(len(gt))])\n",
    "    return (1.0 - cross_entropy/strawman_cross_entropy)*100.0\n",
    "\n",
    "# FAST METRIC FROM GIBA\n",
    "def compute_rce_fast(pred, gt):\n",
    "    cross_entropy = log_loss(gt, pred)\n",
    "    yt = np.mean(gt)     \n",
    "    strawman_cross_entropy = -(yt*np.log(yt) + (1 - yt)*np.log(1 - yt))\n",
    "    return (1.0 - cross_entropy/strawman_cross_entropy)*100.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "txt = ''\n",
    "for i in range(4):\n",
    "    prauc = compute_prauc(oof[:,i], yvalid[:, i])\n",
    "    rce   = compute_rce_fast(oof[:,i], yvalid[:, i])\n",
    "    txt_ = f\"{label_names[i]:20} PRAUC:{prauc:.5f} RCE:{rce:.5f}\"\n",
    "    print(txt_)\n",
    "    txt += txt_ + '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('This notebook took %.1f minutes'%((time.time()-very_start)/60.))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
