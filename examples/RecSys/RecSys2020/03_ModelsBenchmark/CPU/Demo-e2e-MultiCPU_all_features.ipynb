{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) 2020, NVIDIA CORPORATION.\n",
    "\n",
    "Modifications copyright Intel. \n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "    http://www.apache.org/licenses/LICENSE-2.0\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Added missing featrues to use dataset from pre-process "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time\n",
    "start = time.time()\n",
    "very_start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_rows', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "from dask.distributed import Client, wait, LocalCluster\n",
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xianyang/sw/miniconda3/lib/python3.7/site-packages/distributed/node.py:155: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 40571 instead\n",
      "  http_address[\"port\"], self.http_server.port\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://10.1.0.131:35366</li>\n",
       "  <li><b>Dashboard: </b><a href='http://10.1.0.131:40571/status' target='_blank'>http://10.1.0.131:40571/status</a></li>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>8</li>\n",
       "  <li><b>Cores: </b>40</li>\n",
       "  <li><b>Memory: </b>1.60 TB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://10.1.0.131:35366' processes=8 threads=40, memory=1.60 TB>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "client = Client(n_workers=8, \n",
    "                       threads_per_worker=5,\n",
    "                       memory_limit='200GB',ip='10.1.0.131')\n",
    "#client = Client(ip='10.2.48.253',memory_limit='100GB')\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 64.6 ms, sys: 23.1 ms, total: 87.7 ms\n",
      "Wall time: 74 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "path = '/mnt/DP_disk3/Recsys/nv'\n",
    "### train = dd.read_parquet(f'{path}/train-preproc-fold-*.parquet')#,dtypes=dtypes)\n",
    "### Notes: revious scripts cannot generate this dataset. \n",
    "### original tested was downloaded from here:  \n",
    "###  https://github.com/rapidsai/dask-cuda/issues/337 \n",
    "\n",
    "train = dd.read_parquet('/mnt/DP_disk3/Recsys/train-1.parquet')\n",
    "test0 = dd.read_parquet('/mnt/DP_disk3/Recsys/test-0.parquet')\n",
    "test1 = dd.read_parquet('/mnt/DP_disk3/Recsys/test-1.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((Delayed('int-a90eb658-b2d7-4168-bdaf-76220f5dde41'), 27),\n",
       " (Delayed('int-cb398baf-8b19-4a9e-9e7d-294f54be7df3'), 27),\n",
       " (Delayed('int-472ad2f0-1e79-4159-bebc-eb8a042a3fe4'), 27))"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, test0.shape, test1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.1 ms, sys: 2.08 ms, total: 12.2 ms\n",
      "Wall time: 9.69 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# DROP UNUSED COLUMNS\n",
    "#cols_drop = ['links','hashtags0', 'hashtags1', 'fold']\n",
    "cols_drop = ['links','hashtags']\n",
    "\n",
    "train = train.drop(cols_drop,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dask.dataframe.core.DataFrame'> (Delayed('int-714f9e31-4fcf-4246-a16d-c7d2c655ef9e'), 25)\n",
      "CPU times: user 3.96 ms, sys: 0 ns, total: 3.96 ms\n",
      "Wall time: 3.51 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train, = dask.persist(train)\n",
    "print(type(train), train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dask.dataframe.core.DataFrame'> (Delayed('int-22c99e2d-881e-4dbb-a480-2041fa6fcb8e'), 25)\n",
      "CPU times: user 3.89 ms, sys: 2.95 ms, total: 6.84 ms\n",
      "Wall time: 5.19 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train = train.repartition(npartitions=8)\n",
    "train, = dask.persist(train)\n",
    "print(type(train), train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i,p in enumerate(train.partitions):\n",
    "#    print(i,len(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names = ['reply', 'retweet', 'retweet_comment', 'like']\n",
    "for col in train.columns:\n",
    "    if col in label_names:\n",
    "        train[col] = train[col].astype('int64')\n",
    "    elif train[col].dtype=='int64':\n",
    "        train[col] = train[col].astype('int32')\n",
    "    elif train[col].dtype=='int16':\n",
    "        train[col] = train[col].astype('int8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gap: int32 -> uint32; tweet_id: unit32, links uint32, domains uint32; timestamp: datetime64[ns]\n",
    "len_* -> uint8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.02 ms, sys: 5.97 ms, total: 11 ms\n",
      "Wall time: 7.46 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train = train.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Delayed('int-e45f1bb5-bba2-4233-a89e-f012f06341af'), 25)\n",
      "CPU times: user 10.9 ms, sys: 102 µs, total: 11 ms\n",
      "Wall time: 8.59 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train, = dask.persist(train)\n",
    "print(train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color = red> Newly add code </font>\n",
    "\n",
    "## Caculate Engage time & Elapsed Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13.6 ms, sys: 39 µs, total: 13.6 ms\n",
      "Wall time: 10.7 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "## New Add, convert timestamp to timestamp64 for split_time to run \n",
    "\n",
    "label_names = [\"reply\", \"retweet\", \"retweet_comment\", \"like\"]\n",
    "\n",
    "train['timestamp'] = dd.to_datetime(train['timestamp'], unit=\"s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train.head()['timestamp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 41.3 ms, sys: 6.95 ms, total: 48.3 ms\n",
      "Wall time: 39.7 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "# TIME FEATURES\n",
    "# RAPIDS does this 5x faster than Pandas CPU\n",
    "# If we didn't need to copy CPU to GPU to CPU, then 1300x faster!\n",
    "def split_time(df):\n",
    "    #gf = cudf.from_pandas(df[['timestamp']])\n",
    "    df['dt_dow']  = df['timestamp'].dt.weekday#.to_array() \n",
    "    df['dt_hour'] = df['timestamp'].dt.hour#.to_array()\n",
    "    df['dt_minute'] = df['timestamp'].dt.minute#.to_array()\n",
    "    df['dt_second'] = df['timestamp'].dt.second#.to_array()\n",
    "    return df\n",
    "\n",
    "train = split_time(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"timestamp\"] = train[\"timestamp\"].astype(\"int64\") / 1e9\n",
    "train[\"reply\"] = train.reply.mask(train[\"reply\"] == 0.0, np.nan)\n",
    "train[\"retweet\"] = train.retweet.mask(train['retweet'] == 0.0, np.nan)\n",
    "train[\"retweet_comment\"] = train.retweet_comment.mask(train['retweet_comment'] == 0.0, np.nan)\n",
    "train[\"like\"] = train.like.mask(train['like'] == 0.0, np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train.head(1000)[['reply','retweet','retweet_comment','like']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"engage_time\"] = train[label_names].min(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train.head()[[\"engage_time\",\"timestamp\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"elapsed_time\"] = train[\"engage_time\"] - train[\"timestamp\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train.head()[['elapsed_time','engage_time','timestamp']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check this... NaN -> False * 1 -> 0, others -> Ture * 1 -> 1\n",
    "train[label_names] = (train[label_names] > 0) * 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train.head()[['reply','retweet','retweet_comment','like']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train['a_account_creation'] = dd.to_datetime(train['a_account_creation'], unit=\"s\")\n",
    "#train['b_account_creation'] = dd.to_datetime(train['b_account_creation'], unit=\"s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def set_nan(ds):\n",
    "    mask = ds == 0\n",
    "    ds.loc[mask] = np.nan\n",
    "    return ds\n",
    "train['engage_time'] = train['engage_time'].map_partitions(set_nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train['elapsed_time'] = train['engage_time'] - train['timestamp']\n",
    "train['elapsed_time'] = train.elapsed_time.astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0 602391.0\n",
      "16617.73330803138\n"
     ]
    }
   ],
   "source": [
    "print(train['elapsed_time'].min().compute(),train['elapsed_time'].max().compute())\n",
    "print(train['elapsed_time'].mean().compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 26.6 ms, sys: 26.2 ms, total: 52.7 ms\n",
      "Wall time: 46.2 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# TRAIN FIRST 5 DAYS. VALIDATE LAST 2 DAYS\n",
    "VALID_DOW = [1, 2]# order is [3, 4, 5, 6, 0, 1, 2]\n",
    "valid = train[train['dt_dow'].isin(VALID_DOW)].reset_index(drop=True)\n",
    "train = train[~train['dt_dow'].isin(VALID_DOW)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dask.dataframe.core.DataFrame'> (Delayed('int-769b7b3c-363f-4af5-a399-a5f9a31d729b'), 31) (Delayed('int-418bdee4-3396-45c4-8dcc-c4924758dfe5'), 31)\n",
      "CPU times: user 26.4 ms, sys: 2.05 ms, total: 28.5 ms\n",
      "Wall time: 25.3 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train,valid = dask.persist(train,valid)\n",
    "print(type(train), train.shape, valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14.3 s, sys: 2.18 s, total: 16.5 s\n",
      "Wall time: 1min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train = train.set_index('timestamp')\n",
    "valid = valid.set_index('timestamp')\n",
    "train,valid = dask.persist(train,valid)\n",
    "#train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 39.3 ms, sys: 11 ms, total: 50.2 ms\n",
      "Wall time: 42 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train = train.reset_index()\n",
    "valid = valid.reset_index()\n",
    "train,valid = dask.persist(train,valid)\n",
    "#train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i,p in enumerate(train.partitions):\n",
    "#    print(i,len(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i,p in enumerate(valid.partitions):\n",
    "#    print(i,len(p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target Encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MTE_one_shot:\n",
    "    \n",
    "    def __init__(self, folds, smooth, seed=42):\n",
    "        self.folds = folds\n",
    "        self.seed = seed\n",
    "        self.smooth = smooth\n",
    "        \n",
    "    def fit_transform(self, train, x_col, y_col, y_mean=None, out_col = None, out_dtype=None):\n",
    "        \n",
    "        self.y_col = y_col\n",
    "        np.random.seed(self.seed)\n",
    "        \n",
    "        if 'fold' not in train.columns:\n",
    "            fsize = len(train)//self.folds\n",
    "            train['fold'] = 1\n",
    "            train['fold'] = train['fold'].cumsum()\n",
    "            train['fold'] = train['fold']//fsize\n",
    "            train['fold'] = train['fold']%self.folds\n",
    "        \n",
    "        if out_col is None:\n",
    "            tag = x_col if isinstance(x_col,str) else '_'.join(x_col)\n",
    "            out_col = f'TE_{tag}_{self.y_col}'\n",
    "        \n",
    "        if y_mean is None:\n",
    "            y_mean = train[y_col].mean()#.compute().astype('float32')\n",
    "        self.mean = y_mean\n",
    "        \n",
    "        cols = ['fold',x_col] if isinstance(x_col,str) else ['fold']+x_col\n",
    "        \n",
    "        agg_each_fold = train.groupby(cols).agg({y_col:['count','sum']}).reset_index()\n",
    "        agg_each_fold.columns = cols + ['count_y','sum_y']\n",
    "        \n",
    "        agg_all = agg_each_fold.groupby(x_col).agg({'count_y':'sum','sum_y':'sum'}).reset_index()\n",
    "        cols = [x_col] if isinstance(x_col,str) else x_col\n",
    "        agg_all.columns = cols + ['count_y_all','sum_y_all']\n",
    "        \n",
    "        agg_each_fold = agg_each_fold.merge(agg_all,on=x_col,how='left')\n",
    "        agg_each_fold['count_y_all'] = agg_each_fold['count_y_all'] - agg_each_fold['count_y']\n",
    "        agg_each_fold['sum_y_all'] = agg_each_fold['sum_y_all'] - agg_each_fold['sum_y']\n",
    "        agg_each_fold[out_col] = (agg_each_fold['sum_y_all']+self.smooth*self.mean)/(agg_each_fold['count_y_all']+self.smooth)\n",
    "        agg_each_fold = agg_each_fold.drop(['count_y_all','count_y','sum_y_all','sum_y'],axis=1)\n",
    "        \n",
    "        agg_all[out_col] = (agg_all['sum_y_all']+self.smooth*self.mean)/(agg_all['count_y_all']+self.smooth)\n",
    "        agg_all = agg_all.drop(['count_y_all','sum_y_all'],axis=1)\n",
    "        self.agg_all = agg_all\n",
    "        \n",
    "        train.columns\n",
    "        cols = ['fold',x_col] if isinstance(x_col,str) else ['fold']+x_col\n",
    "        train = train.merge(agg_each_fold,on=cols,how='left')\n",
    "        del agg_each_fold\n",
    "        #self.agg_each_fold = agg_each_fold\n",
    "        #train[out_col] = train.map_partitions(lambda cudf_df: cudf_df[out_col].nans_to_nulls())\n",
    "        train[out_col] = train[out_col].fillna(self.mean)\n",
    "        \n",
    "        if out_dtype is not None:\n",
    "            train[out_col] = train[out_col].astype(out_dtype)\n",
    "        return train\n",
    "    \n",
    "    def transform(self, test, x_col, out_col = None, out_dtype=None):\n",
    "        if out_col is None:\n",
    "            tag = x_col if isinstance(x_col,str) else '_'.join(x_col)\n",
    "            out_col = f'TE_{tag}_{self.y_col}'\n",
    "        test = test.merge(self.agg_all,on=x_col,how='left')\n",
    "        test[out_col] = test[out_col].fillna(self.mean)\n",
    "        if out_dtype is not None:\n",
    "            test[out_col] = test[out_col].astype(out_dtype)\n",
    "        return test\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TE_media_reply 17.8 seconds<br>\n",
    "TE_tweet_type_reply 27.1 seconds<br>\n",
    "TE_language_reply 52.5 seconds<br>\n",
    "TE_a_user_id_reply 180.0 seconds<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TE_media_reply 88.8 seconds\n",
      "TE_tweet_type_reply 115.7 seconds\n",
      "TE_language_reply 143.4 seconds\n",
      "TE_a_user_id_reply 205.2 seconds\n",
      "TE_b_user_id_reply 331.9 seconds\n",
      "TE_media_retweet 29.8 seconds\n",
      "TE_tweet_type_retweet 59.3 seconds\n",
      "TE_language_retweet 87.5 seconds\n",
      "TE_a_user_id_retweet 150.9 seconds\n",
      "TE_b_user_id_retweet 277.5 seconds\n",
      "TE_media_retweet_comment 28.9 seconds\n",
      "TE_tweet_type_retweet_comment 58.6 seconds\n",
      "TE_language_retweet_comment 87.4 seconds\n",
      "TE_a_user_id_retweet_comment 150.6 seconds\n",
      "TE_b_user_id_retweet_comment 283.0 seconds\n",
      "TE_media_like 30.2 seconds\n",
      "TE_tweet_type_like 60.2 seconds\n",
      "TE_language_like 90.1 seconds\n",
      "TE_a_user_id_like 155.5 seconds\n",
      "TE_b_user_id_like 282.4 seconds\n",
      "CPU times: user 4min 9s, sys: 35.9 s, total: 4min 45s\n",
      "Wall time: 19min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# cuDF TE ENCODING IS SUPER FAST!!\n",
    "idx = 0; cols = []\n",
    "start = time.time()\n",
    "for t in ['reply', 'retweet', 'retweet_comment', 'like']:\n",
    "    start = time.time()\n",
    "    for c in ['media', 'tweet_type', 'language', 'a_user_id', 'b_user_id']:\n",
    "        out_col = f'TE_{c}_{t}'\n",
    "        encoder = MTE_one_shot(folds=5,smooth=20)\n",
    "        train = encoder.fit_transform(train, c, t, out_col=out_col, out_dtype='float32')\n",
    "        valid = encoder.transform(valid, c, out_col=out_col, out_dtype='float32')\n",
    "        cols.append(out_col)\n",
    "        train,valid = dask.persist(train,valid)\n",
    "        del encoder\n",
    "        #train.head()\n",
    "        wait(train)\n",
    "        wait(valid)\n",
    "        print(out_col,\"%.1f seconds\"%(time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    13979362\n",
       "4    13979361\n",
       "3    13979361\n",
       "2    13979361\n",
       "1    13979361\n",
       "Name: fold, dtype: int64"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['fold'].value_counts().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Column Target Encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 785 ms, sys: 118 ms, total: 903 ms\n",
      "Wall time: 773 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# cuDF TE ENCODING IS SUPER FAST!!\n",
    "idx = 0; cols=[]\n",
    "c = ['domains','language','b_follows_a','tweet_type','media','a_is_verified']\n",
    "for t in ['reply', 'retweet', 'retweet_comment', 'like']:\n",
    "    out_col = f'TE_multi_{t}'\n",
    "    encoder = MTE_one_shot(folds=5,smooth=20)\n",
    "    train = encoder.fit_transform(train, c, t, out_col=out_col, out_dtype='float32')\n",
    "    valid = encoder.transform(valid, c, out_col=out_col, out_dtype='float32')\n",
    "    cols.append(out_col)\n",
    "    del encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 30.9 s, sys: 4.63 s, total: 35.5 s\n",
      "Wall time: 2min 29s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DoneAndNotDoneFutures(done={<Future: finished, type: pandas.DataFrame, key: ('assign-002601277f4712d771c72c38e9d5dd5e', 4)>, <Future: finished, type: pandas.DataFrame, key: ('assign-002601277f4712d771c72c38e9d5dd5e', 2)>, <Future: finished, type: pandas.DataFrame, key: ('assign-002601277f4712d771c72c38e9d5dd5e', 0)>, <Future: finished, type: pandas.DataFrame, key: ('assign-002601277f4712d771c72c38e9d5dd5e', 5)>, <Future: finished, type: pandas.DataFrame, key: ('assign-002601277f4712d771c72c38e9d5dd5e', 7)>, <Future: finished, type: pandas.DataFrame, key: ('assign-002601277f4712d771c72c38e9d5dd5e', 1)>, <Future: finished, type: pandas.DataFrame, key: ('assign-002601277f4712d771c72c38e9d5dd5e', 6)>, <Future: finished, type: pandas.DataFrame, key: ('assign-002601277f4712d771c72c38e9d5dd5e', 3)>}, not_done=set())"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "train,valid = dask.persist(train,valid)\n",
    "wait(train)\n",
    "wait(valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elapsed Time Target Encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TE_media_elapsed_time 0.2 seconds\n",
      "TE_tweet_type_elapsed_time 0.3 seconds\n",
      "TE_language_elapsed_time 0.5 seconds\n",
      "CPU times: user 474 ms, sys: 60.5 ms, total: 534 ms\n",
      "Wall time: 476 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# cuDF TE ENCODING IS SUPER FAST!!\n",
    "start = time.time()\n",
    "idx = 0; cols = []\n",
    "for c in ['media', 'tweet_type', 'language']:#, 'a_user_id', 'b_user_id']:\n",
    "    for t in ['elapsed_time']:\n",
    "        out_col = f'TE_{c}_{t}'\n",
    "        encoder = MTE_one_shot(folds=5,smooth=20)\n",
    "        train = encoder.fit_transform(train, c, t, out_col=out_col)\n",
    "        out_dtype='float32' #if 'user_id' in c else None\n",
    "        valid = encoder.transform(valid, c, out_col=out_col, out_dtype=out_dtype)\n",
    "        cols.append(out_col)\n",
    "        print(out_col,\"%.1f seconds\"%(time.time()-start))\n",
    "        #del encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 23.6 s, sys: 3.49 s, total: 27.1 s\n",
      "Wall time: 2min 10s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DoneAndNotDoneFutures(done={<Future: finished, type: pandas.DataFrame, key: ('assign-6bbb7039e55b979f93eb27b3935fb335', 3)>, <Future: finished, type: pandas.DataFrame, key: ('assign-6bbb7039e55b979f93eb27b3935fb335', 5)>, <Future: finished, type: pandas.DataFrame, key: ('assign-6bbb7039e55b979f93eb27b3935fb335', 0)>, <Future: finished, type: pandas.DataFrame, key: ('assign-6bbb7039e55b979f93eb27b3935fb335', 4)>, <Future: finished, type: pandas.DataFrame, key: ('assign-6bbb7039e55b979f93eb27b3935fb335', 1)>, <Future: finished, type: pandas.DataFrame, key: ('assign-6bbb7039e55b979f93eb27b3935fb335', 2)>, <Future: finished, type: pandas.DataFrame, key: ('assign-6bbb7039e55b979f93eb27b3935fb335', 7)>, <Future: finished, type: pandas.DataFrame, key: ('assign-6bbb7039e55b979f93eb27b3935fb335', 6)>}, not_done=set())"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "train,valid = dask.persist(train,valid)\n",
    "wait(train)\n",
    "wait(valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrequencyEncoder:\n",
    "    \n",
    "    def __init__(self, seed=42):\n",
    "        self.seed = seed\n",
    "        \n",
    "    def fit_transform(self, train, x_col, c_col=None, out_col = None):\n",
    "        np.random.seed(self.seed)\n",
    "        if c_col is None or c_col not in train.columns:\n",
    "            c_col = 'dummy'\n",
    "            train[c_col] = 1\n",
    "            drop = True\n",
    "        else:\n",
    "            drop = False\n",
    "            \n",
    "        if out_col is None:\n",
    "            tag = x_col if isinstance(x_col,str) else '_'.join(x_col)\n",
    "            out_col = f'CE_{tag}_norm'\n",
    "            \n",
    "        cols = [x_col] if isinstance(x_col,str) else x_col\n",
    "        agg_all = train.groupby(cols).agg({c_col:'count'}).reset_index()\n",
    "        if drop:\n",
    "            train = train.drop(c_col,axis=1)\n",
    "        agg_all.columns = cols + [out_col]\n",
    "        agg_all[out_col] = agg_all[out_col].astype('int32')\n",
    "        agg_all[out_col] = agg_all[out_col]*1.0/len(train)\n",
    "        agg_all[out_col] = agg_all[out_col].astype('float32')\n",
    "    \n",
    "        train = train.merge(agg_all,on=cols,how='left')\n",
    "        del agg_all\n",
    "        #print(train.columns)\n",
    "        #train[out_col] = train.map_partitions(lambda cudf_df: cudf_df[out_col].nans_to_nulls())\n",
    "        return train\n",
    "    \n",
    "    def transform(self, test, x_col, c_col=None, out_col = None):\n",
    "        return self.fit_transform(test, x_col, c_col, out_col)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CountEncoder:\n",
    "    \n",
    "    def __init__(self, seed=42):\n",
    "        self.seed = seed\n",
    "        \n",
    "    def fit_transform(self, train, test, x_col, out_col = None):\n",
    "        np.random.seed(self.seed)\n",
    "        \n",
    "        common_cols = [i for i in train.columns if i in test.columns and i!=x_col]\n",
    "\n",
    "        if len(common_cols):\n",
    "            c_col = common_cols[0]\n",
    "            drop = False\n",
    "        else:\n",
    "            c_col = 'dummy'\n",
    "            train[c_col] = 1\n",
    "            test[c_col]=1\n",
    "            drop = True\n",
    "            \n",
    "        if out_col is None:\n",
    "            tag = x_col if isinstance(x_col,str) else '_'.join(x_col)\n",
    "            out_col = f'CE_{tag}_norm'\n",
    "            \n",
    "        cols = [x_col] if isinstance(x_col,str) else x_col\n",
    "        agg_all = train.groupby(cols).agg({c_col:'count'}).reset_index()\n",
    "        agg_all.columns = cols + [out_col]\n",
    "        \n",
    "        agg_test = test.groupby(cols).agg({c_col:'count'}).reset_index()\n",
    "        agg_test.columns = cols + [out_col+'_test']\n",
    "        agg_all = agg_all.merge(agg_test,on=cols,how='left')\n",
    "        agg_all[out_col+'_test'] = agg_all[out_col+'_test'].fillna(0)\n",
    "        agg_all[out_col] = agg_all[out_col] + agg_all[out_col+'_test']\n",
    "        agg_all = agg_all.drop(out_col+'_test', axis=1)\n",
    "        del agg_test\n",
    "            \n",
    "        if drop:\n",
    "            train = train.drop(c_col,axis=1)\n",
    "            test = test.drop(c_col,axis=1)\n",
    "        train = train.merge(agg_all,on=cols,how='left')\n",
    "        test = test.merge(agg_all,on=cols,how='left')\n",
    "        del agg_all\n",
    "        return train,test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CE_media 31.1 seconds\n",
      "CE_tweet_type 67.0 seconds\n",
      "CE_language 107.1 seconds\n",
      "CE_a_user_id 164.3 seconds\n",
      "CE_b_user_id 245.4 seconds\n",
      "CPU times: user 50.1 s, sys: 6.86 s, total: 56.9 s\n",
      "Wall time: 4min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# cuDF CE ENCODING IS SUPER FAST!!\n",
    "start = time.time()\n",
    "idx = 0; cols = []\n",
    "for c in ['media', 'tweet_type', 'language', 'a_user_id', 'b_user_id']:\n",
    "    encoder = CountEncoder()\n",
    "    out_col = f'CE_{c}'\n",
    "    train,valid = encoder.fit_transform(train, valid, c, out_col=out_col)\n",
    "    print\n",
    "    del encoder\n",
    "    train,valid = dask.persist(train,valid)\n",
    "    wait(train)\n",
    "    wait(valid)\n",
    "    print(out_col,\"%.1f seconds\"%(time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CE_media_norm 54.5 seconds\n",
      "CE_tweet_type_norm 99.9 seconds\n",
      "CE_language_norm 147.9 seconds\n",
      "CE_a_user_id_norm 204.6 seconds\n",
      "CE_b_user_id_norm 272.4 seconds\n",
      "CPU times: user 55.1 s, sys: 7.71 s, total: 1min 2s\n",
      "Wall time: 4min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# cuDF CE ENCODING IS SUPER FAST!!\n",
    "idx = 0; cols = []\n",
    "start = time.time()\n",
    "for c in ['media', 'tweet_type', 'language', 'a_user_id', 'b_user_id']:\n",
    "    encoder = FrequencyEncoder()\n",
    "    out_col = f'CE_{c}_norm'\n",
    "    train = encoder.fit_transform(train, c, c_col='tweet_id', out_col=out_col)\n",
    "    valid = encoder.transform(valid, c, c_col='tweet_id', out_col=out_col)\n",
    "    cols.append(out_col)\n",
    "    del encoder\n",
    "    train,valid = dask.persist(train,valid)\n",
    "    wait(train)\n",
    "    wait(valid)\n",
    "    print(out_col,\"%.1f seconds\"%(time.time()-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Difference Encode (Lag Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diff_encode_cudf_v1(train,col,tar,sft=1):\n",
    "    train[col+'_sft'] = train[col].shift(sft)\n",
    "    train[tar+'_sft'] = train[tar].shift(sft)\n",
    "    out_col = f'DE_{col}_{tar}_{sft}'\n",
    "    train[out_col] = train[tar]-train[tar+'_sft']\n",
    "    mask = '__MASK__'\n",
    "    train[mask] = train[col] == train[col+'_sft']\n",
    "    train = train.drop([col+'_sft',tar+'_sft'],axis=1)\n",
    "    train[out_col] = train[out_col]*train[mask]\n",
    "    train = train.drop(mask,axis=1)\n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DE b_user_id b_follower_count 1 31.1 seconds\n",
      "DE b_user_id b_follower_count -1 24.9 seconds\n",
      "DE b_user_id b_following_count 1 25.9 seconds\n",
      "DE b_user_id b_following_count -1 26.1 seconds\n",
      "DE b_user_id language 1 26.7 seconds\n",
      "DE b_user_id language -1 29.7 seconds\n",
      "CPU times: user 29 s, sys: 4.42 s, total: 33.5 s\n",
      "Wall time: 2min 44s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "start = time.time()\n",
    "# cuDF DE ENCODING IS FAST!!\n",
    "idx = 0; cols = []; sc = 'timestamp'\n",
    "for c in ['b_user_id']:\n",
    "    for t in ['b_follower_count','b_following_count','language']:\n",
    "        for s in [1,-1]:\n",
    "            start = time.time()\n",
    "            train = diff_encode_cudf_v1(train, col=c, tar=t, sft=s)\n",
    "            valid = diff_encode_cudf_v1(valid, col=c, tar=t, sft=s)\n",
    "            train,valid = dask.persist(train,valid)\n",
    "            wait(train)\n",
    "            wait(valid)\n",
    "            end = time.time(); idx += 1\n",
    "            print('DE',c,t,s,'%.1f seconds'%(end-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diff Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lang = train[['a_user_id', 'language', 'tweet_id']].drop_duplicates()\n",
    "valid_lang = valid[['a_user_id', 'language', 'tweet_id']].drop_duplicates()\n",
    "train_lang_count = train_lang.groupby(['a_user_id', 'language']).agg({'tweet_id':'count'}).reset_index()\n",
    "valid_lang_count = valid_lang.groupby(['a_user_id', 'language']).agg({'tweet_id':'count'}).reset_index()\n",
    "train_lang_count,valid_lang_count = dask.persist(train_lang_count,valid_lang_count)\n",
    "train_lang_count.head()\n",
    "del train_lang,valid_lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.1 s, sys: 143 ms, total: 1.24 s\n",
      "Wall time: 5.01 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a_user_id</th>\n",
       "      <th>top_language</th>\n",
       "      <th>language_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>54</td>\n",
       "      <td>51.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>75.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>54</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>61</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   a_user_id  top_language  language_count\n",
       "0          0            54            51.0\n",
       "1          1             9            75.0\n",
       "2          1            47             3.0\n",
       "3          1            54             4.0\n",
       "4          1            61             1.0"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "train_lang_count = train_lang_count.merge(valid_lang_count,on=['a_user_id', 'language'],how='left')\n",
    "train_lang_count['tweet_id_y'] = train_lang_count['tweet_id_y'].fillna(0)\n",
    "train_lang_count['tweet_id_x'] = train_lang_count['tweet_id_x'] + train_lang_count['tweet_id_y']\n",
    "train_lang_count = train_lang_count.drop('tweet_id_y',axis=1)\n",
    "train_lang_count.columns = ['a_user_id', 'top_language', 'language_count']\n",
    "train_lang_count, = dask.persist(train_lang_count)\n",
    "train_lang_count.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'sort_values'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m~/sw/miniconda3/lib/python3.7/site-packages/dask/dataframe/core.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3619\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3620\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3621\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"'DataFrame' object has no attribute %r\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3622\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3623\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__dir__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'sort_values'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "train_lang_count = train_lang_count.sort_values(['a_user_id', 'language_count'])\n",
    "train_lang_count['a_user_shifted'] = train_lang_count['a_user_id'].shift(1)\n",
    "train_lang_count = train_lang_count[train_lang_count['a_user_id']!=train_lang_count['a_user_shifted']]\n",
    "train_lang_count = train_lang_count.drop(['a_user_shifted','language_count'],axis=1)\n",
    "train_lang_count.columns = ['a_user_id','top_language']\n",
    "train_lang_count, = dask.persist(train_lang_count)\n",
    "train_lang_count.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diff_language(df,df_lang_count):\n",
    "    df = df.merge(df_lang_count,how='left', left_on='b_user_id', right_on='a_user_id')\n",
    "    df['nan_language'] = df['top_language'].isnull()\n",
    "    df['same_language'] = df['language'] == df['top_language']\n",
    "    df['diff_language'] = df['language'] != df['top_language']\n",
    "    df['same_language'] = df['same_language']*(1-df['nan_language'])\n",
    "    df['diff_language'] = df['diff_language']*(1-df['nan_language'])\n",
    "    df = df.drop('top_language',axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "#train = diff_language(train,train_lang_count)\n",
    "#valid = diff_language(valid,train_lang_count)\n",
    "#train,valid = dask.persist(train,valid)\n",
    "#train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rate feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 59.4 ms, sys: 10.1 ms, total: 69.5 ms\n",
      "Wall time: 58.2 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# follow rate feature\n",
    "train['a_ff_rate'] = (train['a_following_count'] / train['a_follower_count']).astype('float32')\n",
    "train['b_ff_rate'] = (train['b_follower_count']  / train['b_following_count']).astype('float32')\n",
    "valid['a_ff_rate']  = (valid['a_following_count'] / valid['a_follower_count']).astype('float32')\n",
    "valid['b_ff_rate']  = (valid['b_follower_count']  / valid['b_following_count']).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "train,valid = dask.persist(train,valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DoneAndNotDoneFutures(done={<Future: finished, type: pandas.DataFrame, key: ('assign-3bbab3c613472f3dee4e0ba4d8c15292', 4)>, <Future: finished, type: pandas.DataFrame, key: ('assign-3bbab3c613472f3dee4e0ba4d8c15292', 2)>, <Future: finished, type: pandas.DataFrame, key: ('assign-3bbab3c613472f3dee4e0ba4d8c15292', 3)>, <Future: finished, type: pandas.DataFrame, key: ('assign-3bbab3c613472f3dee4e0ba4d8c15292', 6)>, <Future: finished, type: pandas.DataFrame, key: ('assign-3bbab3c613472f3dee4e0ba4d8c15292', 7)>, <Future: finished, type: pandas.DataFrame, key: ('assign-3bbab3c613472f3dee4e0ba4d8c15292', 5)>, <Future: finished, type: pandas.DataFrame, key: ('assign-3bbab3c613472f3dee4e0ba4d8c15292', 0)>, <Future: finished, type: pandas.DataFrame, key: ('assign-3bbab3c613472f3dee4e0ba4d8c15292', 1)>}, not_done=set())"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wait(train)\n",
    "wait(valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarize Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 272 µs, sys: 11 µs, total: 283 µs\n",
      "Wall time: 294 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['tweet_id',\n",
       " 'timestamp',\n",
       " 'a_account_creation',\n",
       " 'b_account_creation',\n",
       " 'fold',\n",
       " 'b_user_id',\n",
       " 'a_user_id',\n",
       " 'dt_dow',\n",
       " 'a_account_creation',\n",
       " 'b_account_creation',\n",
       " 'domains']"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "label_names = ['reply', 'retweet', 'retweet_comment', 'like']\n",
    "#DONT_USE = ['tweet_id','timestamp','a_account_creation','b_account_creation','engage_time',\n",
    "#            'fold','b_user_id','a_user_id', 'dt_dow',\n",
    "#            'a_account_creation', 'b_account_creation', \n",
    "#             'links','domains','hashtags0','hashtags1']\n",
    "DONT_USE = ['tweet_id','timestamp','a_account_creation','b_account_creation',\n",
    "            'fold','b_user_id','a_user_id', 'dt_dow',\n",
    "            'a_account_creation', 'b_account_creation', \n",
    "             'domains']\n",
    "DONT_USE += label_names\n",
    "features = [c for c in train.columns if c not in DONT_USE]\n",
    "\n",
    "RMV = [c for c in DONT_USE if c in train.columns and c not in label_names]\n",
    "RMV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.5 s, sys: 1.44 s, total: 11.9 s\n",
      "Wall time: 49.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for col in RMV:\n",
    "    #print(col, col in train.columns)\n",
    "    if col in train.columns:\n",
    "        train = train.drop(col,axis=1)\n",
    "        train, = dask.persist(train)\n",
    "        train.head()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.85 s, sys: 585 ms, total: 4.43 s\n",
      "Wall time: 18.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for col in RMV:\n",
    "    #print(col, col in valid.columns)\n",
    "    if col in valid.columns:\n",
    "        valid = valid.drop(col,axis=1)\n",
    "        valid, = dask.persist(valid,)\n",
    "        valid.head()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model Validate\n",
    "We will train on random 10% of first 5 days and validation on last 2 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69896806\n",
      "6989680\n",
      "Using 64 features: 64\n",
      "CPU times: user 1min 17s, sys: 18 s, total: 1min 35s\n",
      "Wall time: 14.6 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['media', 'tweet_type', 'language', 'a_follower_count',\n",
       "       'a_following_count', 'a_is_verified', 'b_follower_count',\n",
       "       'b_following_count', 'b_is_verified', 'b_follows_a', 'id',\n",
       "       'len_hashtags', 'len_domains', 'len_links', 'dt_hour', 'dt_minute',\n",
       "       'dt_second', 'engage_time', 'elapsed_time', 'TE_media_reply',\n",
       "       'TE_tweet_type_reply', 'TE_language_reply', 'TE_a_user_id_reply',\n",
       "       'TE_b_user_id_reply', 'TE_media_retweet', 'TE_tweet_type_retweet',\n",
       "       'TE_language_retweet', 'TE_a_user_id_retweet',\n",
       "       'TE_b_user_id_retweet', 'TE_media_retweet_comment',\n",
       "       'TE_tweet_type_retweet_comment', 'TE_language_retweet_comment',\n",
       "       'TE_a_user_id_retweet_comment', 'TE_b_user_id_retweet_comment',\n",
       "       'TE_media_like', 'TE_tweet_type_like', 'TE_language_like',\n",
       "       'TE_a_user_id_like', 'TE_b_user_id_like', 'TE_multi_reply',\n",
       "       'TE_multi_retweet', 'TE_multi_retweet_comment', 'TE_multi_like',\n",
       "       'TE_media_elapsed_time', 'TE_tweet_type_elapsed_time',\n",
       "       'TE_language_elapsed_time', 'CE_media', 'CE_tweet_type',\n",
       "       'CE_language', 'CE_a_user_id', 'CE_b_user_id', 'CE_media_norm',\n",
       "       'CE_tweet_type_norm', 'CE_language_norm', 'CE_a_user_id_norm',\n",
       "       'CE_b_user_id_norm', 'DE_b_user_id_b_follower_count_1',\n",
       "       'DE_b_user_id_b_follower_count_-1',\n",
       "       'DE_b_user_id_b_following_count_1',\n",
       "       'DE_b_user_id_b_following_count_-1', 'DE_b_user_id_language_1',\n",
       "       'DE_b_user_id_language_-1', 'a_ff_rate', 'b_ff_rate'], dtype='<U33')"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "SAMPLE_RATIO = 0.1\n",
    "SEED = 1\n",
    "\n",
    "if SAMPLE_RATIO < 1.0:\n",
    "    print(len(train))\n",
    "    train = train.sample(frac=SAMPLE_RATIO,random_state=42)\n",
    "    train, = dask.persist(train)\n",
    "    train.head()\n",
    "    print(len(train))\n",
    "\n",
    "train = train.compute()\n",
    "Y_train = train[label_names]\n",
    "train = train.drop(label_names,axis=1)\n",
    "\n",
    "features = [c for c in train.columns if c not in DONT_USE]\n",
    "print('Using %i features:'%(len(features)),train.shape[1])\n",
    "np.asarray(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26703585\n",
      "9346255\n"
     ]
    }
   ],
   "source": [
    "SAMPLE_RATIO = 0.35 # VAL SET NOW SIZE OF TEST SET\n",
    "SEED = 1\n",
    "if SAMPLE_RATIO < 1.0:\n",
    "    print(len(valid))\n",
    "    valid = valid.sample(frac=SAMPLE_RATIO,random_state=42)\n",
    "    valid, = dask.persist(valid)\n",
    "    valid.head()\n",
    "    print(len(valid))\n",
    "    \n",
    "valid = valid.compute()\n",
    "Y_valid = valid[label_names]\n",
    "valid = valid.drop(label_names,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train.head()\n",
    "#valid.head()\n",
    "#valid.dtypes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "#valid.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB Version 1.2.1\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "print('XGB Version',xgb.__version__)\n",
    "\n",
    "xgb_parms = { \n",
    "    'max_depth':8, \n",
    "    'learning_rate':0.1, \n",
    "    'subsample':0.8,\n",
    "    'colsample_bytree':0.3, \n",
    "    'eval_metric':'logloss',\n",
    "    'objective':'binary:logistic',\n",
    "    'nthread':40,\n",
    "    'tree_method':'hist',\n",
    "    #'predictor' : 'gpu_predictor'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no dup :) \n",
      "X_train.shape (6989680, 64)\n",
      "X_valid.shape (9346255, 64)\n"
     ]
    }
   ],
   "source": [
    "if train.columns.duplicated().sum()>0:\n",
    "    raise Exception(f'duplicated!: { train.columns[train.columns.duplicated()] }')\n",
    "print('no dup :) ')\n",
    "print(f'X_train.shape {train.shape}')\n",
    "print(f'X_valid.shape {valid.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.51 s, sys: 278 ms, total: 1.79 s\n",
      "Wall time: 47.3 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for col in train.columns:\n",
    "    if train[col].dtype=='bool':\n",
    "        train[col] = train[col].astype('int8')\n",
    "        valid[col] = valid[col].astype('int8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Y_train[['reply','retweet','retweet_comment','like']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########################\n",
      "### reply\n",
      "#########################\n",
      "Creating DMatrix...\n",
      "Took 33.4 seconds\n",
      "Training...\n",
      "Took 99.4 seconds\n",
      "Predicting...\n",
      "Took 5.3 seconds\n",
      "#########################\n",
      "### retweet\n",
      "#########################\n",
      "Creating DMatrix...\n",
      "Took 32.0 seconds\n",
      "Training...\n",
      "Took 100.1 seconds\n",
      "Predicting...\n",
      "Took 5.2 seconds\n",
      "#########################\n",
      "### retweet_comment\n",
      "#########################\n",
      "Creating DMatrix...\n",
      "Took 32.9 seconds\n",
      "Training...\n",
      "Took 97.7 seconds\n",
      "Predicting...\n",
      "Took 5.2 seconds\n",
      "#########################\n",
      "### like\n",
      "#########################\n",
      "Creating DMatrix...\n",
      "Took 32.9 seconds\n",
      "Training...\n",
      "Took 93.0 seconds\n",
      "Predicting...\n",
      "Took 5.4 seconds\n",
      "CPU times: user 4h 12min 57s, sys: 13min 23s, total: 4h 26min 20s\n",
      "Wall time: 9min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# TRAIN AND VALIDATE\n",
    "\n",
    "NROUND = 300\n",
    "VERBOSE_EVAL = 50\n",
    "#ESR = 50\n",
    "    \n",
    "oof = np.zeros((len(valid),len(label_names)))\n",
    "preds = []\n",
    "for i in range(4):\n",
    "\n",
    "    name = label_names[i]\n",
    "    print('#'*25);print('###',name);print('#'*25)\n",
    "       \n",
    "    start = time.time(); print('Creating DMatrix...')\n",
    "        \n",
    "    dtrain = xgb.DMatrix(data=train,label=Y_train.iloc[:, i])\n",
    "    dvalid = xgb.DMatrix(data=valid,label=Y_valid.iloc[:, i])\n",
    "    print('Took %.1f seconds'%(time.time()-start))\n",
    "             \n",
    "    start = time.time(); print('Training...')\n",
    "    model = xgb.train(xgb_parms, \n",
    "                           dtrain=dtrain,\n",
    "                           #evals=[(dtrain,'train'),(dvalid,'valid')],\n",
    "                           num_boost_round=NROUND,\n",
    "                           #early_stopping_rounds=ESR,\n",
    "                           verbose_eval=VERBOSE_EVAL) \n",
    "    print('Took %.1f seconds'%(time.time()-start))\n",
    "        \n",
    "    start = time.time(); print('Predicting...')\n",
    "    #Y_valid[f'pred_{name}'] = xgb.dask.predict(client,model,valid)\n",
    "    oof[:, i] += model.predict(dvalid)\n",
    "    #preds.append(xgb.dask.predict(client,model,valid))\n",
    "    print('Took %.1f seconds'%(time.time()-start))\n",
    "        \n",
    "    del model, dtrain, dvalid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "yvalid = Y_valid[label_names].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Validation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve, auc, log_loss\n",
    "\n",
    "def compute_prauc(pred, gt):\n",
    "  prec, recall, thresh = precision_recall_curve(gt, pred)\n",
    "  prauc = auc(recall, prec)\n",
    "  return prauc\n",
    "\n",
    "def calculate_ctr(gt):\n",
    "  positive = len([x for x in gt if x == 1])\n",
    "  ctr = positive/float(len(gt))\n",
    "  return ctr\n",
    "\n",
    "def compute_rce(pred, gt):\n",
    "    cross_entropy = log_loss(gt, pred)\n",
    "    data_ctr = calculate_ctr(gt)\n",
    "    strawman_cross_entropy = log_loss(gt, [data_ctr for _ in range(len(gt))])\n",
    "    return (1.0 - cross_entropy/strawman_cross_entropy)*100.0\n",
    "\n",
    "# FAST METRIC FROM GIBA\n",
    "def compute_rce_fast(pred, gt):\n",
    "    cross_entropy = log_loss(gt, pred)\n",
    "    yt = np.mean(gt)     \n",
    "    strawman_cross_entropy = -(yt*np.log(yt) + (1 - yt)*np.log(1 - yt))\n",
    "    return (1.0 - cross_entropy/strawman_cross_entropy)*100.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reply                PRAUC:0.23910 RCE:31.28032\n",
      "retweet              PRAUC:0.70965 RCE:50.94263\n",
      "retweet_comment      PRAUC:0.08023 RCE:22.98754\n",
      "like                 PRAUC:0.97411 RCE:82.06254\n",
      "CPU times: user 10min 21s, sys: 30.8 s, total: 10min 51s\n",
      "Wall time: 18.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "txt = ''\n",
    "for i in range(4):\n",
    "    prauc = compute_prauc(oof[:,i], yvalid[:, i])\n",
    "    rce   = compute_rce_fast(oof[:,i], yvalid[:, i])\n",
    "    txt_ = f\"{label_names[i]:20} PRAUC:{prauc:.5f} RCE:{rce:.5f}\"\n",
    "    print(txt_)\n",
    "    txt += txt_ + '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This notebook took 52.4 minutes\n"
     ]
    }
   ],
   "source": [
    "print('This notebook took %.1f minutes'%((time.time()-very_start)/60.))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
