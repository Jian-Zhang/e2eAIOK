{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Workflow:\n",
    "---\n",
    "\n",
    "A. Convert from json to csv using spark\n",
    "\n",
    "B. process csv data for train\n",
    "    1. load data from csv-alike files\n",
    "    2. create a new negative records table (user with false item_record and category) \n",
    "    3. join category by item_id as a positive records table\n",
    "    4. zip negative table and positive table as one, add a positive label to indicates.\n",
    "    5. aggregate same user history items into two new columns: items_history and category history\n",
    "    6. save last two records(one positive / one negative) of each user to local_test, remains save to local_train\n",
    "    7. split local_test with random 1:9 into local_test_splitByUser and local_train_splitByUser\n",
    "    8. build 3 dictionary: mid_voc, uid_voc and cat_voc from local_train_splitByUser using aggregate_count and sort\n",
    "    9. create a new dictionary map for mid and cat, create a mid list according to reviews_info\n",
    "    9. save all files: mid_voc, uid_voc, cat_voc, local_train_splitByUser and local_test_splitByUser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DIEN data process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from python_script.init_spark import *\n",
    "from python_script.utils import *\n",
    "from python_script.data_processor import DataProcessor\n",
    "from timeit import default_timer as timer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "dir_path = \"/mnt/nvme2/chendi/BlueWhale/ai-matrix/macro_benchmark/DIEN_INTEL_TF2/pyspark_data/\"\n",
    "local_prefix_src = \"file://\" + dir_path\n",
    "\n",
    "t0 = timer()\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .master('yarn')\\\n",
    "    .appName(\"DIEN_DATA_PREPARE\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Enable Arrow-based columnar data transfers\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "t1 = timer()\n",
    "# Load reviews_info and item_info from HDFS\n",
    "reviews_info_df, item_info_df = load_csv(spark, local_prefix_src)\n",
    "data_processor = DataProcessor(\n",
    "    spark, reviews_info_df, item_info_df, dir_path)\n",
    "data_processor.process()\n",
    "t3 = timer()\n",
    "print(\"\\n==================== Process Time =======================\\n\")\n",
    "print(\"Total process took %.3f secs\" % (t3 - t0))\n",
    "print(\"Details:\")\n",
    "print(\"start spark took %.3f secs\" % (t1 - t0))\n",
    "print(\"process and save took %.3f secs, includes:\" % (t3 - t1))\n",
    "for key, value in data_processor.elapse_time.items():\n",
    "    print(\"\\t%s %.3f\" % (key, value))\n",
    "print(\"\\n==========================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "==================== Process Time =======================\n",
    "\n",
    "Total process took 263.191 secs\n",
    "\n",
    "Details:\n",
    "\n",
    "start spark took 33.097 secs\n",
    "\n",
    "process and save took 230.093 secs, includes:\n",
    "\n",
    "\tdata_process 162.889\n",
    "    \n",
    "\tgenerate_voc 27.437\n",
    "    \n",
    "\tcombine_and_save_negative_positive 36.814\n",
    "\n",
    "=========================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Json to CSV\n",
    "#### May take about 2min, pls skip if you only want to check data process\n",
    "#### Notice: Seems original json file contains some error, convert by pyspark some rows will return Null items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Json to CSV\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time\n",
    "from timeit import default_timer as timer\n",
    "import random\n",
    "import os.path\n",
    "import pickle\n",
    "import pandas\n",
    "\n",
    "dir_path = \"/mnt/nvme2/chendi/BlueWhale/x-deeplearning/xdl-algorithm-solution/DIEN/data/pyspark_output/\"\n",
    "local_prefix = \"file://\" + dir_path\n",
    "\n",
    "def load_json():\n",
    "    \n",
    "    item_info_df = spark.read.json(local_prefix + 'meta_Books.json')\n",
    "    reviews_info_df = spark.read.json(local_prefix + 'reviews_Books.json')\n",
    "    \n",
    "    item_info_df = item_info_df.select('asin', expr(\"categories[0][size(categories[0]) - 1] as categories\"))\n",
    "    reviews_info_df = reviews_info_df.select('reviewerID', 'asin', 'overall', 'unixReviewTime')\n",
    "    \n",
    "    return reviews_info_df, item_info_df\n",
    "\n",
    "def list_dir(path):   \n",
    "    source_path_dict = {}\n",
    "    dirs = os.listdir(path)\n",
    "    for files in dirs:\n",
    "        try:\n",
    "            sub_dirs = os.listdir(path + \"/\" + files)\n",
    "            for file_name in sub_dirs:\n",
    "                if (file_name.endswith('parquet') or file_name.endswith('csv')):\n",
    "                    source_path_dict[files] = os.path.join(\n",
    "                        path, files, file_name)\n",
    "        except:\n",
    "            source_path_dict[files] = os.path.join(path, files)\n",
    "    return source_path_dict\n",
    "\n",
    "def result_rename_or_convert():   \n",
    "    fpath = dir_path\n",
    "    source_path_dict = list_dir(fpath)\n",
    "    fix = \"-spark\"\n",
    "    try:\n",
    "        os.rename(source_path_dict[\"reviews-info\" + fix], fpath + 'reviews-info')\n",
    "        os.rename(source_path_dict[\"item-info\" + fix], fpath + 'item-info')\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "t0 = timer()\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .master('yarn')\\\n",
    "    .appName(\"DIEN_DATA_PROCESS\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Enable Arrow-based columnar data transfers\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "\n",
    "t1 = timer()\n",
    "reviews_info_df, item_info_df = load_json()\n",
    "t2 = timer()\n",
    "reviews_info_df.repartition(1).write.option(\"sep\", '\\t').format('csv').mode('overwrite').save(local_prefix + 'reviews-info-spark')\n",
    "item_info_df.repartition(1).write.option(\"sep\", '\\t').format('csv').mode('overwrite').save(local_prefix + 'item-info-spark')\n",
    "t3 = timer()\n",
    "result_rename_or_convert()\n",
    "t4 = timer()\n",
    "\n",
    "# should be 2370585\n",
    "print(\"Total length of item-info is \", item_info_df.count())\n",
    "# should be 22507155\n",
    "print(\"Total length of reviews-info is \", reviews_info_df.count())\n",
    "\n",
    "print(\"\\n==================== Convert Time =======================\\n\")\n",
    "print(\"Total process took %.3f secs\" % (t4 - t0))\n",
    "print(\"Details:\")\n",
    "print(\"start spark took %.3f secs\" % (t1 - t0))\n",
    "print(\"load from json took %.3f secs\" % (t2 - t1))\n",
    "print(\"write as csv took %.3f secs\" % (t3 - t2))\n",
    "print(\"rename csv files took %.3f secs\" % (t4 - t3))\n",
    "print(\"\\n==========================================================\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option2: using original python to convert (No need to run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load original json as CSV\n",
    "import sys\n",
    "import random\n",
    "import time\n",
    "from time import time\n",
    "\n",
    "def process_meta(file):\n",
    "    fi = open(file, \"r\")\n",
    "    fo = open(\"item-info\", \"w\")\n",
    "    for line in fi:\n",
    "        obj = eval(line)\n",
    "        cat = obj[\"categories\"][0][-1]\n",
    "        print(obj[\"asin\"] + \"\\t\" + cat, file=fo)\n",
    "\n",
    "def process_reviews(file):\n",
    "    fi = open(file, \"r\")\n",
    "    user_map = {}\n",
    "    fo = open(\"reviews-info\", \"w\")\n",
    "    for line in fi:\n",
    "        obj = eval(line)\n",
    "        userID = obj[\"reviewerID\"]\n",
    "        itemID = obj[\"asin\"]\n",
    "        rating = obj[\"overall\"]\n",
    "        time = obj[\"unixReviewTime\"]\n",
    "        print(userID + \"\\t\" + itemID + \"\\t\" + str(rating) + \"\\t\" + str(time), file=fo)\n",
    "\n",
    "local_prefix = \"/mnt/nvme2/chendi/BlueWhale/x-deeplearning/xdl-algorithm-solution/DIEN/data/\"\n",
    "t0 = timer()\n",
    "process_meta(local_prefix + 'meta_Books.json')\n",
    "process_reviews(local_prefix + 'reviews_Books.json')\n",
    "t1 = timer()\n",
    "\n",
    "print(\"Convert initial csv from json took %.3f secs\" % (t1 - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Old process codes, now we use python module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
