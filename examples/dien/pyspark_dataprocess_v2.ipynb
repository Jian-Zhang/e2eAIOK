{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Workflow:\n",
    "---\n",
    "\n",
    "A. Convert from json to csv using spark\n",
    "\n",
    "B. process csv data for train\n",
    "    1. load data from csv-alike files\n",
    "    2. create a new negative records table (user with false item_record and category) \n",
    "    3. join category by item_id as a positive records table\n",
    "    4. zip negative table and positive table as one, add a positive label to indicates.\n",
    "    5. aggregate same user history items into two new columns: items_history and category history\n",
    "    6. save last two records(one positive / one negative) of each user to local_test, remains save to local_train\n",
    "    7. split local_test with random 1:9 into local_test_splitByUser and local_train_splitByUser\n",
    "    8. build 3 dictionary: mid_voc, uid_voc and cat_voc from local_train_splitByUser using aggregate_count and sort\n",
    "    9. create a new dictionary map for mid and cat, create a mid list according to reviews_info\n",
    "    9. save all files: mid_voc, uid_voc, cat_voc, local_train_splitByUser and local_test_splitByUser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Json to CSV\n",
    "#### May take about 2min, pls skip if you only want to check data process\n",
    "#### Notice: Seems original json file contains some error, convert by pyspark some rows will return Null items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Json to CSV\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time\n",
    "from timeit import default_timer as timer\n",
    "import random\n",
    "import os.path\n",
    "import pickle\n",
    "import pandas\n",
    "\n",
    "dir_path = \"/mnt/nvme2/chendi/BlueWhale/x-deeplearning/xdl-algorithm-solution/DIEN/data/pyspark_output/\"\n",
    "local_prefix = \"file://\" + dir_path\n",
    "\n",
    "def load_json():\n",
    "    \n",
    "    item_info_df = spark.read.json(local_prefix + 'meta_Books.json')\n",
    "    reviews_info_df = spark.read.json(local_prefix + 'reviews_Books.json')\n",
    "    \n",
    "    item_info_df = item_info_df.select('asin', expr(\"categories[0][size(categories[0]) - 1] as categories\"))\n",
    "    reviews_info_df = reviews_info_df.select('reviewerID', 'asin', 'overall', 'unixReviewTime')\n",
    "    \n",
    "    return reviews_info_df, item_info_df\n",
    "\n",
    "def list_dir(path):   \n",
    "    source_path_dict = {}\n",
    "    dirs = os.listdir(path)\n",
    "    for files in dirs:\n",
    "        try:\n",
    "            sub_dirs = os.listdir(path + \"/\" + files)\n",
    "            for file_name in sub_dirs:\n",
    "                if (file_name.endswith('parquet') or file_name.endswith('csv')):\n",
    "                    source_path_dict[files] = os.path.join(\n",
    "                        path, files, file_name)\n",
    "        except:\n",
    "            source_path_dict[files] = os.path.join(path, files)\n",
    "    return source_path_dict\n",
    "\n",
    "def result_rename_or_convert():   \n",
    "    fpath = dir_path\n",
    "    source_path_dict = list_dir(fpath)\n",
    "    fix = \"-spark\"\n",
    "    try:\n",
    "        os.rename(source_path_dict[\"reviews-info\" + fix], fpath + 'reviews-info')\n",
    "        os.rename(source_path_dict[\"item-info\" + fix], fpath + 'item-info')\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "t0 = timer()\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .master('yarn')\\\n",
    "    .appName(\"DIEN_DATA_PROCESS\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Enable Arrow-based columnar data transfers\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "\n",
    "t1 = timer()\n",
    "reviews_info_df, item_info_df = load_json()\n",
    "t2 = timer()\n",
    "reviews_info_df.repartition(1).write.option(\"sep\", '\\t').format('csv').mode('overwrite').save(local_prefix + 'reviews-info-spark')\n",
    "item_info_df.repartition(1).write.option(\"sep\", '\\t').format('csv').mode('overwrite').save(local_prefix + 'item-info-spark')\n",
    "t3 = timer()\n",
    "result_rename_or_convert()\n",
    "t4 = timer()\n",
    "\n",
    "# should be 2370585\n",
    "print(\"Total length of item-info is \", item_info_df.count())\n",
    "# should be 22507155\n",
    "print(\"Total length of reviews-info is \", reviews_info_df.count())\n",
    "\n",
    "print(\"\\n==================== Convert Time =======================\\n\")\n",
    "print(\"Total process took %.3f secs\" % (t4 - t0))\n",
    "print(\"Details:\")\n",
    "print(\"start spark took %.3f secs\" % (t1 - t0))\n",
    "print(\"load from json took %.3f secs\" % (t2 - t1))\n",
    "print(\"write as csv took %.3f secs\" % (t3 - t2))\n",
    "print(\"rename csv files took %.3f secs\" % (t4 - t3))\n",
    "print(\"\\n==========================================================\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#item_info_df = item_info_df.select('asin', expr(\"categories[0][size(categories[0]) - 1] as categories\"))\n",
    "item_info_df = spark.read.json(local_prefix + 'meta_Books.json')\n",
    "print(item_info_df.filter(col('asin').isNull()).count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DIEN data process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Process\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    " \n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time\n",
    "import random\n",
    "import os.path\n",
    "import pickle\n",
    "import pandas\n",
    "\n",
    "dir_path = \"/mnt/nvme2/chendi/BlueWhale/x-deeplearning/xdl-algorithm-solution/DIEN/data/pyspark_output/\"\n",
    "local_prefix_src = \"file://\" + dir_path\n",
    "local_prefix = \"file://\" + dir_path\n",
    "\n",
    "def load_csv():\n",
    "    review_id_field = StructField('review_id', StringType())\n",
    "    movie_id_field = StructField('movie_id', StringType())\n",
    "    overall_field = StructField('overall', FloatType())\n",
    "    unix_time_field = StructField('unix_review_time', IntegerType())\n",
    "    reviews_info_schema = StructType([review_id_field, movie_id_field, overall_field, unix_time_field])\n",
    "\n",
    "    category_field = StructField('category', StringType())\n",
    "    item_info_schema = StructType([movie_id_field, category_field])\n",
    "    \n",
    "    reviews_info_df = spark.read.schema(reviews_info_schema).option('sep', '\\t').csv(local_prefix_src + \"/reviews-info\") \n",
    "    item_info_df = spark.read.schema(item_info_schema).option('sep', '\\t').csv(local_prefix_src + \"/item-info\")\n",
    "    \n",
    "    return reviews_info_df, item_info_df\n",
    "\n",
    "def list_dir(path):   \n",
    "    source_path_dict = {}\n",
    "    dirs = os.listdir(path)\n",
    "    for files in dirs:\n",
    "        try:\n",
    "            sub_dirs = os.listdir(path + \"/\" + files)\n",
    "            for file_name in sub_dirs:\n",
    "                if (file_name.endswith('parquet') or file_name.endswith('csv')):\n",
    "                    source_path_dict[files] = os.path.join(\n",
    "                        path, files, file_name)\n",
    "        except:\n",
    "            source_path_dict[files] = os.path.join(path, files)\n",
    "    return source_path_dict\n",
    "\n",
    "def to_pydict(df):\n",
    "    if df.shape[1] == 2:\n",
    "        keys = df.keys()\n",
    "        l1 = df[keys[0]].to_list()\n",
    "        l2 = df[keys[1]].to_list()\n",
    "        return dict(zip(l1, l2))\n",
    "    return {}\n",
    "        \n",
    "def result_rename_or_convert():   \n",
    "    fpath = dir_path\n",
    "    source_path_dict = list_dir(fpath)\n",
    "    fix = \"_spark\"\n",
    "    os.rename(source_path_dict[\"local_test_splitByUser\" + fix], fpath + '/local_test_splitByUser')\n",
    "    os.rename(source_path_dict[\"local_train_splitByUser\" + fix], fpath + '/local_train_splitByUser')\n",
    "    uid_voc = to_pydict(pandas.read_parquet(source_path_dict[\"uid_voc\" + fix]))\n",
    "    mid_voc = to_pydict(pandas.read_parquet(source_path_dict[\"mid_voc\" + fix]))\n",
    "    cat_voc = to_pydict(pandas.read_parquet(source_path_dict[\"cat_voc\" + fix]))\n",
    "\n",
    "    pickle.dump(uid_voc, open(fpath + '/uid_voc.pkl', \"wb\"), protocol=0)\n",
    "    pickle.dump(mid_voc, open(fpath + '/mid_voc.pkl', \"wb\"), protocol=0)\n",
    "    pickle.dump(cat_voc, open(fpath + '/cat_voc.pkl', \"wb\"), protocol=0)\n",
    "\n",
    "class DataProcessor:\n",
    "    def rand_ordinal_n(self, df, n, name = 'ordinal'):\n",
    "        return df.withColumn(name, (rand() * n).cast(\"int\"))\n",
    "    \n",
    "    def process(self):\n",
    "        self.uid_dict_df.repartition(1).write.format('parquet').mode('overwrite').save(local_prefix + '/uid_voc_spark')\n",
    "        self.mid_dict_df.repartition(1).write.format('parquet').mode('overwrite').save(local_prefix + '/mid_voc_spark')\n",
    "        self.cat_dict_df.repartition(1).write.format('parquet').mode('overwrite').save(local_prefix + '/cat_voc_spark')\n",
    "        \n",
    "        # we need to concat two history array columns to string\n",
    "        self.test_df = self.test_df.select('positive',\\\n",
    "                            'review_id',\\\n",
    "                            'movie_id',\\\n",
    "                            'category',\\\n",
    "                            expr(\"concat_ws('\\x02', concated_movie_id)\"),\\\n",
    "                            expr(\"concat_ws('\\x02', concated_category)\"),\\\n",
    "                            'row_number')\n",
    "        self.test_df.repartition(1)\\\n",
    "                    .write.option(\"sep\",'\\t').format('csv').mode('overwrite').save(local_prefix + '/local_test_splitByUser_spark')\n",
    "        \n",
    "        self.train_df = self.train_df.select('positive',\\\n",
    "                            'review_id',\\\n",
    "                            'movie_id',\\\n",
    "                            'category',\\\n",
    "                            expr(\"concat_ws('\\x02', concated_movie_id)\"),\\\n",
    "                            expr(\"concat_ws('\\x02', concated_category)\"),\n",
    "                            'row_number')          \n",
    "        self.train_df.repartition(1)\\\n",
    "                     .write.option(\"sep\",'\\t').format('csv').mode('overwrite').save(local_prefix + '/local_train_splitByUser_spark')\n",
    "    \n",
    "    def shuffle_data_by_user(self, df):\n",
    "        window_spec = Window.partitionBy(df.review_id).orderBy(df.positive)\n",
    "        return df.withColumn(\"row_number\", row_number().over(window_spec))\n",
    "        #return df.withColumn(\"row_number\", row_number().over(window_spec)).drop(\"row_number\")\n",
    "    \n",
    "    def __init__(self, spark, reviews_info_df, item_info_df):\n",
    "        self.reviews_info_df = reviews_info_df\n",
    "        self.item_info_df = item_info_df\n",
    "\n",
    "        # same as meta_map in process_data.py\n",
    "        self.meta_map_df = self.item_info_df\\\n",
    "                        .groupby(\"movie_id\")\\\n",
    "                        .agg(first(\"category\").alias(\"category\"))\n",
    "        self.meta_map_df.write.format('parquet').mode('overwrite').save(local_prefix + '/meta_map')\n",
    "        self.meta_map_df = spark.read.parquet(local_prefix + '/meta_map')\n",
    "        \n",
    "    \n",
    "        #same as item_list in process_data.py\n",
    "        item_list_df = self.reviews_info_df\\\n",
    "                                .groupby(\"movie_id\")\\\n",
    "                                .count()\\\n",
    "                                .drop('count')\n",
    "\n",
    "        # same as user_map in process_data.py\n",
    "        user_map_df = self.reviews_info_df \n",
    "\n",
    "        asin_list = [row['movie_id'] for row in item_list_df.select('movie_id').collect()]\n",
    "        asin_len = len(asin_list)\n",
    "        broadcast_movie_id_list = spark.sparkContext.broadcast(asin_list)\n",
    "\n",
    "        def get_random_id(asin_total_len, asin):\n",
    "            item_list = broadcast_movie_id_list.value\n",
    "            asin_neg = asin\n",
    "            while True:\n",
    "                asin_neg_index = random.randint(0, asin_total_len - 1)\n",
    "                asin_neg = item_list[asin_neg_index]\n",
    "                if asin_neg == None or asin_neg == asin:\n",
    "                    continue\n",
    "                else:\n",
    "                    break\n",
    "            return asin_neg        \n",
    "\n",
    "        get_random_id_udf = udf(get_random_id, StringType())\n",
    "\n",
    "        ## manual join ##\n",
    "        # same as line 66-75 in process_data.py\n",
    "        negative_df =  user_map_df\\\n",
    "                        .withColumn('positive', lit(0))\\\n",
    "                        .withColumn('rand_false_mid', get_random_id_udf(lit(asin_len), \"movie_id\"))\\\n",
    "                        .join(self.meta_map_df, col(\"rand_false_mid\") == self.meta_map_df.movie_id, 'left_outer')\\\n",
    "                        .withColumn('category', when(col('category').isNotNull(), col('category')).otherwise(\"default_cat\"))\\\n",
    "                        .select(\"positive\", \"review_id\", self.meta_map_df.movie_id, \"overall\", \"unix_review_time\", \"category\")\\\n",
    "\n",
    "        positive_df = user_map_df\\\n",
    "                        .withColumn('positive', lit(1))\\\n",
    "                        .join(self.meta_map_df, 'movie_id', 'left_outer')\\\n",
    "                        .withColumn('category', when(col('category').isNotNull(), col('category')).otherwise(\"default_cat\"))\\\n",
    "                        .select(\"positive\", \"review_id\", \"movie_id\", \"overall\", \"unix_review_time\", \"category\")\\\n",
    "\n",
    "        ## split_test ##\n",
    "        # last two records of one user(one positive one negative) set tag 20190119\n",
    "        # other previous records of one user set tag 20180118\n",
    "\n",
    "        ## local_aggregator\n",
    "        # all tag with 20190119 will be wrote to local_test\n",
    "        user_window = Window.partitionBy('review_id').orderBy('unix_review_time')\n",
    "        last_positive_with_concat_df = positive_df\\\n",
    "                                            .withColumn('uid', row_number().over(user_window))\\\n",
    "                                            .groupby('review_id')\\\n",
    "                                            .agg(last('movie_id').alias('movie_id'),\\\n",
    "                                                 last('category').alias('category'),\\\n",
    "                                                 collect_list('movie_id').alias(\"concated_movie_id\"),\\\n",
    "                                                 collect_list('category').alias(\"concated_category\"),\\\n",
    "                                                 count(\"*\").alias(\"numItemsByUser\"),\n",
    "                                                 last('uid').alias('uid'))\\\n",
    "                                            .withColumn(\"concated_movie_id\", expr(\"slice(concated_movie_id, 1, numItemsByUser - 1)\"))\\\n",
    "                                            .withColumn(\"concated_category\", expr(\"slice(concated_category, 1, numItemsByUser - 1)\"))\\\n",
    "                                            .withColumn(\"numItemsByUser\", expr(\"numItemsByUser - 1\"))\\\n",
    "                                            .filter(col(\"numItemsByUser\") > 0)\n",
    "\n",
    "        ## save data ############################################################################################\n",
    "        last_positive_with_concat_df.write.format('parquet').mode('overwrite').save(local_prefix + '/aggregated_records')\n",
    "        reload_last_positive_with_concat_df = spark.read.parquet(local_prefix + '/aggregated_records')\n",
    "\n",
    "        # by saving to local, we can ensure negative record and positive record will have same history sequence\n",
    "        #########################################################################################################\n",
    "\n",
    "        last_negative_record_of_user_df = negative_df\\\n",
    "                                            .groupby('review_id')\\\n",
    "                                            .agg(last('positive').alias('positive'),\\\n",
    "                                                 last('movie_id').alias('movie_id'),\\\n",
    "                                                 last('category').alias('category'))\n",
    "        last_negative_record_of_user_df = reload_last_positive_with_concat_df\\\n",
    "                                            .join(last_negative_record_of_user_df, 'review_id', 'inner')\\\n",
    "                                            .select(\\\n",
    "                                                    'review_id',\\\n",
    "                                                    'positive',\\\n",
    "                                                    last_negative_record_of_user_df.movie_id.alias('movie_id'),\\\n",
    "                                                    last_negative_record_of_user_df.category.alias('category'),\\\n",
    "                                                    'concated_movie_id',\\\n",
    "                                                    'concated_category',\\\n",
    "                                                    'numItemsByUser')\n",
    "        last_positive_record_of_user_df = reload_last_positive_with_concat_df\\\n",
    "                                            .select('review_id',\\\n",
    "                                                    lit(1).alias('positive'),\\\n",
    "                                                    'movie_id',\\\n",
    "                                                    'category',\\\n",
    "                                                    'concated_movie_id',\\\n",
    "                                                    'concated_category',\\\n",
    "                                                    'numItemsByUser')\n",
    "        union_records_df = last_negative_record_of_user_df\\\n",
    "                            .union(last_positive_record_of_user_df)\n",
    "\n",
    "        ## all local_test will be split with random 1:9 to local_train_splitByUser and local_test_splitByUser\n",
    "        reviews_groupby_user_df = self.rand_ordinal_n(reload_last_positive_with_concat_df, 10).select('review_id', 'ordinal')\n",
    "        union_concated_df = reviews_groupby_user_df\\\n",
    "                            .join(union_records_df, 'review_id', 'inner')\\\n",
    "                            .select('positive',\\\n",
    "                                    'review_id',\\\n",
    "                                    'movie_id',\\\n",
    "                                    'category',\\\n",
    "                                    'concated_movie_id',\\\n",
    "                                    'concated_category',\\\n",
    "                                    'numItemsByUser',\\\n",
    "                                    'ordinal')\n",
    "        ## save data ############################################################################################\n",
    "        union_concated_df.write.format('parquet').mode('overwrite').save(local_prefix + '/local_test')\n",
    "        #########################################################################################################\n",
    "        \n",
    "        ## end if local_test exists ################################################################################\n",
    "        reload_union_concated_df = spark.read.parquet(local_prefix + '/local_test')\n",
    "        \n",
    "        ## split aggregated_labled_df by 1:9\n",
    "        self.test_df = reload_union_concated_df.filter(col(\"ordinal\") == 2).drop(\"ordinal\")\n",
    "        self.train_df = reload_union_concated_df.filter(col(\"ordinal\") != 2).drop(\"ordinal\")\n",
    "        \n",
    "        ## use window function to make sure same user records stay together\n",
    "        self.test_df = self.shuffle_data_by_user(self.test_df)\n",
    "        self.train_df = self.shuffle_data_by_user(self.train_df)\n",
    "            \n",
    "        ## build uid_dict, mid_dict and cat_dict\n",
    "        columns = ['review_id', 'uid']\n",
    "        zero = spark.createDataFrame([(\"A1Y6U82N6TYZPI\",0)], columns)      \n",
    "        \n",
    "        self.uid_dict_df = zero.union(self.train_df\\\n",
    "                            .groupBy('review_id')\\\n",
    "                            .count()\\\n",
    "                            .withColumn('uid', row_number().over(Window.orderBy(desc('count'))))\\\n",
    "                            .drop(\"count\"))\n",
    "        \n",
    "        columns = ['movie_id', 'mid']\n",
    "        zero = spark.createDataFrame([(\"default_mid\",0)], columns)  \n",
    "        self.mid_dict_df = zero.union(self.train_df\\\n",
    "                            .withColumn(\"concated_movie_id\", array_union(col(\"concated_movie_id\"), array(col(\"movie_id\"))))\\\n",
    "                            .select(explode(col(\"concated_movie_id\")).alias(\"movie_id\"))\\\n",
    "                            .groupBy('movie_id')\\\n",
    "                            .count()\\\n",
    "                            .filter(col('movie_id') != \"default_mid\")\\\n",
    "                            .withColumn('mid', row_number().over(Window.orderBy(desc('count'))))\\\n",
    "                            .drop(\"count\"))\n",
    "        \n",
    "        columns = ['category', 'cat']\n",
    "        zero = spark.createDataFrame([(\"default_cat\",0)], columns) \n",
    "        self.cat_dict_df = zero.union(self.train_df\\\n",
    "                            .withColumn(\"concated_category\", array_union(col(\"concated_category\"), array(col(\"category\"))))\\\n",
    "                            .select(explode(col(\"concated_category\")).alias(\"category\"))\\\n",
    "                            .groupBy('category')\\\n",
    "                            .count()\\\n",
    "                            .filter(col('category') != \"default_cat\")\\\n",
    "                            .withColumn('cat', row_number().over(Window.orderBy(desc('count'))))\\\n",
    "                            .drop(\"count\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    " \n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "t0 = timer()\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .master('yarn')\\\n",
    "    .appName(\"DIEN_DATA_PREPARE\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Enable Arrow-based columnar data transfers\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "t1 = timer()\n",
    "# Load reviews_info and item_info from HDFS\n",
    "reviews_info_df, item_info_df = load_csv()\n",
    "data_processor = DataProcessor(spark, reviews_info_df, item_info_df)\n",
    "t2 = timer()\n",
    "data_processor.process()\n",
    "t3 = timer()\n",
    "result_rename_or_convert()\n",
    "t4 = timer()\n",
    "print(\"\\n==================== Process Time =======================\\n\")\n",
    "print(\"Total process took %.3f secs\" % (t4 - t0))\n",
    "print(\"Details:\")\n",
    "print(\"start spark took %.3f secs\" % (t1 - t0))\n",
    "print(\"pre process took %.3f secs\" % (t2 - t1))\n",
    "print(\"process and save took %.3f secs\" % (t3 - t2))\n",
    "print(\"rename took %.3f secs\" % (t4 - t3))\n",
    "print(\"\\n==========================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "==================== Process Time =======================\n",
    "\n",
    "Total process took 119.847 secs\n",
    "\n",
    "Details:\n",
    "\n",
    "start spark took 29.739 secs\n",
    "\n",
    "pre process took 55.900 secs\n",
    "\n",
    "process and save took 26.244 secs\n",
    "\n",
    "rename took 7.964 secs\n",
    "\n",
    "=========================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option2: using original python to convert (No need to run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load original json as CSV\n",
    "import sys\n",
    "import random\n",
    "import time\n",
    "from time import time\n",
    "\n",
    "def process_meta(file):\n",
    "    fi = open(file, \"r\")\n",
    "    fo = open(\"item-info\", \"w\")\n",
    "    for line in fi:\n",
    "        obj = eval(line)\n",
    "        cat = obj[\"categories\"][0][-1]\n",
    "        print(obj[\"asin\"] + \"\\t\" + cat, file=fo)\n",
    "\n",
    "def process_reviews(file):\n",
    "    fi = open(file, \"r\")\n",
    "    user_map = {}\n",
    "    fo = open(\"reviews-info\", \"w\")\n",
    "    for line in fi:\n",
    "        obj = eval(line)\n",
    "        userID = obj[\"reviewerID\"]\n",
    "        itemID = obj[\"asin\"]\n",
    "        rating = obj[\"overall\"]\n",
    "        time = obj[\"unixReviewTime\"]\n",
    "        print(userID + \"\\t\" + itemID + \"\\t\" + str(rating) + \"\\t\" + str(time), file=fo)\n",
    "\n",
    "local_prefix = \"/mnt/nvme2/chendi/BlueWhale/x-deeplearning/xdl-algorithm-solution/DIEN/data/\"\n",
    "t0 = timer()\n",
    "process_meta(local_prefix + 'meta_Books.json')\n",
    "process_reviews(local_prefix + 'reviews_Books.json')\n",
    "t1 = timer()\n",
    "\n",
    "print(\"Convert initial csv from json took %.3f secs\" % (t1 - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
