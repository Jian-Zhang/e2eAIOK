{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Workflow:\n",
    "---\n",
    "\n",
    "A. Convert from json to csv using spark\n",
    "\n",
    "B. process csv data for train\n",
    "    1. load data from csv-alike files\n",
    "    2. create a new negative records table (user with false item_record and category) \n",
    "    3. join category by item_id as a positive records table\n",
    "    4. zip negative table and positive table as one, add a positive label to indicates.\n",
    "    5. aggregate same user history items into two new columns: items_history and category history\n",
    "    6. save last two records(one positive / one negative) of each user to local_test, remains save to local_train\n",
    "    7. split local_test with random 1:9 into local_test_splitByUser and local_train_splitByUser\n",
    "    8. build 3 dictionary: mid_voc, uid_voc and cat_voc from local_train_splitByUser using aggregate_count and sort\n",
    "    9. create a new dictionary map for mid and cat, create a mid list according to reviews_info\n",
    "    9. save all files: mid_voc, uid_voc, cat_voc, local_train_splitByUser and local_test_splitByUser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Json to CSV\n",
    "#### May take about 2min, pls skip if you only want to check data process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total length of item-info is  2370585\n",
      "Total length of reviews-info is  22507155\n",
      "\n",
      "==================== Convert Time =======================\n",
      "\n",
      "Total process took 109.148 secs\n",
      "Details:\n",
      "load from json took 101.544 secs\n",
      "write as csv took 7.604 secs\n",
      "\n",
      "==========================================================\n"
     ]
    }
   ],
   "source": [
    "# Convert Json to CSV\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time\n",
    "from timeit import default_timer as timer\n",
    "import random\n",
    "import os.path\n",
    "\n",
    "local_prefix = \"file:///mnt/nvme2/chendi/BlueWhale/x-deeplearning/xdl-algorithm-solution/DIEN/data/\"\n",
    "\n",
    "def load_json():\n",
    "    \n",
    "    item_info_df = spark.read.json(local_prefix + 'meta_Books.json')\n",
    "    reviews_info_df = spark.read.json(local_prefix + 'reviews_Books.json')\n",
    "    \n",
    "    item_info_df = item_info_df.select('asin', expr(\"categories[0][size(categories[0]) - 1] as categories\"))\n",
    "    reviews_info_df = reviews_info_df.select('reviewerID', 'asin', 'overall', 'unixReviewTime')\n",
    "    \n",
    "    return reviews_info_df, item_info_df\n",
    "\n",
    "t0 = timer()\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .master('yarn')\\\n",
    "    .appName(\"DIEN_DATA_PROCESS\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Enable Arrow-based columnar data transfers\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "\n",
    "t0 = timer()\n",
    "reviews_info_df, item_info_df = load_json()\n",
    "t1 = timer()\n",
    "reviews_info_df.write.format('csv').mode('overwrite').save(local_prefix + 'reviews-info-spark')\n",
    "item_info_df.write.format('csv').mode('overwrite').save(local_prefix + 'item-info-spark')\n",
    "t2 = timer()\n",
    "\n",
    "# should be 2370585\n",
    "print(\"Total length of item-info is \", item_info_df.count())\n",
    "# should be 22507155\n",
    "print(\"Total length of reviews-info is \", reviews_info_df.count())\n",
    "\n",
    "print(\"\\n==================== Convert Time =======================\\n\")\n",
    "print(\"Total process took %.3f secs\" % (t2 - t0))\n",
    "print(\"Details:\")\n",
    "print(\"load from json took %.3f secs\" % (t1 - t0))\n",
    "print(\"write as csv took %.3f secs\" % (t2 - t1))\n",
    "print(\"\\n==========================================================\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DIEN data process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Process\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    " \n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time\n",
    "import random\n",
    "import os.path\n",
    "\n",
    "local_prefix_src = \"file:///mnt/nvme2/chendi/BlueWhale/x-deeplearning/xdl-algorithm-solution/DIEN/data\"\n",
    "local_prefix = \"file:///mnt/nvme2/chendi/BlueWhale/x-deeplearning/xdl-algorithm-solution/DIEN/data/pyspark_output\"\n",
    "\n",
    "def load_csv():\n",
    "\n",
    "    review_id_field = StructField('review_id', StringType())\n",
    "    movie_id_field = StructField('movie_id', StringType())\n",
    "    overall_field = StructField('overall', FloatType())\n",
    "    unix_time_field = StructField('unix_review_time', IntegerType())\n",
    "    reviews_info_schema = StructType([review_id_field, movie_id_field, overall_field, unix_time_field])\n",
    "\n",
    "    category_field = StructField('category', StringType())\n",
    "    item_info_schema = StructType([movie_id_field, category_field])\n",
    "    \n",
    "    reviews_info_df = spark.read.schema(reviews_info_schema).option('sep', '\\t').csv(local_prefix_src + \"/reviews-info\") \n",
    "    item_info_df = spark.read.schema(item_info_schema).option('sep', '\\t').csv(local_prefix_src + \"/item-info\")\n",
    "    \n",
    "    return reviews_info_df, item_info_df\n",
    "\n",
    "def load_pyspark_processed_csv():\n",
    "\n",
    "    review_id_field = StructField('review_id', StringType())\n",
    "    movie_id_field = StructField('movie_id', StringType())\n",
    "    overall_field = StructField('overall', FloatType())\n",
    "    unix_time_field = StructField('unix_review_time', IntegerType())\n",
    "    reviews_info_schema = StructType([review_id_field, movie_id_field, overall_field, unix_time_field])\n",
    "\n",
    "    category_field = StructField('category', StringType())\n",
    "    item_info_schema = StructType([movie_id_field, category_field])\n",
    "    \n",
    "    reviews_info_df = spark.read.schema(reviews_info_schema).option('sep', ',').csv(local_prefix_src + \"/reviews-info-spark\") \n",
    "    item_info_df = spark.read.schema(item_info_schema).option('sep', ',').csv(local_prefix_src + \"/item-info-spark\")\n",
    "    \n",
    "    return reviews_info_df, item_info_df\n",
    "\n",
    "\n",
    "class DataProcessor:\n",
    "    def rand_ordinal_n(self, df, n, name = 'ordinal'):\n",
    "        return df.withColumn(name, (rand() * n).cast(\"int\"))\n",
    "    \n",
    "    def process(self):\n",
    "        self.uid_dict_df.repartition(1).write.format('parquet').mode('overwrite').save(local_prefix + '/dien/output/uid_voc')\n",
    "        self.mid_dict_df.repartition(1).write.format('parquet').mode('overwrite').save(local_prefix + '/dien/output/mid_voc')\n",
    "        self.cat_dict_df.repartition(1).write.format('parquet').mode('overwrite').save(local_prefix + '/dien/output/cat_voc')\n",
    "\n",
    "        # we need to concat two history array columns to string\n",
    "        self.test_df = self.test_df.select('positive',\\\n",
    "                            'review_id',\\\n",
    "                            'movie_id',\\\n",
    "                            'category',\\\n",
    "                            expr(\"concat_ws('\\x02', concated_movie_id)\"),\\\n",
    "                            expr(\"concat_ws('\\x02', concated_category)\"),\\\n",
    "                            'row_number')\n",
    "        self.test_df.repartition(1)\\\n",
    "                    .write.option(\"sep\",'\\t').format('csv').mode('overwrite').save(local_prefix + '/dien/output/local_test_splitByUser')\n",
    "        \n",
    "        self.train_df = self.train_df.select('positive',\\\n",
    "                            'review_id',\\\n",
    "                            'movie_id',\\\n",
    "                            'category',\\\n",
    "                            expr(\"concat_ws('\\x02', concated_movie_id)\"),\\\n",
    "                            expr(\"concat_ws('\\x02', concated_category)\"),\n",
    "                            'row_number')          \n",
    "        self.train_df.repartition(1)\\\n",
    "                     .write.option(\"sep\",'\\t').format('csv').mode('overwrite').save(local_prefix + '/dien/output/local_train_splitByUser')\n",
    "    \n",
    "    def if_local_test_exists(self):\n",
    "        import subprocess\n",
    "\n",
    "        path = local_prefix + \"/dien/output/local_test/_SUCCESS\"\n",
    "        if local_prefix == \"\":\n",
    "            proc = subprocess.Popen(['hadoop', 'fs', '-test', '-e', path])\n",
    "            proc.communicate()\n",
    "\n",
    "            if proc.returncode != 0:\n",
    "                return False\n",
    "            else : \n",
    "                return True\n",
    "        else:\n",
    "            return os.path.isfile(path)  \n",
    "    \n",
    "    def shuffle_data_by_user(self, df):\n",
    "        window_spec = Window.partitionBy(df.review_id).orderBy(df.positive)\n",
    "        return df.withColumn(\"row_number\", row_number().over(window_spec))\n",
    "        #return df.withColumn(\"row_number\", row_number().over(window_spec)).drop(\"row_number\")\n",
    "    \n",
    "    def __init__(self, spark, reviews_info_df, item_info_df):\n",
    "        self.reviews_info_df = reviews_info_df\n",
    "        self.item_info_df = item_info_df\n",
    "\n",
    "        # same as meta_map in process_data.py\n",
    "        self.meta_map_df = self.item_info_df\\\n",
    "                        .groupby(\"movie_id\")\\\n",
    "                        .agg(first(\"category\").alias(\"category\"))\n",
    "        self.meta_map_df.write.format('parquet').mode('overwrite').save(local_prefix + '/dien/output/meta_map')\n",
    "        self.meta_map_df = spark.read.parquet(local_prefix + '/dien/output/meta_map')\n",
    "        \n",
    "        if not self.if_local_test_exists():\n",
    "\n",
    "            #same as item_list in process_data.py\n",
    "            item_list_df = self.reviews_info_df\\\n",
    "                                    .groupby(\"movie_id\")\\\n",
    "                                    .count()\\\n",
    "                                    .drop('count')\n",
    "\n",
    "            # same as user_map in process_data.py\n",
    "            user_map_df = self.reviews_info_df \n",
    "\n",
    "            asin_len = item_list_df.count()\n",
    "            asin_list = [row['movie_id'] for row in item_list_df.select('movie_id').collect()]\n",
    "            broadcast_movie_id_list = spark.sparkContext.broadcast(asin_list)\n",
    "\n",
    "            def get_random_id(asin_total_len, asin):\n",
    "                item_list = broadcast_movie_id_list.value\n",
    "                asin_neg = asin\n",
    "                while True:\n",
    "                    asin_neg_index = random.randint(0, asin_total_len - 1)\n",
    "                    asin_neg = item_list[asin_neg_index]\n",
    "                    if asin_neg == asin:\n",
    "                        continue\n",
    "                    else:\n",
    "                        break\n",
    "                return asin_neg        \n",
    "\n",
    "            get_random_id_udf = udf(get_random_id, StringType())\n",
    "\n",
    "            ## manual join ##\n",
    "            # same as line 66-75 in process_data.py\n",
    "            negative_df =  user_map_df\\\n",
    "                            .withColumn('positive', lit(0))\\\n",
    "                            .withColumn('rand_false_mid', get_random_id_udf(lit(asin_len), \"movie_id\"))\\\n",
    "                            .join(self.meta_map_df, col(\"rand_false_mid\") == self.meta_map_df.movie_id, 'left_outer')\\\n",
    "                            .withColumn('category', when(col('category').isNotNull(), col('category')).otherwise(\"default_cat\"))\\\n",
    "                            .select(\"positive\", \"review_id\", self.meta_map_df.movie_id, \"overall\", \"unix_review_time\", \"category\")\\\n",
    "\n",
    "            positive_df = user_map_df\\\n",
    "                            .withColumn('positive', lit(1))\\\n",
    "                            .join(self.meta_map_df, 'movie_id', 'left_outer')\\\n",
    "                            .withColumn('category', when(col('category').isNotNull(), col('category')).otherwise(\"default_cat\"))\\\n",
    "                            .select(\"positive\", \"review_id\", \"movie_id\", \"overall\", \"unix_review_time\", \"category\")\\\n",
    "\n",
    "            ## split_test ##\n",
    "            # last two records of one user(one positive one negative) set tag 20190119\n",
    "            # other previous records of one user set tag 20180118\n",
    "\n",
    "            ## local_aggregator\n",
    "            # all tag with 20190119 will be wrote to local_test\n",
    "            last_positive_with_concat_df = positive_df\\\n",
    "                                                .groupby('review_id')\\\n",
    "                                                .agg(last('movie_id').alias('movie_id'),\\\n",
    "                                                     last('category').alias('category'),\\\n",
    "                                                     collect_list('movie_id').alias(\"concated_movie_id\"),\\\n",
    "                                                     collect_list('category').alias(\"concated_category\"),\\\n",
    "                                                     count(\"*\").alias(\"numItemsByUser\"))\\\n",
    "                                                .withColumn(\"concated_movie_id\", expr(\"slice(concated_movie_id, 1, numItemsByUser - 1)\"))\\\n",
    "                                                .withColumn(\"concated_category\", expr(\"slice(concated_category, 1, numItemsByUser - 1)\"))\\\n",
    "                                                .withColumn(\"numItemsByUser\", expr(\"numItemsByUser - 1\"))\\\n",
    "                                                .filter(col(\"numItemsByUser\") > 0)\n",
    "            \n",
    "            ## save data ############################################################################################\n",
    "            last_positive_with_concat_df.write.format('parquet').mode('overwrite').save(local_prefix + '/dien/output/aggregated_records')\n",
    "            reload_last_positive_with_concat_df = spark.read.parquet(local_prefix + '/dien/output/aggregated_records')\n",
    "            \n",
    "            # by saving to local, we can ensure negative record and positive record will have same history sequence\n",
    "            #########################################################################################################\n",
    "                  \n",
    "            last_negative_record_of_user_df = negative_df\\\n",
    "                                                .groupby('review_id')\\\n",
    "                                                .agg(last('positive').alias('positive'),\\\n",
    "                                                     last('movie_id').alias('movie_id'),\\\n",
    "                                                     last('category').alias('category'))\n",
    "            last_negative_record_of_user_df = reload_last_positive_with_concat_df\\\n",
    "                                                .join(last_negative_record_of_user_df, 'review_id', 'inner')\\\n",
    "                                                .select(\\\n",
    "                                                        'review_id',\\\n",
    "                                                        'positive',\\\n",
    "                                                        last_negative_record_of_user_df.movie_id.alias('movie_id'),\\\n",
    "                                                        last_negative_record_of_user_df.category.alias('category'),\\\n",
    "                                                        'concated_movie_id',\\\n",
    "                                                        'concated_category',\\\n",
    "                                                        'numItemsByUser')\n",
    "            last_positive_record_of_user_df = reload_last_positive_with_concat_df\\\n",
    "                                                .select('review_id',\\\n",
    "                                                        lit(1).alias('positive'),\\\n",
    "                                                        'movie_id',\\\n",
    "                                                        'category',\\\n",
    "                                                        'concated_movie_id',\\\n",
    "                                                        'concated_category',\\\n",
    "                                                        'numItemsByUser')\n",
    "            union_records_df = last_negative_record_of_user_df\\\n",
    "                                .union(last_positive_record_of_user_df)\n",
    "\n",
    "            ## all local_test will be split with random 1:9 to local_train_splitByUser and local_test_splitByUser\n",
    "            reviews_groupby_user_df = self.rand_ordinal_n(reload_last_positive_with_concat_df, 10).select('review_id', 'ordinal')\n",
    "            union_concated_df = reviews_groupby_user_df\\\n",
    "                                .join(union_records_df, 'review_id', 'inner')\\\n",
    "                                .select('positive',\\\n",
    "                                        'review_id',\\\n",
    "                                        'movie_id',\\\n",
    "                                        'category',\\\n",
    "                                        'concated_movie_id',\\\n",
    "                                        'concated_category',\\\n",
    "                                        'numItemsByUser',\\\n",
    "                                        'ordinal')\n",
    "            ## save data ############################################################################################\n",
    "            union_concated_df.write.format('parquet').mode('overwrite').save(local_prefix + '/dien/output/local_test')\n",
    "            #########################################################################################################\n",
    "        \n",
    "        ## end if local_test exists ################################################################################\n",
    "        reload_union_concated_df = spark.read.parquet(local_prefix + '/dien/output/local_test')\n",
    "        \n",
    "        ## split aggregated_labled_df by 1:9\n",
    "        self.test_df = reload_union_concated_df.filter(col(\"ordinal\") == 2).drop(\"ordinal\")\n",
    "        self.train_df = reload_union_concated_df.filter(col(\"ordinal\") != 2).drop(\"ordinal\")\n",
    "        \n",
    "        ## use window function to make sure same user records stay together\n",
    "        self.test_df = self.shuffle_data_by_user(self.test_df)\n",
    "        self.train_df = self.shuffle_data_by_user(self.train_df)\n",
    "            \n",
    "        ## build uid_dict, mid_dict and cat_dict\n",
    "        columns = ['review_id', 'uid']\n",
    "        zero = spark.createDataFrame([(\"A1Y6U82N6TYZPI\",0)], columns)      \n",
    "        \n",
    "        self.uid_dict_df = zero.union(self.train_df\\\n",
    "                            .groupBy('review_id')\\\n",
    "                            .count()\\\n",
    "                            .withColumn('uid', row_number().over(Window.orderBy(desc('count'))))\\\n",
    "                            .drop(\"count\"))\n",
    "        \n",
    "        columns = ['movie_id', 'mid']\n",
    "        zero = spark.createDataFrame([(\"default_mid\",0)], columns)  \n",
    "        self.mid_dict_df = zero.union(self.train_df\\\n",
    "                            .withColumn(\"concated_movie_id\", array_union(col(\"concated_movie_id\"), array(col(\"movie_id\"))))\\\n",
    "                            .select(explode(col(\"concated_movie_id\")).alias(\"movie_id\"))\\\n",
    "                            .groupBy('movie_id')\\\n",
    "                            .count()\\\n",
    "                            .filter(col('movie_id') != \"default_mid\")\\\n",
    "                            .withColumn('mid', row_number().over(Window.orderBy(desc('count'))))\\\n",
    "                            .drop(\"count\"))\n",
    "        \n",
    "        columns = ['category', 'cat']\n",
    "        zero = spark.createDataFrame([(\"default_cat\",0)], columns) \n",
    "        self.cat_dict_df = zero.union(self.train_df\\\n",
    "                            .withColumn(\"concated_category\", array_union(col(\"concated_category\"), array(col(\"category\"))))\\\n",
    "                            .select(explode(col(\"concated_category\")).alias(\"category\"))\\\n",
    "                            .groupBy('category')\\\n",
    "                            .count()\\\n",
    "                            .filter(col('category') != \"default_cat\")\\\n",
    "                            .withColumn('cat', row_number().over(Window.orderBy(desc('count'))))\\\n",
    "                            .drop(\"count\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.3 s, sys: 592 ms, total: 9.89 s\n",
      "Wall time: 37.8 s\n",
      "CPU times: user 31.5 ms, sys: 12 ms, total: 43.5 ms\n",
      "Wall time: 24 s\n",
      "\n",
      "==================== Process Time =======================\n",
      "\n",
      "Total process took 61.873 secs\n",
      "Details:\n",
      "start spark took 0.003 secs\n",
      "process took 61.870 secs\n",
      "\n",
      "==========================================================\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    " \n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "t0 = timer()\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .master('yarn')\\\n",
    "    .appName(\"DIEN_DATA_PREPARE\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Enable Arrow-based columnar data transfers\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "t1 = timer()\n",
    "# Load reviews_info and item_info from HDFS\n",
    "#reviews_info_df, item_info_df = load_csv()\n",
    "reviews_info_df, item_info_df = load_pyspark_processed_csv()\n",
    "%time data_processor = DataProcessor(spark, reviews_info_df, item_info_df)\n",
    "%time data_processor.process()\n",
    "t2 = timer()\n",
    "print(\"\\n==================== Process Time =======================\\n\")\n",
    "print(\"Total process took %.3f secs\" % (t2 - t0))\n",
    "print(\"Details:\")\n",
    "print(\"start spark took %.3f secs\" % (t1 - t0))\n",
    "print(\"process took %.3f secs\" % (t2 - t1))\n",
    "print(\"\\n==========================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option2: using original python to convert (No need to run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convert initial csv from json took 868.023 secs\n"
     ]
    }
   ],
   "source": [
    "# Load original json as CSV\n",
    "import sys\n",
    "import random\n",
    "import time\n",
    "from time import time\n",
    "\n",
    "def process_meta(file):\n",
    "    fi = open(file, \"r\")\n",
    "    fo = open(\"item-info\", \"w\")\n",
    "    for line in fi:\n",
    "        obj = eval(line)\n",
    "        cat = obj[\"categories\"][0][-1]\n",
    "        print(obj[\"asin\"] + \"\\t\" + cat, file=fo)\n",
    "\n",
    "def process_reviews(file):\n",
    "    fi = open(file, \"r\")\n",
    "    user_map = {}\n",
    "    fo = open(\"reviews-info\", \"w\")\n",
    "    for line in fi:\n",
    "        obj = eval(line)\n",
    "        userID = obj[\"reviewerID\"]\n",
    "        itemID = obj[\"asin\"]\n",
    "        rating = obj[\"overall\"]\n",
    "        time = obj[\"unixReviewTime\"]\n",
    "        print(userID + \"\\t\" + itemID + \"\\t\" + str(rating) + \"\\t\" + str(time), file=fo)\n",
    "\n",
    "local_prefix = \"/mnt/nvme2/chendi/BlueWhale/x-deeplearning/xdl-algorithm-solution/DIEN/data/\"\n",
    "t0 = timer()\n",
    "process_meta(local_prefix + 'meta_Books.json')\n",
    "process_reviews(local_prefix + 'reviews_Books.json')\n",
    "t1 = timer()\n",
    "\n",
    "print(\"Convert initial csv from json took %.3f secs\" % (t1 - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
