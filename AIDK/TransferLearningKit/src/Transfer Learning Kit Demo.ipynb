{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09c721aa",
   "metadata": {},
   "source": [
    "# Content\n",
    "* ### 1. [Framework](#Framework)\n",
    "* ### 2. [Environment Setup](#Environment-setup)\n",
    "* ### 3. [Launch training](#Launch-training)\n",
    "* ### 4. [Optimizations](#Optimizations)\n",
    "* ### 5. [Performance Overview](#Performance-overview)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcac2a67",
   "metadata": {},
   "source": [
    "## 1. Framework\n",
    "\n",
    "\n",
    "Transfer Learning Kit is a general and convenient framework for transfer knowledge from pretrained model and/or source domain data to target task. Its objectives are:\n",
    "* Transfer knowledge from pretrained model with the same/different network structure, which greatly speedups training without accuracy regression.\n",
    "* Transfer knowledge from source domain data without target label.\n",
    "\n",
    "The hierarchy of Transfer Learning Kit is list below. And, there are 5 key components in our Transfer Learning Kit:\n",
    "\n",
    "1.\tBackbone Factory: creates a backbone net according to predefined backbone or user-provided backbone to make basic prediction. \n",
    "2.\tTask Finetunner: creates a pretrained finetuning schema (called “finetunner”) to transfer knowledge from a pretrained model to target model with the same network structure.\n",
    "3.\tDomain Adapter: creates a domain adaption net (called “adapter”) to transfer knowledge from source domain to target domain.\n",
    "4.\tKnowledge Distiller: creates a knowledge distillation net (called “distiller”) to transfer knowledge from teacher model to target model. \n",
    "5.\tTransferrable Model: creates a customized and transferrable model which is a wrapper of backbone, adapter and distiller.\n",
    "![Framework](../doc/imgs/framework.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87340cc8",
   "metadata": {},
   "source": [
    "### 1.1 Finetunner\n",
    "Transfer knowledge from pretrained model to target model with same network structure.\n",
    "\n",
    "* Pretrained models are generated by pretraining process, which is training specific model  on specific dataset and has been performed by DE-NAS, PyTorch, TensorFlow, or HuggingFace.\n",
    "* Finetunner retrieves  the pretrained model with same network structure, and copy pretrained weights from pretrained model to corresponding layer of target model, instead of random initialization for target mode.\n",
    "* Finetunner can greatly improve training speed, and usually achieves better performance.\n",
    "\n",
    "![Finetunner](../doc/imgs/finetunner.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b462e304",
   "metadata": {},
   "source": [
    "### 1.2 Distiller\n",
    "Transfer knowledge from a heavy model (teacher) to a light one (student) with different structure.\n",
    "\n",
    "* Teacher is a large model pretrained on specific dataset, which contains sufficient knowledge for this task, while the student model has much smaller structure. Distiller trains the student not only on the dataset, but also with the help of teacher’s knowledge.\n",
    "* Distiller can take use of the knowledge from the existing pretrained large models but use much less training time. It can also significantly improve the converge  speed and predicting accuracy of a small model, which is very helpful for inference.\n",
    "![Distiller](../doc/imgs/distiller.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5215cbde",
   "metadata": {},
   "source": [
    "### 1.3 Adapter\n",
    "Transfer knowledge from source domain(cheap labels) to target domain (label-free).\n",
    "\n",
    "* Direct applying pre-trained model into target domain always cannot work due to covariate shift and label shift,  while labeling could be expensive in some domains and delays the model deployment time, which make fine-tuning not working.\n",
    "* Adapter aims at reusing the transferable knowledge with the help of another labeled dataset with same learning task. That is, achieving better generalization with little labeled target dataset or achieving a competitive performance in label-free target dataset.\n",
    "![Adapter](../doc/imgs/adapter.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940b6b99",
   "metadata": {},
   "source": [
    "### 1.4 Transferrable Model\n",
    "\n",
    "A transferrable model is a container, which contains a backbone (the original model), a finetunner, an adapter, a distiller, and is used to enhance backbone with transfer learning ability.\n",
    "\n",
    "We can use to make a model to be transferrable:\n",
    "\n",
    "```\n",
    "transferrable_model = make_transferrable (model, loss, finetunner, distiller, adapter,...)\n",
    "```\n",
    "Then we can use transferrable_model to train like original model with the help of transfer learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2fe6b4",
   "metadata": {},
   "source": [
    "## 2. Environment Setup\n",
    "\n",
    "1. build docker image\n",
    "   ```\n",
    "   cd Dockerfile-ubuntu18.04 && docker build -t aidk-pytorch110 . -f DockerfilePytorch110 && cd .. && yes | docker container prune && yes | docker image prune\n",
    "   ```\n",
    "2. run docker\n",
    "   ```\n",
    "   docker run --rm -t -d --name transfer_learning_kit --privileged --network host --shm-size=2g --device=/dev/dri \\\n",
    "   -v ${pretrained_model_path}:/home/vmagent/app/data/pretrained \\\n",
    "   -v ${output_path}:/home/vmagent/app/data/model_saved \\\n",
    "   -v ${dataset_path}:/home/vmagent/app/data/dataset \\\n",
    "   -v ${tlk_code_path}:/home/vmagent/app/TLK \\\n",
    "   -w /home/vmagent/app/TLK \\\n",
    "   aidk-pytorch110 /bin/bash\n",
    "   ``` \n",
    "3. Enter container with `docker exec -it transfer_learning_kit /bin/bash`\n",
    "\n",
    "4. Start the jupyter notebook service and tensorboard service\n",
    "   ```\n",
    "   source /opt/intel/oneapi/setvars.sh --ccl-configuration=cpu_icc --force\n",
    "   conda activate pytorch-1.10.0\n",
    "   pip install jupyter\n",
    "   nohup jupyter notebook --notebook-dir=/home/vmagent/app/TLK --ip=0.0.0.0 --port=8899 --allow-root &\n",
    "   nohup tensorboard --logdir /home/vmagent/app/data/model_saved --host=0.0.0.0 --port=6006 & \n",
    "   ```\n",
    "   Now you can visit TLK demo in `http://${hostname}:8899/`, and see tensorboad log in ` http://${hostname}:6006`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d70f725",
   "metadata": {},
   "source": [
    "## 3. Launch training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28f0017",
   "metadata": {},
   "source": [
    "### 3.1 Finetuner\n",
    "\n",
    "1. Run script to train Resnet50 (or MobileNetV3/VitBase) from scratch on CIFAR100 dataset in 1 epoch:\n",
    "    ```\n",
    "    python main_finetunner_cifar.py ../config/baseline/cifar100_resnet50_CosinLR.yaml --opts solver.epochs 1 \n",
    "   ```   \n",
    "2. Run script to finetune Resnet50 (or MobileNetV3/VitBase) from scratch on CIFAR100 dataset in 1 epoch:\n",
    "   ```\n",
    "    python main_finetunner_cifar.py ../config/finetuner/cifar100_res50PretrainI21k.yaml --opts solver.epochs 1 \n",
    "   ```\n",
    "\n",
    "We can see the result from tensorboard after training 1 epoch: finetuning achieves **81.19%** validation accuracy, while training from scratch achieves only **10.84%**.\n",
    "![Finetuner_Result1](../doc/imgs/finetuner_result.png)\n",
    "\n",
    "All of these models achive huge improvement when using finetuning in only 1 epoch or 2 epochs:\n",
    "- MobilenetV3 (1 epoch): From 6.69% (training from scratch) to 70.67% (training with finetune).\n",
    "- Resnet50(1 epoch): From 10.84% (training from scratch) to 81.77% (training with finetune).\n",
    "- VitBase(2 epoches): From 14.6% (training from scratch) to 64.83% (training with finetune).\n",
    "\n",
    "![Finetuner_Result2](../doc/imgs/finetuner_result2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ade2e7",
   "metadata": {},
   "source": [
    "**Training resnet50 on CIFAR100 from scratch:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e294640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Trial\n",
      "Model Name:resnet50\n",
      "Data Name:cifar100\n",
      "Transfer Learning Strategy:\n",
      "Enable DDP:False\n",
      "Training epochs:1\n",
      "adapter:\n",
      "  feature_layer_name: x\n",
      "  feature_size: 500\n",
      "  type: ''\n",
      "dataset:\n",
      "  data_drop_last: false\n",
      "  num_workers: 1\n",
      "  path: /mnt/DP_disk1/dataset\n",
      "  test:\n",
      "    batch_size: 128\n",
      "  test_transform: pretrainI21k\n",
      "  train_transform: pretrainI21k\n",
      "  type: cifar100\n",
      "  val:\n",
      "    batch_size: 128\n",
      "distiller:\n",
      "  check_logits: false\n",
      "  feature_layer_name: x\n",
      "  feature_size: ''\n",
      "  logits_path: ''\n",
      "  logits_topk: 0\n",
      "  save_logits: false\n",
      "  save_logits_start_epoch: 1\n",
      "  teacher:\n",
      "    is_frozen: true\n",
      "    pretrain: ''\n",
      "    type: resnet50_v2\n",
      "  type: ''\n",
      "  use_saved_logits: false\n",
      "experiment:\n",
      "  log_interval_step: 10\n",
      "  loss:\n",
      "    adapter: 0.0\n",
      "    backbone: 1.0\n",
      "    distiller: 0.0\n",
      "  model_save: /home/vmagent/app/data/model\n",
      "  model_save_interval: 40\n",
      "  project: finetuner\n",
      "  seed: 0\n",
      "  strategy: ''\n",
      "  tag: cifar100_res50_PretrainI21k\n",
      "  tensorboard_dir: /home/vmagent/app/data/model/finetuner/cifar100_res50_PretrainI21k/tensorboard_log/run_resnet50_1666577320\n",
      "  tensorboard_filename_suffix: ''\n",
      "finetuner:\n",
      "  finetuned_lr: 0.01\n",
      "  frozen: false\n",
      "  pretrain: ''\n",
      "  pretrained_num_classes: 10\n",
      "  type: ''\n",
      "optimize:\n",
      "  enable_ipex: true\n",
      "profiler:\n",
      "  active: 2\n",
      "  activities: cpu\n",
      "  repeat: 1\n",
      "  skip_first: 1\n",
      "  trace_file_inference: /home/vmagent/app/data/model/finetuner/cifar100_res50_PretrainI21k/profile/test_profile_resnet50_1666577320\n",
      "  trace_file_training: /home/vmagent/app/data/model/finetuner/cifar100_res50_PretrainI21k/profile/training_profile_resnet50_1666577320\n",
      "  wait: 1\n",
      "  warmup: 1\n",
      "solver:\n",
      "  batch_size: 128\n",
      "  early_stop:\n",
      "    delta: 0.001\n",
      "    flag: true\n",
      "    is_max: true\n",
      "    limitation: 1.0\n",
      "    metric: acc\n",
      "    tolerance_epoch: 200\n",
      "  epochs: 1\n",
      "  optimizer:\n",
      "    lr: 0.00753\n",
      "    momentum: 0.9\n",
      "    type: SGD\n",
      "    weight_decay: 0.00115\n",
      "  scheduler:\n",
      "    T_max: 200\n",
      "    lr_decay_rate: 0.1\n",
      "    lr_decay_stages:\n",
      "    - 150\n",
      "    - 180\n",
      "    - 210\n",
      "    patience: 10\n",
      "    type: CosineAnnealingLR\n",
      "  start_epoch: 1\n",
      "  warmup: 0\n",
      "source_dataset:\n",
      "  num_workers: 2\n",
      "  path: ''\n",
      "  test:\n",
      "    batch_size: 128\n",
      "  type: ''\n",
      "  val:\n",
      "    batch_size: 128\n",
      "\n",
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to /mnt/DP_disk1/dataset/cifar-100-python.tar.gz\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.029480934143066406,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 169001437,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbcbbac48b8f444685a350d5c149108e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/169001437 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /mnt/DP_disk1/dataset/cifar-100-python.tar.gz to /mnt/DP_disk1/dataset\n",
      "Files already downloaded and verified\n",
      "Model params:  23712932\n",
      "Epoch [1] lr: [0.00753]\n",
      "[2022-10-24 02:10:24]  epoch(1) step (0/391) Train: loss = 5.6154;\tacc = 0.0000\n",
      "[2022-10-24 02:10:40]  epoch(1) step (10/391) Train: loss = 5.0343;\tacc = 0.0234\n",
      "[2022-10-24 02:10:48]  epoch(1) step (20/391) Train: loss = 4.8966;\tacc = 0.0234\n",
      "[2022-10-24 02:10:56]  epoch(1) step (30/391) Train: loss = 5.4148;\tacc = 0.0234\n",
      "[2022-10-24 02:11:04]  epoch(1) step (40/391) Train: loss = 5.3916;\tacc = 0.0078\n",
      "[2022-10-24 02:11:12]  epoch(1) step (50/391) Train: loss = 5.4395;\tacc = 0.0078\n",
      "[2022-10-24 02:11:20]  epoch(1) step (60/391) Train: loss = 5.2647;\tacc = 0.0078\n",
      "[2022-10-24 02:11:27]  epoch(1) step (70/391) Train: loss = 4.9104;\tacc = 0.0312\n",
      "[2022-10-24 02:11:35]  epoch(1) step (80/391) Train: loss = 4.9845;\tacc = 0.0469\n",
      "[2022-10-24 02:11:43]  epoch(1) step (90/391) Train: loss = 4.6626;\tacc = 0.0156\n",
      "[2022-10-24 02:11:50]  epoch(1) step (100/391) Train: loss = 4.5900;\tacc = 0.0234\n",
      "[2022-10-24 02:12:04]  epoch(1) step (110/391) Train: loss = 4.4542;\tacc = 0.0469\n",
      "[2022-10-24 02:12:13]  epoch(1) step (120/391) Train: loss = 4.9969;\tacc = 0.0234\n",
      "[2022-10-24 02:12:21]  epoch(1) step (130/391) Train: loss = 4.7688;\tacc = 0.0391\n",
      "[2022-10-24 02:12:29]  epoch(1) step (140/391) Train: loss = 4.7131;\tacc = 0.0078\n",
      "[2022-10-24 02:12:37]  epoch(1) step (150/391) Train: loss = 4.6870;\tacc = 0.0391\n",
      "[2022-10-24 02:12:44]  epoch(1) step (160/391) Train: loss = 4.6772;\tacc = 0.0469\n",
      "[2022-10-24 02:12:52]  epoch(1) step (170/391) Train: loss = 4.2885;\tacc = 0.0703\n",
      "[2022-10-24 02:13:00]  epoch(1) step (180/391) Train: loss = 4.4532;\tacc = 0.0469\n",
      "[2022-10-24 02:13:07]  epoch(1) step (190/391) Train: loss = 4.2191;\tacc = 0.0469\n",
      "[2022-10-24 02:13:15]  epoch(1) step (200/391) Train: loss = 4.4275;\tacc = 0.0859\n",
      "[2022-10-24 02:13:29]  epoch(1) step (210/391) Train: loss = 4.3801;\tacc = 0.0234\n",
      "[2022-10-24 02:13:37]  epoch(1) step (220/391) Train: loss = 3.9879;\tacc = 0.0781\n",
      "[2022-10-24 02:13:45]  epoch(1) step (230/391) Train: loss = 4.2842;\tacc = 0.1016\n",
      "[2022-10-24 02:13:53]  epoch(1) step (240/391) Train: loss = 4.1871;\tacc = 0.0547\n",
      "[2022-10-24 02:14:01]  epoch(1) step (250/391) Train: loss = 4.0060;\tacc = 0.1328\n",
      "[2022-10-24 02:14:09]  epoch(1) step (260/391) Train: loss = 3.9727;\tacc = 0.1016\n",
      "[2022-10-24 02:14:16]  epoch(1) step (270/391) Train: loss = 4.0764;\tacc = 0.0859\n",
      "[2022-10-24 02:14:25]  epoch(1) step (280/391) Train: loss = 3.8670;\tacc = 0.1562\n",
      "[2022-10-24 02:14:32]  epoch(1) step (290/391) Train: loss = 4.2145;\tacc = 0.0703\n",
      "[2022-10-24 02:14:40]  epoch(1) step (300/391) Train: loss = 4.4132;\tacc = 0.0547\n",
      "[2022-10-24 02:14:54]  epoch(1) step (310/391) Train: loss = 4.0500;\tacc = 0.1094\n",
      "[2022-10-24 02:15:02]  epoch(1) step (320/391) Train: loss = 4.0910;\tacc = 0.0547\n",
      "[2022-10-24 02:15:10]  epoch(1) step (330/391) Train: loss = 4.3401;\tacc = 0.1016\n",
      "[2022-10-24 02:15:18]  epoch(1) step (340/391) Train: loss = 4.0369;\tacc = 0.1016\n",
      "[2022-10-24 02:15:26]  epoch(1) step (350/391) Train: loss = 3.9195;\tacc = 0.1172\n",
      "[2022-10-24 02:15:33]  epoch(1) step (360/391) Train: loss = 4.0105;\tacc = 0.0703\n",
      "[2022-10-24 02:15:41]  epoch(1) step (370/391) Train: loss = 4.0241;\tacc = 0.1094\n",
      "[2022-10-24 02:15:49]  epoch(1) step (380/391) Train: loss = 3.9162;\tacc = 0.1406\n",
      "[2022-10-24 02:15:57]  epoch(1) step (390/391) Train: loss = 3.9445;\tacc = 0.1000\n",
      "2022-10-24 02:16:03 0/79\n",
      "2022-10-24 02:16:05 10/79\n",
      "2022-10-24 02:16:07 20/79\n",
      "2022-10-24 02:16:09 30/79\n",
      "2022-10-24 02:16:11 40/79\n",
      "2022-10-24 02:16:13 50/79\n",
      "2022-10-24 02:16:15 60/79\n",
      "2022-10-24 02:16:17 70/79\n",
      "[2022-10-24 02:16:19]  epoch(1) Validation: acc = 0.1084;\tloss = 3.9488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/intel/oneapi/pytorch/latest/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:129: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Epoch: 1\n",
      "Epoch 1 took 357.83730149269104 seconds\n",
      "Total seconds:357.838774\n",
      "2022-10-24 02:16:21 0/79\n",
      "2022-10-24 02:16:24 10/79\n",
      "2022-10-24 02:16:27 20/79\n",
      "2022-10-24 02:16:29 30/79\n",
      "2022-10-24 02:16:32 40/79\n",
      "2022-10-24 02:16:35 50/79\n",
      "2022-10-24 02:16:37 60/79\n",
      "2022-10-24 02:16:40 70/79\n",
      "[2022-10-24 02:16:42]  epoch(0) Test: acc = 0.1084;\tloss = 3.9488\n",
      "Total seconds:21.774717\n",
      "Totally take 482.4181442260742 seconds\n"
     ]
    }
   ],
   "source": [
    "%run main.py --cfg ../config/baseline/cifar100_resnet50_CosinLR.yaml \\\n",
    "--opts solver.epochs 1 dataset.path /mnt/DP_disk1/dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c081c11",
   "metadata": {},
   "source": [
    "**Training resnet50 on CIFAR100 with pretraining:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "716f7fe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Trial\n",
      "Model Name:resnet50\n",
      "Data Name:cifar100\n",
      "Transfer Learning Strategy:OnlyFinetuneStrategy\n",
      "Enable DDP:False\n",
      "Training epochs:1\n",
      "adapter:\n",
      "  feature_layer_name: x\n",
      "  feature_size: 500\n",
      "  type: ''\n",
      "dataset:\n",
      "  data_drop_last: false\n",
      "  num_workers: 1\n",
      "  path: /home/vmagent/app/data/dataset\n",
      "  test:\n",
      "    batch_size: 128\n",
      "  test_transform: pretrainI21k\n",
      "  train_transform: pretrainI21k\n",
      "  type: cifar100\n",
      "  val:\n",
      "    batch_size: 128\n",
      "distiller:\n",
      "  check_logits: false\n",
      "  feature_layer_name: x\n",
      "  feature_size: ''\n",
      "  logits_path: ''\n",
      "  logits_topk: 0\n",
      "  save_logits: false\n",
      "  save_logits_start_epoch: 1\n",
      "  teacher:\n",
      "    is_frozen: true\n",
      "    pretrain: ''\n",
      "    type: resnet50_v2\n",
      "  type: ''\n",
      "  use_saved_logits: false\n",
      "experiment:\n",
      "  log_interval_step: 10\n",
      "  loss:\n",
      "    adapter: 0.0\n",
      "    backbone: 1.0\n",
      "    distiller: 0.0\n",
      "  model_save: /home/vmagent/app/data/model\n",
      "  model_save_interval: 40\n",
      "  project: finetuner\n",
      "  seed: 0\n",
      "  strategy: OnlyFinetuneStrategy\n",
      "  tag: cifar100_res50_PretrainI21k\n",
      "  tensorboard_dir: /home/vmagent/app/data/model/finetuner/cifar100_res50_PretrainI21k/tensorboard_log/run_resnet50_OnlyFinetuneStrategy_1666577936\n",
      "  tensorboard_filename_suffix: ''\n",
      "finetuner:\n",
      "  finetuned_lr: 0.00445\n",
      "  frozen: false\n",
      "  pretrain: /home/vmagent/app/data/pretrained/resnet50_miil_21k.pth\n",
      "  pretrained_num_classes: 11221\n",
      "  type: Basic\n",
      "optimize:\n",
      "  enable_ipex: true\n",
      "profiler:\n",
      "  active: 2\n",
      "  activities: cpu\n",
      "  repeat: 1\n",
      "  skip_first: 1\n",
      "  trace_file_inference: /home/vmagent/app/data/model/finetuner/cifar100_res50_PretrainI21k/profile/test_profile_resnet50_OnlyFinetuneStrategy_1666577936\n",
      "  trace_file_training: /home/vmagent/app/data/model/finetuner/cifar100_res50_PretrainI21k/profile/training_profile_resnet50_OnlyFinetuneStrategy_1666577936\n",
      "  wait: 1\n",
      "  warmup: 1\n",
      "solver:\n",
      "  batch_size: 128\n",
      "  early_stop:\n",
      "    delta: 0.001\n",
      "    flag: true\n",
      "    is_max: true\n",
      "    limitation: 1.0\n",
      "    metric: acc\n",
      "    tolerance_epoch: 200\n",
      "  epochs: 1\n",
      "  optimizer:\n",
      "    lr: 0.00753\n",
      "    momentum: 0.9\n",
      "    type: SGD\n",
      "    weight_decay: 0.00115\n",
      "  scheduler:\n",
      "    T_max: 200\n",
      "    lr_decay_rate: 0.1\n",
      "    lr_decay_stages:\n",
      "    - 150\n",
      "    - 180\n",
      "    - 210\n",
      "    patience: 10\n",
      "    type: CosineAnnealingLR\n",
      "  start_epoch: 1\n",
      "  warmup: 0\n",
      "source_dataset:\n",
      "  num_workers: 2\n",
      "  path: ''\n",
      "  test:\n",
      "    batch_size: 128\n",
      "  type: ''\n",
      "  val:\n",
      "    batch_size: 128\n",
      "\n",
      "Using downloaded and verified file: /home/vmagent/app/data/dataset/cifar-100-python.tar.gz\n",
      "Extracting /home/vmagent/app/data/dataset/cifar-100-python.tar.gz to /home/vmagent/app/data/dataset\n",
      "Files already downloaded and verified\n",
      "Model params:  23712932\n",
      "load pretrained model at /home/vmagent/app/data/pretrained/resnet50_miil_21k.pth\n",
      "could not load layer: fc.weight; mismatch shape: target [torch.Size([100, 2048])] != pretrained [torch.Size([11221, 2048])]\n",
      "could not load layer: fc.bias; mismatch shape: target [torch.Size([100])] != pretrained [torch.Size([11221])]\n",
      "Epoch [1] lr: [0.00445, 0.00753]\n",
      "[2022-10-24 02:19:03]  epoch(1) step (0/391) Train: total_loss = 4.7345;\tbackbone_loss = 4.7345;\tacc = 0.0078\n",
      "[2022-10-24 02:19:29]  epoch(1) step (10/391) Train: total_loss = 4.6532;\tbackbone_loss = 4.6532;\tacc = 0.0312\n",
      "[2022-10-24 02:19:48]  epoch(1) step (20/391) Train: total_loss = 4.3293;\tbackbone_loss = 4.3293;\tacc = 0.0781\n",
      "[2022-10-24 02:20:08]  epoch(1) step (30/391) Train: total_loss = 4.0303;\tbackbone_loss = 4.0303;\tacc = 0.1953\n",
      "[2022-10-24 02:20:26]  epoch(1) step (40/391) Train: total_loss = 3.7216;\tbackbone_loss = 3.7216;\tacc = 0.2578\n",
      "[2022-10-24 02:20:44]  epoch(1) step (50/391) Train: total_loss = 3.3256;\tbackbone_loss = 3.3256;\tacc = 0.2969\n",
      "[2022-10-24 02:21:03]  epoch(1) step (60/391) Train: total_loss = 2.7571;\tbackbone_loss = 2.7571;\tacc = 0.3906\n",
      "[2022-10-24 02:21:21]  epoch(1) step (70/391) Train: total_loss = 2.4403;\tbackbone_loss = 2.4403;\tacc = 0.4531\n",
      "[2022-10-24 02:21:39]  epoch(1) step (80/391) Train: total_loss = 1.8812;\tbackbone_loss = 1.8812;\tacc = 0.5312\n",
      "[2022-10-24 02:21:57]  epoch(1) step (90/391) Train: total_loss = 1.5435;\tbackbone_loss = 1.5435;\tacc = 0.6484\n",
      "[2022-10-24 02:22:15]  epoch(1) step (100/391) Train: total_loss = 1.6234;\tbackbone_loss = 1.6234;\tacc = 0.6094\n",
      "[2022-10-24 02:22:40]  epoch(1) step (110/391) Train: total_loss = 1.2155;\tbackbone_loss = 1.2155;\tacc = 0.7344\n",
      "[2022-10-24 02:22:57]  epoch(1) step (120/391) Train: total_loss = 1.1570;\tbackbone_loss = 1.1570;\tacc = 0.6953\n",
      "[2022-10-24 02:23:15]  epoch(1) step (130/391) Train: total_loss = 1.2724;\tbackbone_loss = 1.2724;\tacc = 0.6875\n",
      "[2022-10-24 02:23:33]  epoch(1) step (140/391) Train: total_loss = 1.2018;\tbackbone_loss = 1.2018;\tacc = 0.6562\n",
      "[2022-10-24 02:23:50]  epoch(1) step (150/391) Train: total_loss = 1.1207;\tbackbone_loss = 1.1207;\tacc = 0.6953\n",
      "[2022-10-24 02:24:08]  epoch(1) step (160/391) Train: total_loss = 1.0235;\tbackbone_loss = 1.0235;\tacc = 0.7266\n",
      "[2022-10-24 02:24:25]  epoch(1) step (170/391) Train: total_loss = 1.0235;\tbackbone_loss = 1.0235;\tacc = 0.7266\n",
      "[2022-10-24 02:24:43]  epoch(1) step (180/391) Train: total_loss = 1.0699;\tbackbone_loss = 1.0699;\tacc = 0.7031\n",
      "[2022-10-24 02:25:00]  epoch(1) step (190/391) Train: total_loss = 0.8481;\tbackbone_loss = 0.8481;\tacc = 0.7656\n",
      "[2022-10-24 02:25:19]  epoch(1) step (200/391) Train: total_loss = 0.8432;\tbackbone_loss = 0.8432;\tacc = 0.7812\n",
      "[2022-10-24 02:25:43]  epoch(1) step (210/391) Train: total_loss = 0.8084;\tbackbone_loss = 0.8084;\tacc = 0.7578\n",
      "[2022-10-24 02:26:01]  epoch(1) step (220/391) Train: total_loss = 0.8794;\tbackbone_loss = 0.8794;\tacc = 0.7344\n",
      "[2022-10-24 02:26:19]  epoch(1) step (230/391) Train: total_loss = 1.0200;\tbackbone_loss = 1.0200;\tacc = 0.7031\n",
      "[2022-10-24 02:26:37]  epoch(1) step (240/391) Train: total_loss = 0.6353;\tbackbone_loss = 0.6353;\tacc = 0.8359\n",
      "[2022-10-24 02:26:55]  epoch(1) step (250/391) Train: total_loss = 0.7896;\tbackbone_loss = 0.7896;\tacc = 0.8359\n",
      "[2022-10-24 02:27:13]  epoch(1) step (260/391) Train: total_loss = 0.9012;\tbackbone_loss = 0.9012;\tacc = 0.7266\n",
      "[2022-10-24 02:27:31]  epoch(1) step (270/391) Train: total_loss = 0.8877;\tbackbone_loss = 0.8877;\tacc = 0.7422\n",
      "[2022-10-24 02:27:48]  epoch(1) step (280/391) Train: total_loss = 0.7917;\tbackbone_loss = 0.7917;\tacc = 0.7656\n",
      "[2022-10-24 02:28:06]  epoch(1) step (290/391) Train: total_loss = 0.7530;\tbackbone_loss = 0.7530;\tacc = 0.7734\n",
      "[2022-10-24 02:28:23]  epoch(1) step (300/391) Train: total_loss = 0.8383;\tbackbone_loss = 0.8383;\tacc = 0.7344\n",
      "[2022-10-24 02:28:46]  epoch(1) step (310/391) Train: total_loss = 0.6723;\tbackbone_loss = 0.6723;\tacc = 0.7812\n",
      "[2022-10-24 02:29:03]  epoch(1) step (320/391) Train: total_loss = 0.7860;\tbackbone_loss = 0.7860;\tacc = 0.7344\n",
      "[2022-10-24 02:29:22]  epoch(1) step (330/391) Train: total_loss = 0.7390;\tbackbone_loss = 0.7390;\tacc = 0.7969\n",
      "[2022-10-24 02:29:39]  epoch(1) step (340/391) Train: total_loss = 0.9617;\tbackbone_loss = 0.9617;\tacc = 0.7109\n",
      "[2022-10-24 02:29:56]  epoch(1) step (350/391) Train: total_loss = 0.6398;\tbackbone_loss = 0.6398;\tacc = 0.8125\n",
      "[2022-10-24 02:30:14]  epoch(1) step (360/391) Train: total_loss = 0.6613;\tbackbone_loss = 0.6613;\tacc = 0.7812\n",
      "[2022-10-24 02:30:31]  epoch(1) step (370/391) Train: total_loss = 0.7564;\tbackbone_loss = 0.7564;\tacc = 0.7656\n",
      "[2022-10-24 02:30:49]  epoch(1) step (380/391) Train: total_loss = 0.7092;\tbackbone_loss = 0.7092;\tacc = 0.7969\n",
      "[2022-10-24 02:31:07]  epoch(1) step (390/391) Train: total_loss = 0.7488;\tbackbone_loss = 0.7488;\tacc = 0.7625\n",
      "2022-10-24 02:31:14 0/79\n",
      "2022-10-24 02:31:20 10/79\n",
      "2022-10-24 02:31:26 20/79\n",
      "2022-10-24 02:31:32 30/79\n",
      "2022-10-24 02:31:38 40/79\n",
      "2022-10-24 02:31:44 50/79\n",
      "2022-10-24 02:31:50 60/79\n",
      "2022-10-24 02:31:57 70/79\n",
      "[2022-10-24 02:32:01]  epoch(1) Validation: acc = 0.8119;\tloss = 0.6312\n",
      "Best Epoch: 1\n",
      "Epoch 1 took 781.8780789375305 seconds\n",
      "Total seconds:781.878842\n",
      "2022-10-24 02:32:03 0/79\n",
      "2022-10-24 02:32:11 10/79\n",
      "2022-10-24 02:32:18 20/79\n",
      "2022-10-24 02:32:25 30/79\n",
      "2022-10-24 02:32:32 40/79\n",
      "2022-10-24 02:32:38 50/79\n",
      "2022-10-24 02:32:45 60/79\n",
      "2022-10-24 02:32:52 70/79\n",
      "[2022-10-24 02:32:57]  epoch(0) Test: acc = 0.8119;\tloss = 0.6312\n",
      "Total seconds:54.523782\n",
      "Totally take 841.1806983947754 seconds\n"
     ]
    }
   ],
   "source": [
    "%run main.py --cfg ../config/finetuner/cifar100_res50PretrainI21k.yaml \\\n",
    "--opts solver.epochs 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d039058",
   "metadata": {},
   "source": [
    "### 3.2 Distiller\n",
    "\n",
    "1. Run script to train Resnet18 from scratch on CIFAR100 dataset:\n",
    "   ```\n",
    "    python main.py --cfg ../config/demo/cifar100_resnet18.yaml\n",
    "   ```   \n",
    "2. Run script to apply distiller from resnet50 to resnet18 on CIFAR100 dataset:\n",
    "   ```\n",
    "    python main.py --cfg ../config/demo/cifar100_kd_res50_res18.yaml\n",
    "   ```\n",
    "\n",
    "From the result we can see, with distiller, resnet18 can achieve accuracy **81.46%** in only 4.1h, while training from scratch can only achieve **75.98%** with 7.3h.\n",
    "![Distiller_Result1](../doc/imgs/kd_res50_res18.png)\n",
    "\n",
    "We can also apply distiller to other models, such as VIT to ResNet18 or ResNet50 to DeNas generated CNN. In these cases, distiller can always both speedup the converage and improve the accuracy.\n",
    "- ResNet18 (distillation from ResNet50): get 1.8x training time speedup and +5.5% accuracy improvement.\n",
    "- ResNet18 (distillation from VIT): get 158x training time speedup and +3.7% accuracy improvement.\n",
    "- DeNas generated CNN (distillation from ResNet50): get 1.7x training time speedup and +0.1% accuracy improvement.\n",
    "\n",
    "\n",
    "![Distiller_Result2](../doc/imgs/kd_3models.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4371a9",
   "metadata": {},
   "source": [
    "**Training resnet18 on CIFAR100 from scratch (for 1 epoch):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f334d6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Trial\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Non-existent config key: solver.scheduler.ReduceLROnPlateau'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/home/vmagent/app/TLK/src/main.py:178\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo Trial\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 178\u001b[0m     \u001b[43mobjective\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotally take \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(time\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;241m-\u001b[39mstart_time)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/home/vmagent/app/TLK/src/main.py:33\u001b[0m, in \u001b[0;36mobjective\u001b[0;34m(args, trial)\u001b[0m\n\u001b[1;32m     31\u001b[0m     os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMASTER_PORT\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m8089\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.gpu_id\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/vmagent/app/TLK/src/main.py:49\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(args, trial)\u001b[0m\n\u001b[1;32m     47\u001b[0m is_distributed \u001b[38;5;241m=\u001b[39m (world_size \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# distributed flag\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mcfg:\n\u001b[0;32m---> 49\u001b[0m     \u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmerge_from_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mopts:\n\u001b[1;32m     51\u001b[0m     cfg\u001b[38;5;241m.\u001b[39mmerge_from_list(args\u001b[38;5;241m.\u001b[39mopts)\n",
      "File \u001b[0;32m/opt/intel/oneapi/pytorch/latest/lib/python3.9/site-packages/yacs/config.py:213\u001b[0m, in \u001b[0;36mCfgNode.merge_from_file\u001b[0;34m(self, cfg_filename)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(cfg_filename, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    212\u001b[0m     cfg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_cfg(f)\n\u001b[0;32m--> 213\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmerge_from_other_cfg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/intel/oneapi/pytorch/latest/lib/python3.9/site-packages/yacs/config.py:217\u001b[0m, in \u001b[0;36mCfgNode.merge_from_other_cfg\u001b[0;34m(self, cfg_other)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmerge_from_other_cfg\u001b[39m(\u001b[38;5;28mself\u001b[39m, cfg_other):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;124;03m\"\"\"Merge `cfg_other` into this CfgNode.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 217\u001b[0m     \u001b[43m_merge_a_into_b\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg_other\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/intel/oneapi/pytorch/latest/lib/python3.9/site-packages/yacs/config.py:478\u001b[0m, in \u001b[0;36m_merge_a_into_b\u001b[0;34m(a, b, root, key_list)\u001b[0m\n\u001b[1;32m    476\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, CfgNode):\n\u001b[1;32m    477\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 478\u001b[0m         \u001b[43m_merge_a_into_b\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_list\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:\n\u001b[1;32m    480\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[0;32m/opt/intel/oneapi/pytorch/latest/lib/python3.9/site-packages/yacs/config.py:478\u001b[0m, in \u001b[0;36m_merge_a_into_b\u001b[0;34m(a, b, root, key_list)\u001b[0m\n\u001b[1;32m    476\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, CfgNode):\n\u001b[1;32m    477\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 478\u001b[0m         \u001b[43m_merge_a_into_b\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_list\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:\n\u001b[1;32m    480\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[0;32m/opt/intel/oneapi/pytorch/latest/lib/python3.9/site-packages/yacs/config.py:491\u001b[0m, in \u001b[0;36m_merge_a_into_b\u001b[0;34m(a, b, root, key_list)\u001b[0m\n\u001b[1;32m    489\u001b[0m     root\u001b[38;5;241m.\u001b[39mraise_key_rename_error(full_key)\n\u001b[1;32m    490\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 491\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNon-existent config key: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(full_key))\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Non-existent config key: solver.scheduler.ReduceLROnPlateau'"
     ]
    }
   ],
   "source": [
    "%run main.py --cfg ../config/demo/cifar100_resnet18.yaml --opts solver.epochs 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32256a5",
   "metadata": {},
   "source": [
    "**Training resnet18 with distillation from ResNet50 on CIFAR100 (for 1 epoch):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cca48412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Trial\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Non-existent config key: solver.scheduler.ReduceLROnPlateau'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/home/vmagent/app/TLK/src/main.py:178\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo Trial\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 178\u001b[0m     \u001b[43mobjective\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotally take \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(time\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;241m-\u001b[39mstart_time)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/home/vmagent/app/TLK/src/main.py:33\u001b[0m, in \u001b[0;36mobjective\u001b[0;34m(args, trial)\u001b[0m\n\u001b[1;32m     31\u001b[0m     os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMASTER_PORT\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m8089\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.gpu_id\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/vmagent/app/TLK/src/main.py:49\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(args, trial)\u001b[0m\n\u001b[1;32m     47\u001b[0m is_distributed \u001b[38;5;241m=\u001b[39m (world_size \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# distributed flag\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mcfg:\n\u001b[0;32m---> 49\u001b[0m     \u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmerge_from_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mopts:\n\u001b[1;32m     51\u001b[0m     cfg\u001b[38;5;241m.\u001b[39mmerge_from_list(args\u001b[38;5;241m.\u001b[39mopts)\n",
      "File \u001b[0;32m/opt/intel/oneapi/pytorch/latest/lib/python3.9/site-packages/yacs/config.py:213\u001b[0m, in \u001b[0;36mCfgNode.merge_from_file\u001b[0;34m(self, cfg_filename)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(cfg_filename, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    212\u001b[0m     cfg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_cfg(f)\n\u001b[0;32m--> 213\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmerge_from_other_cfg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/intel/oneapi/pytorch/latest/lib/python3.9/site-packages/yacs/config.py:217\u001b[0m, in \u001b[0;36mCfgNode.merge_from_other_cfg\u001b[0;34m(self, cfg_other)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmerge_from_other_cfg\u001b[39m(\u001b[38;5;28mself\u001b[39m, cfg_other):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;124;03m\"\"\"Merge `cfg_other` into this CfgNode.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 217\u001b[0m     \u001b[43m_merge_a_into_b\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg_other\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/intel/oneapi/pytorch/latest/lib/python3.9/site-packages/yacs/config.py:478\u001b[0m, in \u001b[0;36m_merge_a_into_b\u001b[0;34m(a, b, root, key_list)\u001b[0m\n\u001b[1;32m    476\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, CfgNode):\n\u001b[1;32m    477\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 478\u001b[0m         \u001b[43m_merge_a_into_b\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_list\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:\n\u001b[1;32m    480\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[0;32m/opt/intel/oneapi/pytorch/latest/lib/python3.9/site-packages/yacs/config.py:478\u001b[0m, in \u001b[0;36m_merge_a_into_b\u001b[0;34m(a, b, root, key_list)\u001b[0m\n\u001b[1;32m    476\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, CfgNode):\n\u001b[1;32m    477\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 478\u001b[0m         \u001b[43m_merge_a_into_b\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_list\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:\n\u001b[1;32m    480\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[0;32m/opt/intel/oneapi/pytorch/latest/lib/python3.9/site-packages/yacs/config.py:491\u001b[0m, in \u001b[0;36m_merge_a_into_b\u001b[0;34m(a, b, root, key_list)\u001b[0m\n\u001b[1;32m    489\u001b[0m     root\u001b[38;5;241m.\u001b[39mraise_key_rename_error(full_key)\n\u001b[1;32m    490\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 491\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNon-existent config key: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(full_key))\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Non-existent config key: solver.scheduler.ReduceLROnPlateau'"
     ]
    }
   ],
   "source": [
    "%run main.py --cfg ../config/demo/cifar100_kd_res50_res18.yaml --opts solver.epochs 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a3bedf",
   "metadata": {},
   "source": [
    "**Training resnet18 with distillation from VIT on CIFAR100 (for 1 epoch):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d2566061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Trial\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Non-existent config key: solver.scheduler.ReduceLROnPlateau'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/home/vmagent/app/TLK/src/main.py:178\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo Trial\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 178\u001b[0m     \u001b[43mobjective\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotally take \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(time\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;241m-\u001b[39mstart_time)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/home/vmagent/app/TLK/src/main.py:33\u001b[0m, in \u001b[0;36mobjective\u001b[0;34m(args, trial)\u001b[0m\n\u001b[1;32m     31\u001b[0m     os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMASTER_PORT\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m8089\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.gpu_id\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/vmagent/app/TLK/src/main.py:49\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(args, trial)\u001b[0m\n\u001b[1;32m     47\u001b[0m is_distributed \u001b[38;5;241m=\u001b[39m (world_size \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# distributed flag\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mcfg:\n\u001b[0;32m---> 49\u001b[0m     \u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmerge_from_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mopts:\n\u001b[1;32m     51\u001b[0m     cfg\u001b[38;5;241m.\u001b[39mmerge_from_list(args\u001b[38;5;241m.\u001b[39mopts)\n",
      "File \u001b[0;32m/opt/intel/oneapi/pytorch/latest/lib/python3.9/site-packages/yacs/config.py:213\u001b[0m, in \u001b[0;36mCfgNode.merge_from_file\u001b[0;34m(self, cfg_filename)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(cfg_filename, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    212\u001b[0m     cfg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_cfg(f)\n\u001b[0;32m--> 213\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmerge_from_other_cfg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/intel/oneapi/pytorch/latest/lib/python3.9/site-packages/yacs/config.py:217\u001b[0m, in \u001b[0;36mCfgNode.merge_from_other_cfg\u001b[0;34m(self, cfg_other)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmerge_from_other_cfg\u001b[39m(\u001b[38;5;28mself\u001b[39m, cfg_other):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;124;03m\"\"\"Merge `cfg_other` into this CfgNode.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 217\u001b[0m     \u001b[43m_merge_a_into_b\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg_other\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/intel/oneapi/pytorch/latest/lib/python3.9/site-packages/yacs/config.py:478\u001b[0m, in \u001b[0;36m_merge_a_into_b\u001b[0;34m(a, b, root, key_list)\u001b[0m\n\u001b[1;32m    476\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, CfgNode):\n\u001b[1;32m    477\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 478\u001b[0m         \u001b[43m_merge_a_into_b\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_list\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:\n\u001b[1;32m    480\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[0;32m/opt/intel/oneapi/pytorch/latest/lib/python3.9/site-packages/yacs/config.py:478\u001b[0m, in \u001b[0;36m_merge_a_into_b\u001b[0;34m(a, b, root, key_list)\u001b[0m\n\u001b[1;32m    476\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, CfgNode):\n\u001b[1;32m    477\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 478\u001b[0m         \u001b[43m_merge_a_into_b\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_list\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:\n\u001b[1;32m    480\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[0;32m/opt/intel/oneapi/pytorch/latest/lib/python3.9/site-packages/yacs/config.py:491\u001b[0m, in \u001b[0;36m_merge_a_into_b\u001b[0;34m(a, b, root, key_list)\u001b[0m\n\u001b[1;32m    489\u001b[0m     root\u001b[38;5;241m.\u001b[39mraise_key_rename_error(full_key)\n\u001b[1;32m    490\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 491\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNon-existent config key: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(full_key))\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Non-existent config key: solver.scheduler.ReduceLROnPlateau'"
     ]
    }
   ],
   "source": [
    "%run main.py --cfg ../config/demo/cifar100_kd_vit_res18.yaml --opts solver.epochs 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e992746",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
