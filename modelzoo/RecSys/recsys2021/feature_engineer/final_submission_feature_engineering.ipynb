{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/env/bin/python\n",
    "\n",
    "import init\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql import *\n",
    "from pyspark import *\n",
    "import pyspark.sql.functions as f\n",
    "from timeit import default_timer as timer\n",
    "import logging\n",
    "from RecsysSchema import RecsysSchema\n",
    "from pyrecdp.data_processor import *\n",
    "from pyrecdp.utils import *\n",
    "import hashlib\n",
    "\n",
    "def categorifyAllFeatures(df, proc, output_name=\"categorified\", gen_dict=False):\n",
    "    dict_dfs = []\n",
    "    if gen_dict:\n",
    "        # only call below function when target dicts were not pre-prepared\n",
    "        op_multiItems = GenerateDictionary(\n",
    "            ['present_domains', 'present_links', 'hashtags'], doSplit=True)\n",
    "        op_singleItems = GenerateDictionary(['tweet_id', 'language', {'src_cols': [\n",
    "                                            'engaged_with_user_id', 'engaging_user_id'], 'col_name': 'user_id'}])\n",
    "        proc.reset_ops([op_multiItems, op_singleItems])\n",
    "        t1 = timer()\n",
    "        dict_dfs = proc.generate_dicts(df)\n",
    "        t2 = timer()\n",
    "        print(\"Generate Dictionary took %.3f\" % (t2 - t1))\n",
    "    else:\n",
    "        # or we can simply load from pre-gened\n",
    "        dict_names = ['hashtags', 'language', 'present_domains',\n",
    "                      'present_links', 'tweet_id', 'user_id']\n",
    "        dict_dfs = [{'col_name': name, 'dict': proc.spark.read.parquet(\n",
    "            \"%s/%s/%s/%s\" % (proc.path_prefix, proc.current_path, proc.dicts_path, name))} for name in dict_names]\n",
    "\n",
    "    # pre-defined dict\n",
    "    # pre-define\n",
    "    media = {\n",
    "        '': 0,\n",
    "        'GIF': 1,\n",
    "        'GIF_GIF': 2,\n",
    "        'GIF_Photo': 3,\n",
    "        'GIF_Video': 4,\n",
    "        'Photo': 5,\n",
    "        'Photo_GIF': 6,\n",
    "        'Photo_Photo': 7,\n",
    "        'Photo_Video': 8,\n",
    "        'Video': 9,\n",
    "        'Video_GIF': 10,\n",
    "        'Video_Photo': 11,\n",
    "        'Video_Video': 12\n",
    "    }\n",
    "\n",
    "    tweet_type = {'Quote': 0, 'Retweet': 1, 'TopLevel': 2}\n",
    "\n",
    "    media_df = proc.spark.createDataFrame(convert_to_spark_dict(media))\n",
    "    tweet_type_df = proc.spark.createDataFrame(\n",
    "        convert_to_spark_dict(tweet_type))\n",
    "\n",
    "    dict_dfs.append({'col_name': 'present_media', 'dict': media_df})\n",
    "    dict_dfs.append({'col_name': 'tweet_type', 'dict': tweet_type_df})\n",
    "\n",
    "    for i in dict_dfs:\n",
    "        dict_name = i['col_name']\n",
    "        dict_df = i['dict']\n",
    "        print(\"%s has numRows as %d\" % (dict_name, dict_df.count()))\n",
    "\n",
    "    ###### 2. define operations and append them to data processor ######\n",
    "\n",
    "    # 1. define operations\n",
    "    # 1.1 fill na and features\n",
    "    op_fillna_str = FillNA(\n",
    "        ['present_domains', 'present_links', 'hashtags', 'present_media', 'tweet_id'], \"\")\n",
    "    op_fillna_num = FillNA(['reply_timestamp', 'retweet_timestamp',\n",
    "                        'retweet_with_comment_timestamp', 'like_timestamp'], 0)\n",
    "    op_feature_modification_type_convert = FeatureModification(cols=['tweet_timestamp',\n",
    "                                                                     'engaged_with_user_follower_count',\n",
    "                                                                     'engaged_with_user_following_count',\n",
    "                                                                     'engaged_with_user_account_creation',\n",
    "                                                                     'engaging_user_follower_count',\n",
    "                                                                     'engaging_user_following_count',\n",
    "                                                                     'engaging_user_account_creation',\n",
    "                                                                     'reply_timestamp',\n",
    "                                                                     'retweet_timestamp',\n",
    "                                                                     'retweet_with_comment_timestamp',\n",
    "                                                                     'like_timestamp'], op='toInt')\n",
    "    op_feature_add_originals = FeatureAdd(\n",
    "        cols={\"original_present_domains\": \"f.col('present_domains')\",\\\n",
    "              \"original_present_links\": \"f.col('present_links')\",\\\n",
    "              \"original_hashtags\": \"f.col('hashtags')\",\\\n",
    "              \"original_language\": \"f.col('language')\",\\\n",
    "              \"original_tweet_id\": \"f.col('tweet_id')\",\\\n",
    "              \"original_present_media\": \"f.col('present_media')\",\\\n",
    "              \"original_tweet_type\": \"f.col('tweet_type')\",\\\n",
    "              \"original_engaged_with_user_id\": \"f.col('engaged_with_user_id')\",\\\n",
    "              \"original_engaging_user_id\": \"f.col('engaging_user_id')\"\n",
    "             }, \n",
    "        op='inline')\n",
    "    op_feature_modification_present_media_replace = FeatureModification(\n",
    "        cols={'present_media': \"f.concat_ws('_', f.slice(f.split(f.col('present_media'),'\\t'), 1, 2))\"}, op='inline')\n",
    "    op_feature_add_len_hashtags = FeatureAdd(\n",
    "        cols={'len_hashtags': \"f.when(f.col('hashtags') == '', f.lit(0)).otherwise(f.size(f.split(f.col('hashtags'), '\\t')))\"}, op='inline')\n",
    "    op_feature_add_len_domains = FeatureAdd(\n",
    "        cols={'len_domains': \"f.when(f.col('present_domains') == '', f.lit(0)).otherwise(f.size(f.split(f.col('present_domains'), '\\t')))\"}, op='inline')\n",
    "    op_feature_add_len_links = FeatureAdd(\n",
    "        cols={'len_links': \"f.when(f.col('present_links') == '', f.lit(0)).otherwise(f.size(f.split(f.col('present_links'), '\\t')))\"}, op='inline')\n",
    "    op_new_feature_dt_dow = FeatureAdd(cols={\n",
    "        \"dt_dow\": \"f.dayofweek(f.from_unixtime(f.col('tweet_timestamp'))).cast(t.IntegerType())\",\n",
    "        \"dt_hour\": \"f.hour(f.from_unixtime(f.col('tweet_timestamp'))).cast(t.IntegerType())\",\n",
    "        \"dt_minute\": \"f.minute(f.from_unixtime(f.col('tweet_timestamp'))).cast(t.IntegerType())\",\n",
    "        \"dt_second\": \"f.second(f.from_unixtime(f.col('tweet_timestamp'))).cast(t.IntegerType())\"}, op='inline')\n",
    "    op_feature_add_engage_time = FeatureAdd(\n",
    "        cols={'engage_time': \"f.least(f.col('reply_timestamp'), f.col('retweet_timestamp'), f.col('retweet_with_comment_timestamp'), f.col('like_timestamp'))\"}, op='inline')\n",
    "    op_feature_change = FeatureModification(cols={\n",
    "        \"reply_timestamp\": \"f.when(f.col('reply_timestamp') > 0, 1).otherwise(0)\",\n",
    "        \"retweet_timestamp\": \"f.when(f.col('retweet_timestamp') > 0, 1).otherwise(0)\",\n",
    "        \"retweet_with_comment_timestamp\": \"f.when(f.col('retweet_with_comment_timestamp') > 0, 1).otherwise(0)\",\n",
    "        \"like_timestamp\": \"f.when(f.col('like_timestamp') > 0, 1).otherwise(0)\"}, op='inline')\n",
    "    op_fillna_tweet_timestamp = FillNA(['tweet_timestamp'], -1)\n",
    "    \n",
    "    ops = [op_fillna_str, op_fillna_num,\n",
    "           op_feature_modification_type_convert, op_feature_add_originals, \n",
    "           op_feature_modification_present_media_replace,\n",
    "           op_feature_add_len_hashtags, op_feature_add_len_domains, op_feature_add_len_links,\n",
    "           op_new_feature_dt_dow, op_feature_add_engage_time, op_feature_change, op_fillna_tweet_timestamp]\n",
    "    proc.reset_ops(ops)\n",
    "\n",
    "    # 1.3 categorify\n",
    "    # since language dict is small, we may use udf to make partition more even\n",
    "    op_categorify_multi = Categorify(\n",
    "        ['present_domains', 'present_links', 'hashtags'], dict_dfs=dict_dfs, doSplit=True, keepMostFrequent=True)\n",
    "    op_categorify_1 = Categorify(['language', 'present_media', 'tweet_type'], dict_dfs=dict_dfs)\n",
    "    op_fillna_for_categorified = FillNA(['present_domains', 'present_links', 'hashtags', 'language',\n",
    "                                         'present_media', 'tweet_type', ], -1)\n",
    "    ops_1 = [op_categorify_multi, op_categorify_1, op_fillna_for_categorified]\n",
    "    proc.append_ops(ops_1)\n",
    "\n",
    "    t1 = timer()\n",
    "    df = proc.transform(df)\n",
    "    t2 = timer()\n",
    "    print(\"Data Process 1 and udf categorify took %.3f\" % (t2 - t1))\n",
    "    ### process 2\n",
    "    \n",
    "    op_categorify_2 = Categorify([{'engaged_with_user_id': 'user_id'}, {'engaging_user_id': 'user_id'}], dict_dfs=dict_dfs)\n",
    "    op_fillna_for_categorified = FillNA(['engaged_with_user_id', 'engaging_user_id'], -1)\n",
    "    ops = [op_categorify_2, op_fillna_for_categorified]\n",
    "    proc.reset_ops(ops)\n",
    "\n",
    "    t1 = timer()\n",
    "    df = proc.transform(df)\n",
    "    t2 = timer()\n",
    "    print(\"Data Process 2 and udf categorify took %.3f\" % (t2 - t1))\n",
    "    ### process 3\n",
    "    op_categorify_3 = Categorify(['tweet_id'], dict_dfs=dict_dfs)\n",
    "    op_fillna_for_categorified = FillNA(['tweet_id'], -1)\n",
    "    ops = [op_categorify_3, op_fillna_for_categorified]\n",
    "    proc.reset_ops(ops)\n",
    "\n",
    "    t1 = timer()\n",
    "    df = proc.transform(df, output_name)\n",
    "    t2 = timer()    \n",
    "    print(\"Data Process 3 and udf categorify took %.3f\" % (t2 - t1))\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def categorifyTweetText(df, proc, output_name=\"tweet_text_categorified\", gen_dict=False):\n",
    "    dict_dfs = []\n",
    "    if gen_dict:\n",
    "        # only call below function when target dicts were not pre-prepared\n",
    "        op_multiItems = GenerateDictionary(\n",
    "            ['tweet'], doSplit=True, withCount=True, sep=' ')\n",
    "        proc.reset_ops([op_multiItems])\n",
    "        ##### transform #####\n",
    "        t1 = timer()\n",
    "        dict_dfs = proc.generate_dicts(df)\n",
    "        t2 = timer()\n",
    "        print(\"Generate Dictionary took %.3f\" % (t2 - t1))\n",
    "    else:\n",
    "        # or we can simply load from pre-gened\n",
    "        name = \"tweet\"\n",
    "        tweet_dict_df = proc.spark.read.parquet(\n",
    "            \"%s/%s/%s/%s\" % (proc.path_prefix, proc.current_path, proc.dicts_path, name))\n",
    "        dict_dfs = [{'col_name': 'tweet', 'dict': tweet_dict_df}]        \n",
    "\n",
    "    tweet_dict_df = dict_dfs[0]['dict']\n",
    "    freqRange = [2, 100000]\n",
    "    tweet_dict_df = tweet_dict_df.filter((f.col('count') <= f.lit(\n",
    "        freqRange[1])) & (f.col('count') >= f.lit(freqRange[0])))\n",
    "    dict_dfs = [{'col_name': 'tweet', 'dict': tweet_dict_df}]\n",
    "    \n",
    "    op_fillNA = FillNA(['tweet'], \"\")\n",
    "    op_rename = FeatureAdd(cols={\"original_tweet\": \"f.col('tweet')\"}, op='inline')\n",
    "    op_categorify = Categorify(['tweet'], dict_dfs=dict_dfs, doSplit=True, sep=' ', doSortForArray=True)\n",
    "    proc.reset_ops([op_fillNA, op_rename, op_categorify])\n",
    "    t1 = timer()\n",
    "    df = proc.transform(df, name=output_name)\n",
    "    t2 = timer()\n",
    "    print(\"Categorify tweet took %.3f\" % (t2 - t1))\n",
    "    return df    \n",
    "\n",
    "\n",
    "def categorifyTweetHash(df, proc, output_name=\"tweet_text_processed\", gen_dict=False):\n",
    "    dict_dfs = []\n",
    "    if gen_dict:\n",
    "        # only call below function when target dicts were not pre-prepared\n",
    "        op_gen_dict = GenerateDictionary(\n",
    "            ['tw_hash','tw_first_word','tw_second_word','tw_last_word','tw_llast_word'])\n",
    "        proc.reset_ops([op_gen_dict])\n",
    "        ##### transform #####\n",
    "        t1 = timer()\n",
    "        dict_dfs = proc.generate_dicts(df)\n",
    "        t2 = timer()\n",
    "        print(\"Generate Dictionary took %.3f\" % (t2 - t1))\n",
    "    else:\n",
    "        # or we can simply load from pre-gened\n",
    "        dict_names = ['tw_hash','tw_first_word',\n",
    "                      'tw_second_word','tw_last_word','tw_llast_word']\n",
    "        dict_dfs = [{'col_name': name, 'dict': proc.spark.read.parquet(\n",
    "            \"%s/%s/%s/%s\" % (proc.path_prefix, proc.current_path, proc.dicts_path, name))} for name in dict_names]\n",
    "    \n",
    "    op_categorify = Categorify(['tw_first_word', 'tw_second_word', 'tw_last_word', 'tw_llast_word', 'tw_hash'], dict_dfs=dict_dfs, saveTmpToDisk=True)\n",
    "    proc.reset_ops([op_categorify])\n",
    "    t1 = timer()\n",
    "    df = proc.transform(df)\n",
    "    t2 = timer()\n",
    "    print(\"Categorify hash features took %.3f\" % (t2 - t1))\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def decodeBertTokenizer(df, proc, output_name=\"data_all_with_text\"):\n",
    "    from transformers import BertTokenizer\n",
    "    tokenizer = BertTokenizer.from_pretrained(\n",
    "        'bert-base-multilingual-cased', do_lower_case=False)\n",
    "\n",
    "    # define UDF\n",
    "    tokenizer_decode = f.udf(lambda x: tokenizer.decode(\n",
    "        [int(n) for n in x.split('\\t')]))\n",
    "    format_url = f.udf(lambda x: x.replace(\n",
    "        'https : / / t. co / ', 'https://t.co/').replace('@ ', '@'))\n",
    "\n",
    "    # define operations\n",
    "    op_feature_modification_tokenizer_decode = FeatureAdd(\n",
    "        cols={'tweet': 'text_tokens'}, udfImpl=tokenizer_decode)\n",
    "    op_feature_modification_format_url = FeatureModification(\n",
    "        cols=['tweet'], udfImpl=format_url)\n",
    "\n",
    "    # execute\n",
    "    proc.reset_ops([op_feature_modification_tokenizer_decode,\n",
    "                    op_feature_modification_format_url])\n",
    "    t1 = timer()\n",
    "    df = proc.transform(df, name=output_name)\n",
    "    t2 = timer()\n",
    "    print(\"BertTokenizer decode and format took %.3f\" % (t2 - t1))\n",
    "\n",
    "    return df\n",
    "\n",
    "def tweet_first_last_word(df, proc, output_name=\"data_with_word\"):\n",
    "    t1 = timer()\n",
    "    freqRange = [2, 100000]\n",
    "    name = \"tweet\"\n",
    "    tweet_dict_df = proc.spark.read.parquet(\n",
    "        \"%s/%s/%s/%s\" % (proc.path_prefix, proc.current_path, proc.dicts_path, name))\n",
    "    tweet_dict_df = tweet_dict_df.filter((f.col('count') <= f.lit(\n",
    "        freqRange[1])) & (f.col('count') >= f.lit(freqRange[0])))    \n",
    "    tweet_dict_df.show()\n",
    "    df = df.withColumn('row_id', spk_func.monotonically_increasing_id())\n",
    "    df.write.format('parquet').mode('overwrite').save(\"%s/%s/tmp_valid_with_row_id\" % (proc.path_prefix, proc.current_path))  \n",
    "    df = spark.read.parquet(\"%s/%s/tmp_valid_with_row_id\" % (proc.path_prefix, proc.current_path))\n",
    "    \n",
    "    tmp_df = df.select('row_id', 'tweet')\\\n",
    "               .withColumn('tweet_word', f.explode(f.split(f.col('tweet'), ' ')))\\\n",
    "               .join(tweet_dict_df.withColumnRenamed('dict_col', 'tweet_word').hint('shuffle_hash'), 'tweet_word', 'left')\\\n",
    "               .select('row_id', 'tweet_word', 'count')\n",
    "    tmp_df.write.format('parquet').mode('overwrite').save(\"%s/%s/tmp_valid_joined_tweet_word\" % (proc.path_prefix, proc.current_path))\n",
    "    tmp_df = spark.read.parquet(\"%s/%s/tmp_valid_joined_tweet_word\" % (proc.path_prefix, proc.current_path))\n",
    "    first_word_df = tmp_df\\\n",
    "           .withColumn('max_count', f.max('count').over(Window.partitionBy('row_id')))\\\n",
    "           .where((f.col('count') == f.col('max_count')))\\\n",
    "           .groupby('row_id').agg(f.first('tweet_word').alias('tw_first_word'))\\\n",
    "           .select('row_id', 'tw_first_word')\n",
    "    last_word_df = tmp_df\\\n",
    "           .withColumn('min_count', f.min('count').over(Window.partitionBy('row_id')))\\\n",
    "           .where((f.col('count') == f.col('min_count')))\\\n",
    "           .groupby('row_id').agg(f.last('tweet_word').alias('tw_last_word'))\\\n",
    "           .select('row_id', 'tw_last_word')\n",
    "    df = df.join(first_word_df.hint('shuffle_hash'), 'row_id', 'left')\\\n",
    "           .join(last_word_df.hint('shuffle_hash'), 'row_id', 'left')\\\n",
    "           .drop('row_id')\n",
    "    df.write.format('parquet').mode('overwrite').save(\"%s/%s/%s\" % (proc.path_prefix, proc.current_path, output_name))\n",
    "    \n",
    "    t2 = timer()\n",
    "    print(\"Feature Engineering for tweet text: encoded tweet column took %.3f\" % (t2 - t1))\n",
    "    return df\n",
    "\n",
    "\n",
    "def tweetFeatureEngineer(df, proc, output_name=\"tweet_feature_engineer\"):\n",
    "    \n",
    "    def extract_hash(text, split_text='@', no=0):\n",
    "        text = text.lower()\n",
    "        uhash = ''\n",
    "        text_split = text.split('@')\n",
    "        if len(text_split) > (no+1):\n",
    "            text_split = text_split[no+1].split(' ')\n",
    "            cl_loop = True\n",
    "            uhash += clean_text(text_split[0])\n",
    "            while cl_loop:\n",
    "                if len(text_split) > 1:\n",
    "                    if text_split[1] in ['_']:\n",
    "                        uhash += clean_text(text_split[1]) + \\\n",
    "                            clean_text(text_split[2])\n",
    "                        text_split = text_split[2:]\n",
    "                    else:\n",
    "                        cl_loop = False\n",
    "                else:\n",
    "                    cl_loop = False\n",
    "        hash_object = hashlib.md5(uhash.encode('utf-8'))\n",
    "        return hash_object.hexdigest()\n",
    "\n",
    "    def clean_text(text):\n",
    "        if len(text) > 1:\n",
    "            if text[-1] in ['!', '?', ':', ';', '.', ',']:\n",
    "                return(text[:-1])\n",
    "        return(text)\n",
    "\n",
    "    # features upon tweet\n",
    "    to_notsign = f.udf(lambda x: x.replace('\\[CLS\\] RT @', ''))\n",
    "    count_space = f.udf(lambda x: x.count(' '))\n",
    "    count_text_length = f.udf(lambda x: len(x))\n",
    "    user_defined_hash = f.udf(\n",
    "        lambda x: extract_hash(x, split_text='RT @', no=0))\n",
    "    # features upon tweet_nortsign\n",
    "    count_at = f.udf(lambda x: x.count('@'))\n",
    "    user_define_hash_1 = f.udf(lambda x: extract_hash(x))\n",
    "    user_define_hash_2 = f.udf(lambda x: extract_hash(x, no=1))\n",
    "\n",
    "    # features upon tweet\n",
    "    op_fillna_for_tweet = FillNA(['original_tweet'], \"\")\n",
    "    op_feature_add_tweet_nortsign = FeatureAdd(\n",
    "        cols={'tweet_nortsign': 'original_tweet'}, udfImpl=to_notsign)\n",
    "    op_feature_add_count_words = FeatureAdd(\n",
    "        cols={'count_words': 'original_tweet'}, udfImpl=count_space)\n",
    "    op_feature_add_count_char = FeatureAdd(\n",
    "        cols={'count_char': 'original_tweet'}, udfImpl=count_text_length)\n",
    "    op_feature_add_tw_uhash = FeatureAdd(\n",
    "        cols={'tw_uhash': 'original_tweet'}, udfImpl=user_defined_hash)\n",
    "    op_feature_add_tw_hash = FeatureAdd(\n",
    "        cols={'tw_hash': \"f.hash(f.col('original_tweet'))%1000000000\"}, op='inline')\n",
    "    # features upon tweet_nortsign\n",
    "    op_feature_add_count_at = FeatureAdd(\n",
    "        cols={'count_ats': 'tweet_nortsign'}, udfImpl=count_at)\n",
    "    op_feature_add_tw_uhash0 = FeatureAdd(\n",
    "        cols={'tw_hash0': 'tweet_nortsign'}, udfImpl=user_define_hash_1)\n",
    "    op_feature_add_tw_uhash1 = FeatureAdd(\n",
    "        cols={'tw_hash1': 'tweet_nortsign'}, udfImpl=user_define_hash_2)\n",
    "    op_feature_add_tw_first_word = FeatureAdd(\n",
    "        {'tw_first_word': \"f.col('tweet').getItem(0)\"}, op='inline')\n",
    "    op_feature_add_tw_second_word = FeatureAdd(\n",
    "        {'tw_second_word': \"f.col('tweet').getItem(1)\"}, op='inline')\n",
    "    op_feature_add_tw_last_word = FeatureAdd(\n",
    "        {'tw_last_word': \"f.col('tweet').getItem(f.size(f.col('tweet')) - 1)\"}, op='inline')\n",
    "    op_feature_add_tw_second_last_word = FeatureAdd(\n",
    "        {'tw_llast_word': \"f.col('tweet').getItem(f.size(f.col('tweet')) - 1)\"}, op='inline')\n",
    "    op_feature_add_tw_word_len = FeatureAdd(\n",
    "        {'tw_len': \"f.size(f.col('tweet'))\"}, op='inline')\n",
    "    op_feature_modification_fillna = FillNA(\n",
    "        ['tw_hash', 'tw_first_word', 'tw_second_word', 'tw_last_word', 'tw_llast_word', 'tw_len'], -1)\n",
    "\n",
    "    proc.reset_ops([op_fillna_for_tweet, \n",
    "                    op_feature_add_tweet_nortsign, op_feature_add_count_words, op_feature_add_count_char,\n",
    "                    op_feature_add_tw_uhash, op_feature_add_tw_hash,\n",
    "                    op_feature_add_count_at, op_feature_add_tw_uhash0, op_feature_add_tw_uhash1,\n",
    "                    op_feature_add_tw_first_word, op_feature_add_tw_second_word,\n",
    "                    op_feature_add_tw_last_word, op_feature_add_tw_second_last_word, op_feature_add_tw_word_len,\n",
    "                    op_feature_modification_fillna])\n",
    "    t1 = timer()\n",
    "    df = proc.transform(df, name=output_name)\n",
    "    t2 = timer()\n",
    "    print(\"feature engineering upon Frequency: encoded tweet column took %.3f\" % (t2 - t1))\n",
    "    return df\n",
    "\n",
    "def get_train_data_with_amount_of_days(df, proc, only_train=False, num_of_day = 20, output_name = \"data_splitted_by_day\"):\n",
    "    categorified_with_text_df = df\n",
    "    # categorified_with_text_df.cache()\n",
    "    # 1.1 get timestamp range\n",
    "    import datetime\n",
    "    min_timestamp = categorified_with_text_df.select('tweet_timestamp').agg({'tweet_timestamp': 'min'}).collect()[0]['min(tweet_timestamp)']\n",
    "    max_timestamp = categorified_with_text_df.select('tweet_timestamp').agg({'tweet_timestamp': 'max'}).collect()[0]['max(tweet_timestamp)']\n",
    "    seconds_in_day = 3600 * 24\n",
    "\n",
    "    print(\n",
    "        \"min_timestamp is %s, max_timestamp is %s, %d days max is %s\" % (\n",
    "            datetime.datetime.fromtimestamp(min_timestamp).strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            datetime.datetime.fromtimestamp(max_timestamp).strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            num_of_day,\n",
    "            datetime.datetime.fromtimestamp(min_timestamp + num_of_day * seconds_in_day).strftime('%Y-%m-%d %H:%M:%S')\n",
    "        ))\n",
    "\n",
    "    if only_train:\n",
    "        min_timestamp += 2 * seconds_in_day\n",
    "        num_of_day -= 4\n",
    "    time_range_split = {\n",
    "        'target': (min_timestamp, seconds_in_day * num_of_day + min_timestamp)\n",
    "    }\n",
    "\n",
    "    print(time_range_split)\n",
    "\n",
    "    # 1.2 save ranged data for train\n",
    "    # filtering out train range data and save\n",
    "    train_start, train_end = time_range_split['target']\n",
    "    df = categorified_with_text_df.filter(\n",
    "        (f.col('tweet_timestamp') >= f.lit(train_start)) & (f.col('tweet_timestamp') < f.lit(train_end)))\n",
    "    output_path = \"%s/%s/%s\" % (proc.path_prefix, proc.current_path, output_name)\n",
    "    df.write.format('parquet').mode('overwrite').save(output_path)\n",
    "    return proc.spark.read.parquet(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_prefix = \"hdfs://\"\n",
    "current_path = \"/recsys2021_0608_processed/sample_0_3/\"\n",
    "original_folder = \"/recsys2021_0608/\"\n",
    "dicts_folder = \"recsys_dicts/\"\n",
    "recsysSchema = RecsysSchema()\n",
    "\n",
    "##### 1. Start spark and initialize data processor #####\n",
    "t0 = timer()\n",
    "spark = SparkSession.builder.master('yarn')\\\n",
    "    .appName(\"Recsys2021_data_process\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "schema = recsysSchema.toStructType()\n",
    "\n",
    "# 1.1 prepare dataFrames\n",
    "# 1.2 create RecDP DataProcessor\n",
    "proc = DataProcessor(spark, path_prefix,\n",
    "                     current_path=current_path, dicts_path=dicts_folder)\n",
    "\n",
    "# ===============================================\n",
    "# basic: Do categorify for all columns for xgboost\n",
    "# df = spark.read.schema(schema).option('sep', '\\x01').csv(path_prefix + original_folder)\n",
    "# df = spark.read.parquet(path_prefix + original_folder)\n",
    "# df = get_train_data_with_amount_of_days(df, proc, 0.5)\n",
    "# df = spark.read.parquet(\"%s/data_splitted_by_0days/\" % current_path)\n",
    "# rename firstly\n",
    "#df = df.withColumnRenamed('enaging_user_following_count', 'engaging_user_following_count')\n",
    "#df = df.withColumnRenamed('enaging_user_is_verified', 'engaging_user_is_verified')\n",
    "#df = categorifyAllFeatures(df, proc, gen_dict=False)\n",
    "#\n",
    "## ===============================================\n",
    "## optional: do bert decode\n",
    "df = spark.read.parquet(\"/recsys2021_0608_processed/sample_0_3/validate_decoded\")\n",
    "#df = decodeBertTokenizer(df, proc, \"validate_decoded\")\n",
    "df = tweet_first_last_word(df, proc, \"validate_decoded_with_word\")\n",
    "#\n",
    "## ===============================================\n",
    "## optional: do tweet text feature engineering\n",
    "## step1: categorify tweet text\n",
    "# df = spark.read.parquet(\"%s/data_all_with_text/\" % current_path)\n",
    "# df = categorifyTweetText(df, proc, gen_dict=False)\n",
    "\n",
    "# step2: add new feature with categorified tweet\n",
    "# df = spark.read.parquet(\"%s/tweet_text_categorified/\" % current_path)\n",
    "# df = tweetFeatureEngineer(df, proc)\n",
    "\n",
    "# step3: categorify tweet hash\n",
    "# df = spark.read.parquet(\"%s/tweet_feature_engineer/\" % current_path)\n",
    "# df = categorifyTweetHash(df, proc, gen_dict=False)\n",
    "# output is tweet_text_processed\n",
    "# ===============================================\n",
    "#df = spark.read.parquet(\"%s/tweet_feature_engineer/\" % current_path)\n",
    "#df = categorifyTweetHash(df, proc, gen_dict=False, output_name=\"verify\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
