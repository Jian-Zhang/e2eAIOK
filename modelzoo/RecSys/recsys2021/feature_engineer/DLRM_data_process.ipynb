{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DLRM Categorify Performance Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. FillNA performance\n",
    "1. Baseline: Read from HDFS and write back, took 12min 35s\n",
    "[Baseline](http://sr602:18080/history/application_1615165886357_0496/SQL/execution/?id=53) [code block](#FillNA-overhead-evaluation)\n",
    "\n",
    "2. Read from HDFS and do fillNA to all columns and write, took 11min 57s \n",
    "[FillNAToAll](http://sr602:18080/history/application_1615165886357_0497/SQL/execution/?id=107) [code block](#Step3:-Fill-NA)\n",
    "\n",
    "3. Read from HDFS do fillNA to c1 column and write, took  12min 52s\n",
    "[FillNAToOne](http://sr602:18080/history/application_1615165886357_0497/SQL/execution/?id=215) [code block](#FillNA-overhead-evaluation)\n",
    "\n",
    "4. Read from HDFS do fillNA to c14-c39 column and write, took 12min 32s\n",
    "[FillNATo26](http://sr602:18080/history/application_1615165886357_0497/SQL/execution/?id=216) [code block](#FillNA-overhead-evaluation)\n",
    "\n",
    "\n",
    "##### Conclusion\n",
    "* FillNA project has very small performance overhead scaling from one column to 39 columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Categorify & FillNA performance\n",
    "1. Separate Join and FillNA to two phases, took 31min 49s + 11min 57s\n",
    "[step 1: Categorify](http://sr602:18080/history/application_1615165886357_0497/SQL/execution/?id=105)\n",
    "[step 2: FillNA](http://sr602:18080/history/application_1615165886357_0497/SQL/execution/?id=107)\n",
    "\n",
    "2. AllInOne as one phase, took 34min 24s\n",
    "[allInOne](http://sr602:18080/history/application_1615165886357_0497/SQL/execution/?id=214)\n",
    "\n",
    "##### Conclusion\n",
    "* FillNA project inside WSCG brought overhead, should avoid that!!!!\n",
    "* Reason for that is due to w/ project BHJ WSCG need no memcpy, comparison as below\n",
    "* firstly we do fillNA in the same WSCG with categorify, which took 54min [Link](http://sr602:18080/history/application_1615165886357_0497/SQL/execution/?id=161)\n",
    "* Since project should not be that heavy, we seperate categorify and FillNA to two stage, and it only took 34 min(24 + 10)\n",
    "[Link](http://sr602:18080/history/application_1615165886357_0497/SQL/execution/?id=214)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Start spark job ######\n",
    "\n",
    "from DataProcessUtils.init_spark import *\n",
    "from DataProcessUtils.utils import *\n",
    "from RecsysSchema import RecsysSchema\n",
    "\n",
    "import logging\n",
    "from timeit import default_timer as timer\n",
    "import os\n",
    "from pyspark import *\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "path_prefix = \"hdfs://\"\n",
    "csv_folder = \"/dlrm/csv_raw_data/\"\n",
    "parquet_folder = \"/dlrm/parquet_raw_data/\"\n",
    "file = \"/dlrm/raw_data/day_0\"\n",
    "#path = [os.path.join(path_prefix, folder, file) for file in files]\n",
    "csv_path = os.path.join(path_prefix, csv_folder)\n",
    "parquet_path = os.path.join(path_prefix, parquet_folder)\n",
    "#path = os.path.join(path_prefix, file)\n",
    "\n",
    "\n",
    "##### 1. Start spark and initialize data processor #####\n",
    "t0 = timer()\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .master('yarn')\\\n",
    "    .appName(\"DLRM_JOIN_FILLNA\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Enable Arrow-based columnar data transfers\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert CSV to parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LABEL_COL = 0\n",
    "#INT_COLS = list(range(1, 14))\n",
    "#CAT_COLS = list(range(14, 40))\n",
    "#label_fields = [StructField('_c%d' % LABEL_COL, IntegerType())]\n",
    "#int_fields = [StructField('_c%d' % i, IntegerType()) for i in INT_COLS]\n",
    "#str_fields = [StructField('_c%d' % i, StringType()) for i in CAT_COLS]\n",
    "#schema = StructType(label_fields + int_fields + str_fields)\n",
    "\n",
    "#files = [\"day_%d\" % i for i in range(0, 24)]\n",
    "#for file_name in files:\n",
    "#    df = spark.read.schema(schema).option('sep', '\\t').csv(os.path.join(path_prefix, csv_folder, file_name))\n",
    "#    df.write.format('parquet').mode('overwrite').save(os.path.join(path_prefix, parquet_folder, file_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Dictionary for all Category columns\n",
    "* DLRM categorify applied to 26 columns, _c14 -> _c39, corresponding dictionary length and size listed as below\n",
    "\n",
    "| col_id | numRows  | broadcastsize | broadcast elapse time |\n",
    "| ------ | -------  | ----------- | ----------- |\n",
    "| _c14   | 7912888\t| 108.4 MiB\t  | 7 s   |\n",
    "| _c15   | 33822\t| 35.8 MiB\t  | 2 s   |\n",
    "| _c16   | 17138\t| 2.3 KiB\t  | 92 ms |\n",
    "| _c17   | 7338\t    | 3.8 KiB\t  | 0.1 s |\n",
    "| _c18   | 20045\t| 4.2 MiB\t  | 0.9 s |\n",
    "| _c19   | 3\t    | 95.1 MiB\t  | 6 s   |\n",
    "| _c20   | 7104\t    | 191.0 KiB\t  | 0.3 s |\n",
    "| _c21   | 1381\t    | 978.0 B\t  | 0.4 s |\n",
    "| _c22   | 62\t    | 19.6 KiB\t  | 0.6 s |\n",
    "| _c23   | 5554113\t| 4.1 KiB\t  | 88 ms |\n",
    "| _c24   | 582468\t| 683.0 B\t  | 0.6 s |\n",
    "| _c25   | 245827\t| 7.9 MiB\t  | 0.9 s |\n",
    "| _c26   | 10\t    | 39.8 KiB\t  | 0.6 s |\n",
    "| _c27   | 2208\t    | 73.9 MiB\t  | 5 s   |\n",
    "| _c28   | 10666\t| 169.1 KiB\t  | 0.6 s |\n",
    "| _c29   | 103\t    | 222.0 B\t  | 0.5 s |\n",
    "| _c30   | 3\t    | 3.4 MiB\t  | 0.9 s |\n",
    "| _c31   | 967\t    | 3.7 KiB\t  | 0.6 s |\n",
    "| _c32   | 14\t    | 26.9 KiB\t  | 0.5 s |\n",
    "| _c33   | 8165895\t| 116.8 KiB\t  | 0.6 s |\n",
    "| _c34   | 2675939\t| 297.0 KiB\t  | 0.6 s |\n",
    "| _c35   | 7156452\t| 221.0 B\t  | 0.5 s |\n",
    "| _c36   | 302515\t| 115.3 KiB\t  | 0.1 s |\n",
    "| _c37   | 12021\t| 268.1 KiB\t  | 0.6 s |\n",
    "| _c38   | 96\t    | 105.1 MiB\t  | 7 s   |\n",
    "| _c39   | 34\t    | 503.2 KiB\t  | 0.7 s |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "#files = [os.path.join(parquet_folder, \"day_%d\" % i) for i in range(0, 24)]\n",
    "#df = spark.read.parquet(*files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def gen_dict_df(input_df, i):\n",
    "#    singular_df = input_df.select(i)\n",
    "#    singular_df = singular_df.filter(\"%s is not null\" % i)\n",
    "#    singular_df = singular_df.groupBy(i).count()\n",
    "#    singular_df = singular_df.withColumnRenamed('count', 'model_count').withColumnRenamed(i, 'data')\n",
    "#    return singular_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cols = ['_c%d' % i for i in CAT_COLS]\n",
    "#for col in cols:\n",
    "#    col_df = gen_dict_df(df, col).filter(\"model_count >= 15\")  # frequency_limit\n",
    "#    col_df.cache()\n",
    "#    col_df.write.format('parquet').mode(\n",
    "#                'overwrite').save(\"/dlrm/models/%s/\" % col)\n",
    "#    print(\"%s saved, total numRows is %d\" % (col, col_df.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorify for all category cols, start from here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f\n",
    "def get_mapping_udf(broadcast_data, default=None):\n",
    "    first_value = next(iter(broadcast_data.values()))\n",
    "    if not isinstance(first_value, int) and not isinstance(first_value, str) and not isinstance(first_value, float):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    broadcast_dict = spark.sparkContext.broadcast(\n",
    "        broadcast_data)\n",
    "\n",
    "    def get_mapped(x):\n",
    "        broadcast_data = broadcast_dict.value\n",
    "        if x in broadcast_data:\n",
    "            return broadcast_data[x]\n",
    "        else:\n",
    "            return default\n",
    "\n",
    "    # switch return type\n",
    "    if isinstance(first_value, int):\n",
    "        return udf(get_mapped, IntegerType())\n",
    "    if isinstance(first_value, str):\n",
    "        return udf(get_mapped, StringType())\n",
    "    if isinstance(first_value, float):\n",
    "        return udf(get_mapped, FloatType())\n",
    "    \n",
    "def categorify_with_udf(df, dict_df, i):\n",
    "    sorted_data = dict_df.orderBy('model_count').collect()\n",
    "    dict_data = dict((row['data'], idx) for (row, idx) in zip(\n",
    "        sorted_data, range(0, len(sorted_data))))\n",
    "    udf_impl = get_mapping_udf(dict_data)\n",
    "    df = df.withColumn(i, udf_impl(f.col(i)))\n",
    "    return df\n",
    "\n",
    "def categorify_with_join(df, dict_df, i):\n",
    "    dict_df = dict_df.withColumn('id', row_number().over(\n",
    "        Window.orderBy(desc('model_count')))).withColumn('id', f.col('id') - 1).select('data', 'id')\n",
    "    df = df.join(dict_df.hint('shuffle_hash'), f.col(i) == dict_df.data, 'left')\\\n",
    "           .withColumn(i, dict_df.id).drop(\"id\", \"data\")\n",
    "    return df\n",
    "\n",
    "def categorify_with_bhj(df, dict_df, i):\n",
    "    dict_df = dict_df.withColumn('id', row_number().over(\n",
    "        Window.orderBy(desc('model_count')))).withColumn('id', f.col('id') - 1).select('data', 'id')\n",
    "    df = df.join(dict_df.hint('broadcast'), f.col(i) == dict_df.data, 'left')\\\n",
    "           .withColumn(i, dict_df.id).drop(\"id\", \"data\")\n",
    "    return df\n",
    "\n",
    "def categorify_strategy_decision_maker(dict_dfs):\n",
    "    small_cols = []\n",
    "    long_cols = []\n",
    "    huge_cols = []\n",
    "    for (col, dict_df) in dict_dfs:\n",
    "        if (dict_df.count() > 1000000000):\n",
    "            huge_cols.append(col)\n",
    "        elif (dict_df.count() > 1000000000):\n",
    "            long_cols.append(col)\n",
    "        else:\n",
    "            small_cols.append(col)\n",
    "    return {'short_dict': small_cols, 'long_dict': long_cols, 'huge_dict': huge_cols}\n",
    "\n",
    "def categorify(df, dict_dfs):\n",
    "    strategy = categorify_strategy_decision_maker(dict_dfs)\n",
    "    # for short dict, we will do bhj\n",
    "    for (col, dict_df) in dict_dfs:\n",
    "        if col in strategy['short_dict']:\n",
    "            df = categorify_with_bhj(df, dict_df, col)    \n",
    "    # for long dict, we will do shj all along    \n",
    "    for (col, dict_df) in dict_dfs:\n",
    "        if col in strategy['long_dict']:\n",
    "            df = categorify_with_bhj(df, dict_df, col)\n",
    "    # for huge dict, we will do shj seperately\n",
    "    for (col, dict_df) in dict_dfs:\n",
    "        if col in strategy['huge_dict']:\n",
    "            df = categorify_with_join(df, dict_df, col)\n",
    "            tmp_path = \"/dlrm/tmp/%s/\" % uuid.uuid1()\n",
    "            df.write.format('parquet').mode('overwrite').save(tmp_path)\n",
    "            df = spark.read.parquet(tmp_path)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Load all pre-generated dicts and original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "files = [os.path.join(parquet_folder, \"day_%d\" % i) for i in range(0, 24)]\n",
    "df = spark.read.parquet(*files)\n",
    "LABEL_COL = 0\n",
    "INT_COLS = list(range(1, 14))\n",
    "CAT_COLS = list(range(14, 40))\n",
    "cols = ['_c%d' % i for i in CAT_COLS]\n",
    "models_folder = \"/dlrm/models/\"\n",
    "dict_dfs = [(col, spark.read.parquet(os.path.join(models_folder, col))) for col in cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 Categorify and FillNA - All In One\n",
    "* We tried to do short dict / long dict and FillNA seperately, \n",
    "* noticed this data is not very memory intensive, so decided to do All In One\n",
    "\n",
    "##### Notice ! Now we verified option 1 performed better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### option 1: seperate categorify and fillNA in two stages ######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'short_dict': ['_c14', '_c15', '_c16', '_c17', '_c18', '_c19', '_c20', '_c21', '_c22', '_c23', '_c24', '_c25', '_c26', '_c27', '_c28', '_c29', '_c30', '_c31', '_c32', '_c33', '_c34', '_c35', '_c36', '_c37', '_c38', '_c39'], 'long_dict': [], 'huge_dict': []}\n",
      "CPU times: user 282 ms, sys: 180 ms, total: 462 ms\n",
      "Wall time: 34min 24s\n"
     ]
    }
   ],
   "source": [
    "df = categorify(df, dict_dfs)\n",
    "cols = ['_c%d' % i for i in INT_COLS + CAT_COLS]\n",
    "df= df.repartition(2000)\n",
    "df = df.fillna(0, cols)\n",
    "%time df.write.format('parquet').mode('overwrite').save(\"/dlrm/tmp/%s/\" % uuid.uuid1())\n",
    "# save to: /dlrm/tmp/d684684e-a970-11eb-b161-a4bf0121f496\n",
    "# spark plan: http://sr602:18080/history/application_1615165886357_0497/SQL/execution/?id=162"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### option 2: categorify and fillNA in same WSCG stages ######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'short_dict': ['_c14', '_c15', '_c16', '_c17', '_c18', '_c19', '_c20', '_c21', '_c22', '_c23', '_c24', '_c25', '_c26', '_c27', '_c28', '_c29', '_c30', '_c31', '_c32', '_c33', '_c34', '_c35', '_c36', '_c37', '_c38', '_c39'], 'long_dict': [], 'huge_dict': []}\n",
      "FillNA to below cols:\n",
      "['_c1', '_c2', '_c3', '_c4', '_c5', '_c6', '_c7', '_c8', '_c9', '_c10', '_c11', '_c12', '_c13', '_c14', '_c15', '_c16', '_c17', '_c18', '_c19', '_c20', '_c21', '_c22', '_c23', '_c24', '_c25', '_c26', '_c27', '_c28', '_c29', '_c30', '_c31', '_c32', '_c33', '_c34', '_c35', '_c36', '_c37', '_c38', '_c39']\n",
      "CPU times: user 463 ms, sys: 302 ms, total: 765 ms\n",
      "Wall time: 54min 24s\n"
     ]
    }
   ],
   "source": [
    "df = categorify(df, dict_dfs)\n",
    "cols = ['_c%d' % i for i in INT_COLS + CAT_COLS]\n",
    "print(\"FillNA to below cols:\")\n",
    "print(cols)\n",
    "df = df.fillna(0, cols)\n",
    "%time df.write.format('parquet').mode('overwrite').save(\"/dlrm/tmp/%s/\" % uuid.uuid1())\n",
    "# save to: /dlrm/tmp/e2b4b890-a965-11eb-b161-a4bf0121f496\n",
    "# spark plan: http://sr602:18080/history/application_1615165886357_0497/SQL/execution/?id=161"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### option 3: using stringIndexer for categorify and fillNA in two stages ######\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "def categorify(df, dict_dfs):\n",
    "    strategy = categorify_strategy_decision_maker(dict_dfs)\n",
    "    indexers = []\n",
    "    # for short dict, we will do bhj\n",
    "    for (col, dict_df) in dict_dfs:\n",
    "        if col in strategy['short_dict'] or col in strategy['long_dict']:\n",
    "            indexer = StringIndexer(inputCol=col, outputCol=col+\"_index\").fit(df)\n",
    "            indexers.append(indexer)\n",
    "    # for huge dict, we will do shj seperately\n",
    "    for (col, dict_df) in dict_dfs:\n",
    "        if col in strategy['huge_dict']:\n",
    "            df = categorify_with_join(df, dict_df, col)\n",
    "            tmp_path = \"/dlrm/tmp/%s/\" % uuid.uuid1()\n",
    "            df.write.format('parquet').mode('overwrite').save(tmp_path)\n",
    "            df = spark.read.parquet(tmp_path)\n",
    "    pipeline = Pipeline(stages=indexers)\n",
    "    df = pipeline.fit(df).transform(df)\n",
    "    return df\n",
    "\n",
    "df = categorify(df, dict_dfs)\n",
    "cols = ['_c%d' % i for i in INT_COLS + CAT_COLS]\n",
    "#df= df.repartition(2000)\n",
    "df = df.fillna(0, cols)\n",
    "%time df.write.format('parquet').mode('overwrite').save(\"/dlrm/tmp/%s/\" % uuid.uuid1())\n",
    "# save to: /dlrm/tmp/d684684e-a970-11eb-b161-a4bf0121f496\n",
    "# spark plan: http://sr602:18080/history/application_1615165886357_0497/SQL/execution/?id=162"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Below is why we decided to perform 'All In One'\n",
    "### Step 1: Categorify\n",
    "#### option 1: use python UDF to do categorify, took 1h 26min 36s  -> Bad\n",
    "#### option 2: use Join to do categorify, took 28min and 18 min (tried bhj to all 26 dict, took 31min 49s) -> Looks much better\n",
    "* First round do BHJ to small dict, took 28min\n",
    "* Second round do BHJ to long dict, took 18 min\n",
    "* Then tried to do above BHJ at once, took 31min 49s\n",
    "\n",
    "#### option 3: use scala UDF to do categorify, still wip -> similiar to StringIndexer\n",
    "#### option 4: use Join to do categorify(BHJ + SHJ) -> very stupid idea\n",
    "* SHJ will need to do repartition by join key, materialize every round\n",
    "* SHJ should only apply to huge dict (which BHJ will definitely go OOM case)\n",
    "\n",
    "### Step 2: FillNA\n",
    "* Took about 11min 57s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Do categorify for all short disk columns (Categorify part I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14.7 ms, sys: 6.78 ms, total: 21.5 ms\n",
      "Wall time: 20.6 s\n",
      "{'short_dict': ['_c15', '_c16', '_c17', '_c18', '_c19', '_c20', '_c21', '_c22', '_c24', '_c25', '_c26', '_c27', '_c28', '_c29', '_c30', '_c31', '_c32', '_c36', '_c37', '_c38', '_c39'], 'long_dict': ['_c14', '_c23', '_c33', '_c34', '_c35'], 'huge_dict': []}\n"
     ]
    }
   ],
   "source": [
    "%time categorify_strategy = categorify_strategy_decision_maker(dict_dfs)\n",
    "print(categorify_strategy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 766 ms, sys: 288 ms, total: 1.05 s\n",
      "Wall time: 1h 26min 36s\n"
     ]
    }
   ],
   "source": [
    "###### Option 1: Do Python UDF to all short dist columns ######\n",
    "###### Comment: Python UDF performed very bad!!!!! ######\n",
    "\n",
    "def categorify(df, dict_dfs):\n",
    "    for (col, dict_df) in dict_dfs:\n",
    "        df = categorify_with_udf(df, dict_df, col)\n",
    "    return df\n",
    "\n",
    "cols = categorify_strategy['short_dict']\n",
    "models_folder = \"/dlrm/models/\"\n",
    "dict_dfs = [(col, spark.read.parquet(os.path.join(models_folder, col))) for col in cols]\n",
    "df = categorify(df, dict_dfs)\n",
    "%time df.write.format('parquet').mode('overwrite').save(\"/dlrm/tmp/%s/\" % uuid.uuid1())\n",
    "# spark plan: http://sr602:18080/history/application_1615165886357_0493/SQL/execution/?id=70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 164 ms, sys: 78.4 ms, total: 243 ms\n",
      "Wall time: 28min 36s\n"
     ]
    }
   ],
   "source": [
    "###### Option 2: part 1 Do BHJ to all short dist columns ######\n",
    "###### Comment: Since dict are short, BHJ did pretty well ! ######\n",
    "\n",
    "cols = categorify_strategy['short_dict']\n",
    "models_folder = \"/dlrm/models/\"\n",
    "dict_dfs = [(col, spark.read.parquet(os.path.join(models_folder, col))) for col in cols]\n",
    "df = categorify(df, dict_dfs)\n",
    "%time df.write.format('parquet').mode('overwrite').save(\"/dlrm/tmp/%s/\" % uuid.uuid1())\n",
    "# spark plan: http://sr602:18080/history/application_1615165886357_0493/SQL/execution/?id=71"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 134 ms, sys: 119 ms, total: 253 ms\n",
      "Wall time: 18min\n"
     ]
    }
   ],
   "source": [
    "###### Option 2: part 2 try to do bhj to long dict ######\n",
    "###### Comment: Since shj need to do repartition based on join key,  ######\n",
    "######          and long dict seems fit memory well, use BHJ as well ######\n",
    "\n",
    "def categorify(df, dict_dfs):\n",
    "    for (col, dict_df) in dict_dfs:\n",
    "        df = categorify_with_bhj(df, dict_df, col)\n",
    "    return df\n",
    "df = spark.read.parquet(\"/dlrm/tmp/7538e986-a8f6-11eb-914a-a4bf0121f496\")\n",
    "cols = categorify_strategy['long_dict']\n",
    "models_folder = \"/dlrm/models/\"\n",
    "dict_dfs = [(col, spark.read.parquet(os.path.join(models_folder, col))) for col in cols]\n",
    "df = categorify(df, dict_dfs)\n",
    "%time df.write.format('parquet').mode('overwrite').save(\"/dlrm/tmp/%s/\" % uuid.uuid1())\n",
    "# saved to /dlrm/tmp/7ab036be-a944-11eb-8f99-a4bf0121f496\n",
    "# spark plan: http://sr602:18080/history/application_1615165886357_0496/SQL/execution/?id=52"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'short_dict': ['_c14', '_c15', '_c16', '_c17', '_c18', '_c19', '_c20', '_c21', '_c22', '_c23', '_c24', '_c25', '_c26', '_c27', '_c28', '_c29', '_c30', '_c31', '_c32', '_c33', '_c34', '_c35', '_c36', '_c37', '_c38', '_c39'], 'long_dict': [], 'huge_dict': []}\n",
      "CPU times: user 240 ms, sys: 191 ms, total: 432 ms\n",
      "Wall time: 31min 49s\n"
     ]
    }
   ],
   "source": [
    "###### option 2 (part 1 & part 2) ######\n",
    "df = categorify(df, dict_dfs)\n",
    "%time df.write.format('parquet').mode('overwrite').save(\"/dlrm/tmp/%s/\" % uuid.uuid1())\n",
    "# spark plan: http://sr602:8088/proxy/application_1615165886357_0497/SQL/execution/?id=105"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Option 3: Can we do with scala UDFs ######\n",
    "###### Comment:  This should be a similiar approach as StringIndexer did ######\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step3: Fill NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 108 ms, sys: 73.9 ms, total: 182 ms\n",
      "Wall time: 11min 57s\n"
     ]
    }
   ],
   "source": [
    "#read, fillNAToAllColumns and write\n",
    "df = spark.read.parquet(\"/dlrm/tmp/7ab036be-a944-11eb-8f99-a4bf0121f496\")\n",
    "cols = ['_c%d' % i for i in INT_COLS + CAT_COLS]\n",
    "#print(cols)\n",
    "df = df.fillna(0, cols)\n",
    "%time df.write.format('parquet').mode('overwrite').save(\"/dlrm/tmp/%s/\" % uuid.uuid1())\n",
    "# spark plan: http://sr602:18080/history/application_1615165886357_0497/SQL/execution/?id=107"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FillNA overhead evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 108 ms, sys: 83.9 ms, total: 192 ms\n",
      "Wall time: 12min 35s\n"
     ]
    }
   ],
   "source": [
    "# baseline: simply read and write\n",
    "df = spark.read.parquet(\"/dlrm/tmp/7ab036be-a944-11eb-8f99-a4bf0121f496\")\n",
    "cols = ['_c%d' % i for i in INT_COLS + CAT_COLS]\n",
    "#print(cols)\n",
    "df = df.select(*cols)\n",
    "%time df.write.format('parquet').mode('overwrite').save(\"/dlrm/tmp/%s/\" % uuid.uuid1())\n",
    "# spark plan: http://sr602:18080/history/application_1615165886357_0497/SQL/execution/?id=108"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 122 ms, sys: 69 ms, total: 191 ms\n",
      "Wall time: 12min 52s\n"
     ]
    }
   ],
   "source": [
    "# fillNA to one column\n",
    "df = spark.read.parquet(\"/dlrm/tmp/7ab036be-a944-11eb-8f99-a4bf0121f496\")\n",
    "cols = ['_c%d' % i for i in [1]]\n",
    "#print(cols)\n",
    "df = df.fillna(0, cols)\n",
    "%time df.write.format('parquet').mode('overwrite').save(\"/dlrm/tmp/%s/\" % uuid.uuid1())\n",
    "# spark plan: http://sr602:18080/history/application_1615165886357_0497/SQL/execution/?id=215"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 117 ms, sys: 68.7 ms, total: 185 ms\n",
      "Wall time: 12min 32s\n"
     ]
    }
   ],
   "source": [
    "# fillNA to all categorify columns\n",
    "df = spark.read.parquet(\"/dlrm/tmp/7ab036be-a944-11eb-8f99-a4bf0121f496\")\n",
    "cols = ['_c%d' % i for i in CAT_COLS]\n",
    "#print(cols)\n",
    "df = df.fillna(0, cols)\n",
    "%time df.write.format('parquet').mode('overwrite').save(\"/dlrm/tmp/%s/\" % uuid.uuid1())\n",
    "# save to: \n",
    "# spark plan: http://sr602:18080/history/application_1615165886357_0497/SQL/execution/?id=215"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Redundant, skip belows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def assign_id_with_window(df):\n",
    "#    windowed = Window.partitionBy('column_id').orderBy(desc('count'))\n",
    "#    return (df\n",
    "#            .withColumn('id', row_number().over(windowed))\n",
    "#            .withColumnRenamed('count', 'model_count'))\n",
    "#\n",
    "#def get_column_counts_with_frequency_limit(df, frequency_limit = None):\n",
    "#    cols = ['_c%d' % i for i in CAT_COLS]\n",
    "#    df = (df\n",
    "#        .select(posexplode(array(*cols)))\n",
    "#        .withColumnRenamed('pos', 'column_id')\n",
    "#        .withColumnRenamed('col', 'data')\n",
    "#        .filter('data is not null')\n",
    "#        .groupBy('column_id', 'data')\n",
    "#        .count())\n",
    "#    if frequency_limit:\n",
    "#        exclude = []\n",
    "#        default_limit = frequency_limit\n",
    "#        if default_limit:\n",
    "#            remain = [x - CAT_COLS[0] for x in CAT_COLS if x not in exclude]\n",
    "#            df = df.filter((~col('column_id').isin(remain)) | (col('count') >= default_limit))\n",
    "#    return df\n",
    "#\n",
    "#def get_column_models(combined_model):\n",
    "#    for i in CAT_COLS:\n",
    "#        model = (combined_model\n",
    "#            .filter('column_id == %d' % (i - CAT_COLS[0]))\n",
    "#            .drop('column_id'))\n",
    "#        yield i, model\n",
    "#col_counts = get_column_counts_with_frequency_limit(df, 15)\n",
    "#combined_model_df = assign_id_with_window(col_counts)\n",
    "#column_models_df_dict = get_column_models(combined_model_df)\n",
    "#for i, column_models_df in column_models_df_dict:\n",
    "#    column_models_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+--------+\n",
      "|    _c14|    _c15|    _c16|\n",
      "+--------+--------+--------+\n",
      "|62770d79|e21f5d58|afea442f|\n",
      "|e5f3fd8d|a0aaffa6|6faa15d5|\n",
      "|62770d79|ad984203|62bec60d|\n",
      "|    null|710103fd|c73d2eb5|\n",
      "|02e197c5|c2ced437|a2427619|\n",
      "|8a2b1e43|3fa554c6|0b8e4616|\n",
      "|a80f39e1|79782afd|ddb2d2e1|\n",
      "|a46c5216|ddc72fb0|49c7ebf8|\n",
      "|9318c40b|53c06361|fea787e5|\n",
      "|62770d79|ad984203|ddd956c1|\n",
      "|ad98e872|3dbb483e|6faa15d5|\n",
      "|09fd7fc8|d92abd7e|2826bc68|\n",
      "|359aaecc|f501e8d6|43504368|\n",
      "|7ffd46c3|710103fd|a1407382|\n",
      "|9a8cb066|7a06385f|417e6103|\n",
      "|77519961|a0d14bda|b69d8ea9|\n",
      "|447072d2|831049b5|02c63370|\n",
      "|3a20c9b6|c2ae8fa1|6faa15d5|\n",
      "|e5f3fd8d|a15d1051|72181f31|\n",
      "|072027fa|21789080|3223c131|\n",
      "+--------+--------+--------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---+--------+\n",
      "|pos|     col|\n",
      "+---+--------+\n",
      "|  0|62770d79|\n",
      "|  1|e21f5d58|\n",
      "|  2|afea442f|\n",
      "|  0|e5f3fd8d|\n",
      "|  1|a0aaffa6|\n",
      "|  2|6faa15d5|\n",
      "|  0|62770d79|\n",
      "|  1|ad984203|\n",
      "|  2|62bec60d|\n",
      "|  0|    null|\n",
      "|  1|710103fd|\n",
      "|  2|c73d2eb5|\n",
      "|  0|02e197c5|\n",
      "|  1|c2ced437|\n",
      "|  2|a2427619|\n",
      "|  0|8a2b1e43|\n",
      "|  1|3fa554c6|\n",
      "|  2|0b8e4616|\n",
      "|  0|a80f39e1|\n",
      "|  1|79782afd|\n",
      "+---+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cols = ['_c%d' % i for i in [14, 15, 16]]\n",
    "new_df = df.select(posexplode(array(*cols)))\n",
    "df.select(*cols).show()\n",
    "new_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1648.save.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:226)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:178)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:175)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:171)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:122)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:121)\n\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:944)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:763)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:944)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:396)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:380)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:269)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Job 366 cancelled because killed via the Web UI\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2023)\n\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:1919)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleStageCancellation$1(DAGScheduler.scala:1908)\n\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofInt.foreach(ArrayOps.scala:246)\n\tat org.apache.spark.scheduler.DAGScheduler.handleStageCancellation(DAGScheduler.scala:1901)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2166)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2152)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2141)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m/hadoop/spark/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m    825\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    826\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 827\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    828\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    829\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/hadoop/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/hadoop/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/hadoop/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o1648.save.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:226)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:178)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:175)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:171)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:122)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:121)\n\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:944)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:763)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:944)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:396)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:380)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:269)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Job 366 cancelled because killed via the Web UI\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2023)\n\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:1919)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleStageCancellation$1(DAGScheduler.scala:1908)\n\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofInt.foreach(ArrayOps.scala:246)\n\tat org.apache.spark.scheduler.DAGScheduler.handleStageCancellation(DAGScheduler.scala:1901)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2166)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2152)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2141)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n"
     ]
    }
   ],
   "source": [
    "###### option 1: seperate categorify and fillNA in two stages ######\n",
    "df = categorify(df, dict_dfs)\n",
    "cols = ['_c%d' % i for i in INT_COLS + CAT_COLS]\n",
    "df = df.fillna(0, cols)\n",
    "%time df.write.format('parquet').mode('overwrite').save(\"/dlrm/tmp/%s/\" % uuid.uuid1())\n",
    "# save to: /dlrm/tmp/d684684e-a970-11eb-b161-a4bf0121f496\n",
    "# spark plan: http://sr602:18080/history/application_1615165886357_0497/SQL/execution/?id=162"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
